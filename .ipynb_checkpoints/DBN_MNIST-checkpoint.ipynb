{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NumPy\n",
    "import numpy as np\n",
    "np.random.seed(42)  # Set random seed\n",
    "\n",
    "# Import scikit-learn DBN dependency modules\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.classification import accuracy_score\n",
    "\n",
    "# Import deep-belief network\n",
    "# Found at https://github.com/albertbup/deep-belief-network\n",
    "from dbn.tensorflow import SupervisedDBNClassification\n",
    "\n",
    "# Graphs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and format dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml(\"mnist_784\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of 1000 MNIST datapoints\n",
    "class_count = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# Get 100 of each class\n",
    "for i in range(0, len(mnist.data)):\n",
    "    target_value = int(mnist.target[i])\n",
    "    if class_count[target_value] < 100:\n",
    "        X.append(mnist.data[i])\n",
    "        Y.append(target_value)\n",
    "        class_count[target_value] += 1\n",
    "    if sum(class_count) == 1000:\n",
    "        break\n",
    "\n",
    "# Convert to NumPy\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Scale data for MNIST\n",
    "X = (X / 256).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_belief_net(hidden_layers_structure=[256, 256], \n",
    "                    learning_rate_rbm=0.05,\n",
    "                    learning_rate=0.1,\n",
    "                    n_epochs_rbm=10,\n",
    "                    n_iter_backprop=100,\n",
    "                    batch_size=32,\n",
    "                    activation_function='relu',\n",
    "                    dropout_p=0.2):\n",
    "\n",
    "    # Splitting data\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "    # Training\n",
    "    classifier = SupervisedDBNClassification(hidden_layers_structure=hidden_layers_structure,\n",
    "                                             learning_rate_rbm=learning_rate_rbm,\n",
    "                                             learning_rate=learning_rate,\n",
    "                                             n_epochs_rbm=n_epochs_rbm,\n",
    "                                             n_iter_backprop=n_iter_backprop,\n",
    "                                             batch_size=batch_size,\n",
    "                                             activation_function=activation_function,\n",
    "                                             dropout_p=dropout_p)\n",
    "    classifier.fit(X_train, Y_train)\n",
    "\n",
    "    # Save the model\n",
    "    classifier.save('model.pkl')\n",
    "\n",
    "    # Restore\n",
    "    classifier = SupervisedDBNClassification.load('model.pkl')\n",
    "\n",
    "    # Test\n",
    "    Y_pred = classifier.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, Y_pred)\n",
    "    print('Done.\\nAccuracy: %f' % acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run DBN tests below:\n",
    "Default setting (base performance)\n",
    "\n",
    "hidden_layers_structure=[256, 256]\n",
    "learning_rate_rbm=0.05\n",
    "learning_rate=0.1\n",
    "n_epochs_rbm=10\n",
    "n_iter_backprop=100\n",
    "batch_size=32\n",
    "activation_function='relu'\n",
    "dropout_p=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      "WARNING:tensorflow:From C:\\Users\\ALIMI\\Travail python2022\\DBN_Mr_Yassine\\dbn\\tensorflow\\models.py:151: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 50.802662\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 60.613663\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 50.652267\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 42.363338\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 41.087261\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 58.575821\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 45.028019\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 46.514248\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 31.058884\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 34.474251\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 131.230072\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 148.412552\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 180.380722\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 188.834381\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 228.656799\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 222.335159\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 138.648148\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 191.445023\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 153.702499\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 153.362122\n",
      "[END] Pre-training step\n",
      "WARNING:tensorflow:From C:\\Users\\ALIMI\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1096: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.740879\n",
      ">> Epoch 1 finished \tANN training loss 0.453924\n",
      ">> Epoch 2 finished \tANN training loss 0.376081\n",
      ">> Epoch 3 finished \tANN training loss 0.350390\n",
      ">> Epoch 4 finished \tANN training loss 0.287730\n",
      ">> Epoch 5 finished \tANN training loss 0.240652\n",
      ">> Epoch 6 finished \tANN training loss 0.220042\n",
      ">> Epoch 7 finished \tANN training loss 0.203324\n",
      ">> Epoch 8 finished \tANN training loss 0.177993\n",
      ">> Epoch 9 finished \tANN training loss 0.158155\n",
      ">> Epoch 10 finished \tANN training loss 0.141582\n",
      ">> Epoch 11 finished \tANN training loss 0.131120\n",
      ">> Epoch 12 finished \tANN training loss 0.121698\n",
      ">> Epoch 13 finished \tANN training loss 0.100991\n",
      ">> Epoch 14 finished \tANN training loss 0.091841\n",
      ">> Epoch 15 finished \tANN training loss 0.093313\n",
      ">> Epoch 16 finished \tANN training loss 0.076929\n",
      ">> Epoch 17 finished \tANN training loss 0.080416\n",
      ">> Epoch 18 finished \tANN training loss 0.067496\n",
      ">> Epoch 19 finished \tANN training loss 0.061841\n",
      ">> Epoch 20 finished \tANN training loss 0.051572\n",
      ">> Epoch 21 finished \tANN training loss 0.051893\n",
      ">> Epoch 22 finished \tANN training loss 0.046011\n",
      ">> Epoch 23 finished \tANN training loss 0.042742\n",
      ">> Epoch 24 finished \tANN training loss 0.038027\n",
      ">> Epoch 25 finished \tANN training loss 0.034139\n",
      ">> Epoch 26 finished \tANN training loss 0.029650\n",
      ">> Epoch 27 finished \tANN training loss 0.028128\n",
      ">> Epoch 28 finished \tANN training loss 0.029218\n",
      ">> Epoch 29 finished \tANN training loss 0.023742\n",
      ">> Epoch 30 finished \tANN training loss 0.024107\n",
      ">> Epoch 31 finished \tANN training loss 0.020375\n",
      ">> Epoch 32 finished \tANN training loss 0.019032\n",
      ">> Epoch 33 finished \tANN training loss 0.019501\n",
      ">> Epoch 34 finished \tANN training loss 0.016588\n",
      ">> Epoch 35 finished \tANN training loss 0.016849\n",
      ">> Epoch 36 finished \tANN training loss 0.013565\n",
      ">> Epoch 37 finished \tANN training loss 0.015647\n",
      ">> Epoch 38 finished \tANN training loss 0.013256\n",
      ">> Epoch 39 finished \tANN training loss 0.012850\n",
      ">> Epoch 40 finished \tANN training loss 0.011267\n",
      ">> Epoch 41 finished \tANN training loss 0.010910\n",
      ">> Epoch 42 finished \tANN training loss 0.010203\n",
      ">> Epoch 43 finished \tANN training loss 0.009070\n",
      ">> Epoch 44 finished \tANN training loss 0.008616\n",
      ">> Epoch 45 finished \tANN training loss 0.008238\n",
      ">> Epoch 46 finished \tANN training loss 0.008010\n",
      ">> Epoch 47 finished \tANN training loss 0.007488\n",
      ">> Epoch 48 finished \tANN training loss 0.007079\n",
      ">> Epoch 49 finished \tANN training loss 0.006532\n",
      ">> Epoch 50 finished \tANN training loss 0.006658\n",
      ">> Epoch 51 finished \tANN training loss 0.005796\n",
      ">> Epoch 52 finished \tANN training loss 0.005387\n",
      ">> Epoch 53 finished \tANN training loss 0.004965\n",
      ">> Epoch 54 finished \tANN training loss 0.005312\n",
      ">> Epoch 55 finished \tANN training loss 0.006516\n",
      ">> Epoch 56 finished \tANN training loss 0.005392\n",
      ">> Epoch 57 finished \tANN training loss 0.004413\n",
      ">> Epoch 58 finished \tANN training loss 0.003986\n",
      ">> Epoch 59 finished \tANN training loss 0.003994\n",
      ">> Epoch 60 finished \tANN training loss 0.004433\n",
      ">> Epoch 61 finished \tANN training loss 0.003990\n",
      ">> Epoch 62 finished \tANN training loss 0.003725\n",
      ">> Epoch 63 finished \tANN training loss 0.003416\n",
      ">> Epoch 64 finished \tANN training loss 0.003505\n",
      ">> Epoch 65 finished \tANN training loss 0.003711\n",
      ">> Epoch 66 finished \tANN training loss 0.003126\n",
      ">> Epoch 67 finished \tANN training loss 0.002980\n",
      ">> Epoch 68 finished \tANN training loss 0.003289\n",
      ">> Epoch 69 finished \tANN training loss 0.002972\n",
      ">> Epoch 70 finished \tANN training loss 0.002515\n",
      ">> Epoch 71 finished \tANN training loss 0.004003\n",
      ">> Epoch 72 finished \tANN training loss 0.002592\n",
      ">> Epoch 73 finished \tANN training loss 0.002393\n",
      ">> Epoch 74 finished \tANN training loss 0.002076\n",
      ">> Epoch 75 finished \tANN training loss 0.002217\n",
      ">> Epoch 76 finished \tANN training loss 0.001922\n",
      ">> Epoch 77 finished \tANN training loss 0.002186\n",
      ">> Epoch 78 finished \tANN training loss 0.001763\n",
      ">> Epoch 79 finished \tANN training loss 0.001740\n",
      ">> Epoch 80 finished \tANN training loss 0.001964\n",
      ">> Epoch 81 finished \tANN training loss 0.001520\n",
      ">> Epoch 82 finished \tANN training loss 0.001709\n",
      ">> Epoch 83 finished \tANN training loss 0.001536\n",
      ">> Epoch 84 finished \tANN training loss 0.001592\n",
      ">> Epoch 85 finished \tANN training loss 0.001527\n",
      ">> Epoch 86 finished \tANN training loss 0.001369\n",
      ">> Epoch 87 finished \tANN training loss 0.001391\n",
      ">> Epoch 88 finished \tANN training loss 0.001222\n",
      ">> Epoch 89 finished \tANN training loss 0.001163\n",
      ">> Epoch 90 finished \tANN training loss 0.001048\n",
      ">> Epoch 91 finished \tANN training loss 0.001100\n",
      ">> Epoch 92 finished \tANN training loss 0.001272\n",
      ">> Epoch 93 finished \tANN training loss 0.001171\n",
      ">> Epoch 94 finished \tANN training loss 0.001119\n",
      ">> Epoch 95 finished \tANN training loss 0.001128\n",
      ">> Epoch 96 finished \tANN training loss 0.001223\n",
      ">> Epoch 97 finished \tANN training loss 0.001206\n",
      ">> Epoch 98 finished \tANN training loss 0.001098\n",
      ">> Epoch 99 finished \tANN training loss 0.001762\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.920000\n"
     ]
    }
   ],
   "source": [
    "# Run DBN\n",
    "default_acc = deep_belief_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.92\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(default_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different 1-layer structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store accuracies\n",
    "onelayer_acc = [0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting 1\n",
    "\n",
    "hidden_layers_structure=[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 71.883064\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 49.219536\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 72.821922\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 50.523182\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 51.346024\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 69.615524\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 75.025543\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 59.427540\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 82.042488\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 57.877441\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.799542\n",
      ">> Epoch 1 finished \tANN training loss 0.587551\n",
      ">> Epoch 2 finished \tANN training loss 0.490077\n",
      ">> Epoch 3 finished \tANN training loss 0.431253\n",
      ">> Epoch 4 finished \tANN training loss 0.405054\n",
      ">> Epoch 5 finished \tANN training loss 0.342474\n",
      ">> Epoch 6 finished \tANN training loss 0.318522\n",
      ">> Epoch 7 finished \tANN training loss 0.304979\n",
      ">> Epoch 8 finished \tANN training loss 0.272345\n",
      ">> Epoch 9 finished \tANN training loss 0.248468\n",
      ">> Epoch 10 finished \tANN training loss 0.227654\n",
      ">> Epoch 11 finished \tANN training loss 0.217763\n",
      ">> Epoch 12 finished \tANN training loss 0.198856\n",
      ">> Epoch 13 finished \tANN training loss 0.190808\n",
      ">> Epoch 14 finished \tANN training loss 0.174909\n",
      ">> Epoch 15 finished \tANN training loss 0.166025\n",
      ">> Epoch 16 finished \tANN training loss 0.148653\n",
      ">> Epoch 17 finished \tANN training loss 0.143548\n",
      ">> Epoch 18 finished \tANN training loss 0.131650\n",
      ">> Epoch 19 finished \tANN training loss 0.123233\n",
      ">> Epoch 20 finished \tANN training loss 0.116856\n",
      ">> Epoch 21 finished \tANN training loss 0.106459\n",
      ">> Epoch 22 finished \tANN training loss 0.097501\n",
      ">> Epoch 23 finished \tANN training loss 0.095554\n",
      ">> Epoch 24 finished \tANN training loss 0.087316\n",
      ">> Epoch 25 finished \tANN training loss 0.083704\n",
      ">> Epoch 26 finished \tANN training loss 0.077825\n",
      ">> Epoch 27 finished \tANN training loss 0.072895\n",
      ">> Epoch 28 finished \tANN training loss 0.066946\n",
      ">> Epoch 29 finished \tANN training loss 0.069489\n",
      ">> Epoch 30 finished \tANN training loss 0.061696\n",
      ">> Epoch 31 finished \tANN training loss 0.056340\n",
      ">> Epoch 32 finished \tANN training loss 0.054868\n",
      ">> Epoch 33 finished \tANN training loss 0.050262\n",
      ">> Epoch 34 finished \tANN training loss 0.048128\n",
      ">> Epoch 35 finished \tANN training loss 0.046277\n",
      ">> Epoch 36 finished \tANN training loss 0.043269\n",
      ">> Epoch 37 finished \tANN training loss 0.041793\n",
      ">> Epoch 38 finished \tANN training loss 0.039154\n",
      ">> Epoch 39 finished \tANN training loss 0.035572\n",
      ">> Epoch 40 finished \tANN training loss 0.034953\n",
      ">> Epoch 41 finished \tANN training loss 0.032984\n",
      ">> Epoch 42 finished \tANN training loss 0.030934\n",
      ">> Epoch 43 finished \tANN training loss 0.031275\n",
      ">> Epoch 44 finished \tANN training loss 0.030837\n",
      ">> Epoch 45 finished \tANN training loss 0.028003\n",
      ">> Epoch 46 finished \tANN training loss 0.026107\n",
      ">> Epoch 47 finished \tANN training loss 0.025139\n",
      ">> Epoch 48 finished \tANN training loss 0.025589\n",
      ">> Epoch 49 finished \tANN training loss 0.022883\n",
      ">> Epoch 50 finished \tANN training loss 0.021873\n",
      ">> Epoch 51 finished \tANN training loss 0.021470\n",
      ">> Epoch 52 finished \tANN training loss 0.020200\n",
      ">> Epoch 53 finished \tANN training loss 0.019853\n",
      ">> Epoch 54 finished \tANN training loss 0.021023\n",
      ">> Epoch 55 finished \tANN training loss 0.020325\n",
      ">> Epoch 56 finished \tANN training loss 0.018010\n",
      ">> Epoch 57 finished \tANN training loss 0.018231\n",
      ">> Epoch 58 finished \tANN training loss 0.016271\n",
      ">> Epoch 59 finished \tANN training loss 0.016705\n",
      ">> Epoch 60 finished \tANN training loss 0.015342\n",
      ">> Epoch 61 finished \tANN training loss 0.014624\n",
      ">> Epoch 62 finished \tANN training loss 0.014535\n",
      ">> Epoch 63 finished \tANN training loss 0.013463\n",
      ">> Epoch 64 finished \tANN training loss 0.012835\n",
      ">> Epoch 65 finished \tANN training loss 0.012424\n",
      ">> Epoch 66 finished \tANN training loss 0.013055\n",
      ">> Epoch 67 finished \tANN training loss 0.012515\n",
      ">> Epoch 68 finished \tANN training loss 0.012381\n",
      ">> Epoch 69 finished \tANN training loss 0.011504\n",
      ">> Epoch 70 finished \tANN training loss 0.011227\n",
      ">> Epoch 71 finished \tANN training loss 0.011003\n",
      ">> Epoch 72 finished \tANN training loss 0.010125\n",
      ">> Epoch 73 finished \tANN training loss 0.011115\n",
      ">> Epoch 74 finished \tANN training loss 0.010266\n",
      ">> Epoch 75 finished \tANN training loss 0.009766\n",
      ">> Epoch 76 finished \tANN training loss 0.009552\n",
      ">> Epoch 77 finished \tANN training loss 0.009363\n",
      ">> Epoch 78 finished \tANN training loss 0.009341\n",
      ">> Epoch 79 finished \tANN training loss 0.008733\n",
      ">> Epoch 80 finished \tANN training loss 0.008560\n",
      ">> Epoch 81 finished \tANN training loss 0.008042\n",
      ">> Epoch 82 finished \tANN training loss 0.007451\n",
      ">> Epoch 83 finished \tANN training loss 0.007263\n",
      ">> Epoch 84 finished \tANN training loss 0.007120\n",
      ">> Epoch 85 finished \tANN training loss 0.006927\n",
      ">> Epoch 86 finished \tANN training loss 0.006961\n",
      ">> Epoch 87 finished \tANN training loss 0.007290\n",
      ">> Epoch 88 finished \tANN training loss 0.007030\n",
      ">> Epoch 89 finished \tANN training loss 0.006491\n",
      ">> Epoch 90 finished \tANN training loss 0.006019\n",
      ">> Epoch 91 finished \tANN training loss 0.006182\n",
      ">> Epoch 92 finished \tANN training loss 0.006417\n",
      ">> Epoch 93 finished \tANN training loss 0.006092\n",
      ">> Epoch 94 finished \tANN training loss 0.005787\n",
      ">> Epoch 95 finished \tANN training loss 0.005550\n",
      ">> Epoch 96 finished \tANN training loss 0.006072\n",
      ">> Epoch 97 finished \tANN training loss 0.005957\n",
      ">> Epoch 98 finished \tANN training loss 0.005567\n",
      ">> Epoch 99 finished \tANN training loss 0.005239\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.910000\n"
     ]
    }
   ],
   "source": [
    "onelayer_acc[0] = deep_belief_net(hidden_layers_structure=[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.91\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(onelayer_acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting 2\n",
    "\n",
    "hidden_layers_structure=[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 45.414230\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 36.285255\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 43.517979\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 53.830082\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 38.417015\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 42.754887\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 61.840523\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 28.709108\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 43.236549\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 41.682289\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.792403\n",
      ">> Epoch 1 finished \tANN training loss 0.558377\n",
      ">> Epoch 2 finished \tANN training loss 0.454259\n",
      ">> Epoch 3 finished \tANN training loss 0.400325\n",
      ">> Epoch 4 finished \tANN training loss 0.373724\n",
      ">> Epoch 5 finished \tANN training loss 0.324249\n",
      ">> Epoch 6 finished \tANN training loss 0.283223\n",
      ">> Epoch 7 finished \tANN training loss 0.286118\n",
      ">> Epoch 8 finished \tANN training loss 0.252217\n",
      ">> Epoch 9 finished \tANN training loss 0.224174\n",
      ">> Epoch 10 finished \tANN training loss 0.208734\n",
      ">> Epoch 11 finished \tANN training loss 0.189222\n",
      ">> Epoch 12 finished \tANN training loss 0.174400\n",
      ">> Epoch 13 finished \tANN training loss 0.160516\n",
      ">> Epoch 14 finished \tANN training loss 0.147868\n",
      ">> Epoch 15 finished \tANN training loss 0.136332\n",
      ">> Epoch 16 finished \tANN training loss 0.134410\n",
      ">> Epoch 17 finished \tANN training loss 0.120683\n",
      ">> Epoch 18 finished \tANN training loss 0.108563\n",
      ">> Epoch 19 finished \tANN training loss 0.100035\n",
      ">> Epoch 20 finished \tANN training loss 0.094514\n",
      ">> Epoch 21 finished \tANN training loss 0.090196\n",
      ">> Epoch 22 finished \tANN training loss 0.083452\n",
      ">> Epoch 23 finished \tANN training loss 0.075955\n",
      ">> Epoch 24 finished \tANN training loss 0.073228\n",
      ">> Epoch 25 finished \tANN training loss 0.067497\n",
      ">> Epoch 26 finished \tANN training loss 0.061881\n",
      ">> Epoch 27 finished \tANN training loss 0.057142\n",
      ">> Epoch 28 finished \tANN training loss 0.054623\n",
      ">> Epoch 29 finished \tANN training loss 0.051979\n",
      ">> Epoch 30 finished \tANN training loss 0.046972\n",
      ">> Epoch 31 finished \tANN training loss 0.045256\n",
      ">> Epoch 32 finished \tANN training loss 0.041754\n",
      ">> Epoch 33 finished \tANN training loss 0.039661\n",
      ">> Epoch 34 finished \tANN training loss 0.038830\n",
      ">> Epoch 35 finished \tANN training loss 0.035795\n",
      ">> Epoch 36 finished \tANN training loss 0.032724\n",
      ">> Epoch 37 finished \tANN training loss 0.032022\n",
      ">> Epoch 38 finished \tANN training loss 0.030272\n",
      ">> Epoch 39 finished \tANN training loss 0.028746\n",
      ">> Epoch 40 finished \tANN training loss 0.026873\n",
      ">> Epoch 41 finished \tANN training loss 0.026083\n",
      ">> Epoch 42 finished \tANN training loss 0.026594\n",
      ">> Epoch 43 finished \tANN training loss 0.022998\n",
      ">> Epoch 44 finished \tANN training loss 0.022573\n",
      ">> Epoch 45 finished \tANN training loss 0.021450\n",
      ">> Epoch 46 finished \tANN training loss 0.020896\n",
      ">> Epoch 47 finished \tANN training loss 0.020846\n",
      ">> Epoch 48 finished \tANN training loss 0.018584\n",
      ">> Epoch 49 finished \tANN training loss 0.018938\n",
      ">> Epoch 50 finished \tANN training loss 0.016996\n",
      ">> Epoch 51 finished \tANN training loss 0.017308\n",
      ">> Epoch 52 finished \tANN training loss 0.015784\n",
      ">> Epoch 53 finished \tANN training loss 0.015339\n",
      ">> Epoch 54 finished \tANN training loss 0.014552\n",
      ">> Epoch 55 finished \tANN training loss 0.013740\n",
      ">> Epoch 56 finished \tANN training loss 0.014268\n",
      ">> Epoch 57 finished \tANN training loss 0.012771\n",
      ">> Epoch 58 finished \tANN training loss 0.012065\n",
      ">> Epoch 59 finished \tANN training loss 0.011734\n",
      ">> Epoch 60 finished \tANN training loss 0.012192\n",
      ">> Epoch 61 finished \tANN training loss 0.011301\n",
      ">> Epoch 62 finished \tANN training loss 0.010572\n",
      ">> Epoch 63 finished \tANN training loss 0.010339\n",
      ">> Epoch 64 finished \tANN training loss 0.010474\n",
      ">> Epoch 65 finished \tANN training loss 0.010081\n",
      ">> Epoch 66 finished \tANN training loss 0.009656\n",
      ">> Epoch 67 finished \tANN training loss 0.009271\n",
      ">> Epoch 68 finished \tANN training loss 0.009088\n",
      ">> Epoch 69 finished \tANN training loss 0.009002\n",
      ">> Epoch 70 finished \tANN training loss 0.008970\n",
      ">> Epoch 71 finished \tANN training loss 0.008535\n",
      ">> Epoch 72 finished \tANN training loss 0.008833\n",
      ">> Epoch 73 finished \tANN training loss 0.008431\n",
      ">> Epoch 74 finished \tANN training loss 0.007508\n",
      ">> Epoch 75 finished \tANN training loss 0.007415\n",
      ">> Epoch 76 finished \tANN training loss 0.007602\n",
      ">> Epoch 77 finished \tANN training loss 0.007047\n",
      ">> Epoch 78 finished \tANN training loss 0.006930\n",
      ">> Epoch 79 finished \tANN training loss 0.006668\n",
      ">> Epoch 80 finished \tANN training loss 0.006308\n",
      ">> Epoch 81 finished \tANN training loss 0.006420\n",
      ">> Epoch 82 finished \tANN training loss 0.006179\n",
      ">> Epoch 83 finished \tANN training loss 0.006544\n",
      ">> Epoch 84 finished \tANN training loss 0.005859\n",
      ">> Epoch 85 finished \tANN training loss 0.005690\n",
      ">> Epoch 86 finished \tANN training loss 0.005678\n",
      ">> Epoch 87 finished \tANN training loss 0.005546\n",
      ">> Epoch 88 finished \tANN training loss 0.005765\n",
      ">> Epoch 89 finished \tANN training loss 0.005562\n",
      ">> Epoch 90 finished \tANN training loss 0.005201\n",
      ">> Epoch 91 finished \tANN training loss 0.005090\n",
      ">> Epoch 92 finished \tANN training loss 0.004959\n",
      ">> Epoch 93 finished \tANN training loss 0.004830\n",
      ">> Epoch 94 finished \tANN training loss 0.005204\n",
      ">> Epoch 95 finished \tANN training loss 0.004768\n",
      ">> Epoch 96 finished \tANN training loss 0.004679\n",
      ">> Epoch 97 finished \tANN training loss 0.004691\n",
      ">> Epoch 98 finished \tANN training loss 0.004748\n",
      ">> Epoch 99 finished \tANN training loss 0.004642\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.915000\n"
     ]
    }
   ],
   "source": [
    "onelayer_acc[1] = deep_belief_net(hidden_layers_structure=[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.915\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(onelayer_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting 3\n",
    "\n",
    "hidden_layers_structure=[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 77.476326\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 94.029198\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 40.229782\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 43.480961\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 65.467094\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 40.833336\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 31.919632\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 41.051559\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 38.219448\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 46.930855\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.783310\n",
      ">> Epoch 1 finished \tANN training loss 0.549260\n",
      ">> Epoch 2 finished \tANN training loss 0.463997\n",
      ">> Epoch 3 finished \tANN training loss 0.396184\n",
      ">> Epoch 4 finished \tANN training loss 0.352683\n",
      ">> Epoch 5 finished \tANN training loss 0.333062\n",
      ">> Epoch 6 finished \tANN training loss 0.289539\n",
      ">> Epoch 7 finished \tANN training loss 0.270611\n",
      ">> Epoch 8 finished \tANN training loss 0.246243\n",
      ">> Epoch 9 finished \tANN training loss 0.225119\n",
      ">> Epoch 10 finished \tANN training loss 0.213675\n",
      ">> Epoch 11 finished \tANN training loss 0.202141\n",
      ">> Epoch 12 finished \tANN training loss 0.188057\n",
      ">> Epoch 13 finished \tANN training loss 0.163305\n",
      ">> Epoch 14 finished \tANN training loss 0.156307\n",
      ">> Epoch 15 finished \tANN training loss 0.137869\n",
      ">> Epoch 16 finished \tANN training loss 0.131596\n",
      ">> Epoch 17 finished \tANN training loss 0.125882\n",
      ">> Epoch 18 finished \tANN training loss 0.114390\n",
      ">> Epoch 19 finished \tANN training loss 0.105198\n",
      ">> Epoch 20 finished \tANN training loss 0.101032\n",
      ">> Epoch 21 finished \tANN training loss 0.089239\n",
      ">> Epoch 22 finished \tANN training loss 0.085690\n",
      ">> Epoch 23 finished \tANN training loss 0.085752\n",
      ">> Epoch 24 finished \tANN training loss 0.075805\n",
      ">> Epoch 25 finished \tANN training loss 0.071583\n",
      ">> Epoch 26 finished \tANN training loss 0.067185\n",
      ">> Epoch 27 finished \tANN training loss 0.060088\n",
      ">> Epoch 28 finished \tANN training loss 0.058456\n",
      ">> Epoch 29 finished \tANN training loss 0.052903\n",
      ">> Epoch 30 finished \tANN training loss 0.052745\n",
      ">> Epoch 31 finished \tANN training loss 0.047205\n",
      ">> Epoch 32 finished \tANN training loss 0.046610\n",
      ">> Epoch 33 finished \tANN training loss 0.043999\n",
      ">> Epoch 34 finished \tANN training loss 0.041643\n",
      ">> Epoch 35 finished \tANN training loss 0.037786\n",
      ">> Epoch 36 finished \tANN training loss 0.034954\n",
      ">> Epoch 37 finished \tANN training loss 0.033461\n",
      ">> Epoch 38 finished \tANN training loss 0.033022\n",
      ">> Epoch 39 finished \tANN training loss 0.034786\n",
      ">> Epoch 40 finished \tANN training loss 0.030337\n",
      ">> Epoch 41 finished \tANN training loss 0.027136\n",
      ">> Epoch 42 finished \tANN training loss 0.026427\n",
      ">> Epoch 43 finished \tANN training loss 0.028217\n",
      ">> Epoch 44 finished \tANN training loss 0.024056\n",
      ">> Epoch 45 finished \tANN training loss 0.024212\n",
      ">> Epoch 46 finished \tANN training loss 0.023414\n",
      ">> Epoch 47 finished \tANN training loss 0.023089\n",
      ">> Epoch 48 finished \tANN training loss 0.020701\n",
      ">> Epoch 49 finished \tANN training loss 0.019389\n",
      ">> Epoch 50 finished \tANN training loss 0.019352\n",
      ">> Epoch 51 finished \tANN training loss 0.018409\n",
      ">> Epoch 52 finished \tANN training loss 0.017390\n",
      ">> Epoch 53 finished \tANN training loss 0.016837\n",
      ">> Epoch 54 finished \tANN training loss 0.016487\n",
      ">> Epoch 55 finished \tANN training loss 0.015638\n",
      ">> Epoch 56 finished \tANN training loss 0.014420\n",
      ">> Epoch 57 finished \tANN training loss 0.014166\n",
      ">> Epoch 58 finished \tANN training loss 0.013992\n",
      ">> Epoch 59 finished \tANN training loss 0.013447\n",
      ">> Epoch 60 finished \tANN training loss 0.013118\n",
      ">> Epoch 61 finished \tANN training loss 0.012236\n",
      ">> Epoch 62 finished \tANN training loss 0.011784\n",
      ">> Epoch 63 finished \tANN training loss 0.011592\n",
      ">> Epoch 64 finished \tANN training loss 0.011207\n",
      ">> Epoch 65 finished \tANN training loss 0.011314\n",
      ">> Epoch 66 finished \tANN training loss 0.010311\n",
      ">> Epoch 67 finished \tANN training loss 0.009886\n",
      ">> Epoch 68 finished \tANN training loss 0.010130\n",
      ">> Epoch 69 finished \tANN training loss 0.009324\n",
      ">> Epoch 70 finished \tANN training loss 0.009377\n",
      ">> Epoch 71 finished \tANN training loss 0.008784\n",
      ">> Epoch 72 finished \tANN training loss 0.008783\n",
      ">> Epoch 73 finished \tANN training loss 0.008712\n",
      ">> Epoch 74 finished \tANN training loss 0.008220\n",
      ">> Epoch 75 finished \tANN training loss 0.009286\n",
      ">> Epoch 76 finished \tANN training loss 0.008008\n",
      ">> Epoch 77 finished \tANN training loss 0.007511\n",
      ">> Epoch 78 finished \tANN training loss 0.007517\n",
      ">> Epoch 79 finished \tANN training loss 0.007242\n",
      ">> Epoch 80 finished \tANN training loss 0.007096\n",
      ">> Epoch 81 finished \tANN training loss 0.006769\n",
      ">> Epoch 82 finished \tANN training loss 0.006714\n",
      ">> Epoch 83 finished \tANN training loss 0.006599\n",
      ">> Epoch 84 finished \tANN training loss 0.006354\n",
      ">> Epoch 85 finished \tANN training loss 0.006042\n",
      ">> Epoch 86 finished \tANN training loss 0.005984\n",
      ">> Epoch 87 finished \tANN training loss 0.005717\n",
      ">> Epoch 88 finished \tANN training loss 0.005882\n",
      ">> Epoch 89 finished \tANN training loss 0.005977\n",
      ">> Epoch 90 finished \tANN training loss 0.005671\n",
      ">> Epoch 91 finished \tANN training loss 0.005543\n",
      ">> Epoch 92 finished \tANN training loss 0.005214\n",
      ">> Epoch 93 finished \tANN training loss 0.005137\n",
      ">> Epoch 94 finished \tANN training loss 0.005082\n",
      ">> Epoch 95 finished \tANN training loss 0.005186\n",
      ">> Epoch 96 finished \tANN training loss 0.004884\n",
      ">> Epoch 97 finished \tANN training loss 0.004751\n",
      ">> Epoch 98 finished \tANN training loss 0.004773\n",
      ">> Epoch 99 finished \tANN training loss 0.004624\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.910000\n"
     ]
    }
   ],
   "source": [
    "onelayer_acc[2] = deep_belief_net(hidden_layers_structure=[300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.91\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(onelayer_acc[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting 4\n",
    "\n",
    "hidden_layers_structure=[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 76.508209\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 55.080544\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 38.907104\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 47.255497\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 56.742077\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 38.577198\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 55.470825\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 46.093624\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 36.421066\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 32.891293\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.829531\n",
      ">> Epoch 1 finished \tANN training loss 0.570312\n",
      ">> Epoch 2 finished \tANN training loss 0.475225\n",
      ">> Epoch 3 finished \tANN training loss 0.415622\n",
      ">> Epoch 4 finished \tANN training loss 0.361415\n",
      ">> Epoch 5 finished \tANN training loss 0.324670\n",
      ">> Epoch 6 finished \tANN training loss 0.295907\n",
      ">> Epoch 7 finished \tANN training loss 0.275827\n",
      ">> Epoch 8 finished \tANN training loss 0.254593\n",
      ">> Epoch 9 finished \tANN training loss 0.235754\n",
      ">> Epoch 10 finished \tANN training loss 0.222178\n",
      ">> Epoch 11 finished \tANN training loss 0.200712\n",
      ">> Epoch 12 finished \tANN training loss 0.179394\n",
      ">> Epoch 13 finished \tANN training loss 0.167557\n",
      ">> Epoch 14 finished \tANN training loss 0.157992\n",
      ">> Epoch 15 finished \tANN training loss 0.152153\n",
      ">> Epoch 16 finished \tANN training loss 0.139974\n",
      ">> Epoch 17 finished \tANN training loss 0.122666\n",
      ">> Epoch 18 finished \tANN training loss 0.115121\n",
      ">> Epoch 19 finished \tANN training loss 0.105786\n",
      ">> Epoch 20 finished \tANN training loss 0.097660\n",
      ">> Epoch 21 finished \tANN training loss 0.091725\n",
      ">> Epoch 22 finished \tANN training loss 0.084845\n",
      ">> Epoch 23 finished \tANN training loss 0.081502\n",
      ">> Epoch 24 finished \tANN training loss 0.074462\n",
      ">> Epoch 25 finished \tANN training loss 0.069918\n",
      ">> Epoch 26 finished \tANN training loss 0.064744\n",
      ">> Epoch 27 finished \tANN training loss 0.061617\n",
      ">> Epoch 28 finished \tANN training loss 0.056188\n",
      ">> Epoch 29 finished \tANN training loss 0.055571\n",
      ">> Epoch 30 finished \tANN training loss 0.051942\n",
      ">> Epoch 31 finished \tANN training loss 0.049073\n",
      ">> Epoch 32 finished \tANN training loss 0.043022\n",
      ">> Epoch 33 finished \tANN training loss 0.041094\n",
      ">> Epoch 34 finished \tANN training loss 0.038909\n",
      ">> Epoch 35 finished \tANN training loss 0.036306\n",
      ">> Epoch 36 finished \tANN training loss 0.035280\n",
      ">> Epoch 37 finished \tANN training loss 0.033102\n",
      ">> Epoch 38 finished \tANN training loss 0.031871\n",
      ">> Epoch 39 finished \tANN training loss 0.029242\n",
      ">> Epoch 40 finished \tANN training loss 0.028371\n",
      ">> Epoch 41 finished \tANN training loss 0.026208\n",
      ">> Epoch 42 finished \tANN training loss 0.025588\n",
      ">> Epoch 43 finished \tANN training loss 0.024393\n",
      ">> Epoch 44 finished \tANN training loss 0.023542\n",
      ">> Epoch 45 finished \tANN training loss 0.023028\n",
      ">> Epoch 46 finished \tANN training loss 0.021649\n",
      ">> Epoch 47 finished \tANN training loss 0.020691\n",
      ">> Epoch 48 finished \tANN training loss 0.019979\n",
      ">> Epoch 49 finished \tANN training loss 0.018928\n",
      ">> Epoch 50 finished \tANN training loss 0.017665\n",
      ">> Epoch 51 finished \tANN training loss 0.017382\n",
      ">> Epoch 52 finished \tANN training loss 0.016298\n",
      ">> Epoch 53 finished \tANN training loss 0.015319\n",
      ">> Epoch 54 finished \tANN training loss 0.014617\n",
      ">> Epoch 55 finished \tANN training loss 0.014130\n",
      ">> Epoch 56 finished \tANN training loss 0.013414\n",
      ">> Epoch 57 finished \tANN training loss 0.013307\n",
      ">> Epoch 58 finished \tANN training loss 0.012758\n",
      ">> Epoch 59 finished \tANN training loss 0.012102\n",
      ">> Epoch 60 finished \tANN training loss 0.013135\n",
      ">> Epoch 61 finished \tANN training loss 0.011928\n",
      ">> Epoch 62 finished \tANN training loss 0.011289\n",
      ">> Epoch 63 finished \tANN training loss 0.010563\n",
      ">> Epoch 64 finished \tANN training loss 0.010779\n",
      ">> Epoch 65 finished \tANN training loss 0.010019\n",
      ">> Epoch 66 finished \tANN training loss 0.010024\n",
      ">> Epoch 67 finished \tANN training loss 0.009669\n",
      ">> Epoch 68 finished \tANN training loss 0.009116\n",
      ">> Epoch 69 finished \tANN training loss 0.008755\n",
      ">> Epoch 70 finished \tANN training loss 0.008667\n",
      ">> Epoch 71 finished \tANN training loss 0.008438\n",
      ">> Epoch 72 finished \tANN training loss 0.008046\n",
      ">> Epoch 73 finished \tANN training loss 0.007994\n",
      ">> Epoch 74 finished \tANN training loss 0.008057\n",
      ">> Epoch 75 finished \tANN training loss 0.007416\n",
      ">> Epoch 76 finished \tANN training loss 0.007250\n",
      ">> Epoch 77 finished \tANN training loss 0.007219\n",
      ">> Epoch 78 finished \tANN training loss 0.007242\n",
      ">> Epoch 79 finished \tANN training loss 0.007041\n",
      ">> Epoch 80 finished \tANN training loss 0.006762\n",
      ">> Epoch 81 finished \tANN training loss 0.006415\n",
      ">> Epoch 82 finished \tANN training loss 0.006224\n",
      ">> Epoch 83 finished \tANN training loss 0.006599\n",
      ">> Epoch 84 finished \tANN training loss 0.005863\n",
      ">> Epoch 85 finished \tANN training loss 0.006003\n",
      ">> Epoch 86 finished \tANN training loss 0.005592\n",
      ">> Epoch 87 finished \tANN training loss 0.005977\n",
      ">> Epoch 88 finished \tANN training loss 0.005370\n",
      ">> Epoch 89 finished \tANN training loss 0.005424\n",
      ">> Epoch 90 finished \tANN training loss 0.005224\n",
      ">> Epoch 91 finished \tANN training loss 0.004995\n",
      ">> Epoch 92 finished \tANN training loss 0.004827\n",
      ">> Epoch 93 finished \tANN training loss 0.004883\n",
      ">> Epoch 94 finished \tANN training loss 0.004543\n",
      ">> Epoch 95 finished \tANN training loss 0.004558\n",
      ">> Epoch 96 finished \tANN training loss 0.004311\n",
      ">> Epoch 97 finished \tANN training loss 0.004562\n",
      ">> Epoch 98 finished \tANN training loss 0.004251\n",
      ">> Epoch 99 finished \tANN training loss 0.004214\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.925000\n"
     ]
    }
   ],
   "source": [
    "onelayer_acc[3] = deep_belief_net(hidden_layers_structure=[400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.925\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(onelayer_acc[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting 5\n",
    "\n",
    "hidden_layers_structure=[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 56.192665\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 49.764816\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 39.970650\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 47.955738\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 38.048859\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 36.557178\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 22.669106\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 48.748360\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 57.597557\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 32.826927\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.807150\n",
      ">> Epoch 1 finished \tANN training loss 0.564415\n",
      ">> Epoch 2 finished \tANN training loss 0.470352\n",
      ">> Epoch 3 finished \tANN training loss 0.396800\n",
      ">> Epoch 4 finished \tANN training loss 0.366528\n",
      ">> Epoch 5 finished \tANN training loss 0.313878\n",
      ">> Epoch 6 finished \tANN training loss 0.300360\n",
      ">> Epoch 7 finished \tANN training loss 0.273658\n",
      ">> Epoch 8 finished \tANN training loss 0.244247\n",
      ">> Epoch 9 finished \tANN training loss 0.226939\n",
      ">> Epoch 10 finished \tANN training loss 0.216623\n",
      ">> Epoch 11 finished \tANN training loss 0.193205\n",
      ">> Epoch 12 finished \tANN training loss 0.183846\n",
      ">> Epoch 13 finished \tANN training loss 0.165159\n",
      ">> Epoch 14 finished \tANN training loss 0.158479\n",
      ">> Epoch 15 finished \tANN training loss 0.142269\n",
      ">> Epoch 16 finished \tANN training loss 0.130295\n",
      ">> Epoch 17 finished \tANN training loss 0.121850\n",
      ">> Epoch 18 finished \tANN training loss 0.110316\n",
      ">> Epoch 19 finished \tANN training loss 0.103539\n",
      ">> Epoch 20 finished \tANN training loss 0.102060\n",
      ">> Epoch 21 finished \tANN training loss 0.086728\n",
      ">> Epoch 22 finished \tANN training loss 0.084157\n",
      ">> Epoch 23 finished \tANN training loss 0.079235\n",
      ">> Epoch 24 finished \tANN training loss 0.069335\n",
      ">> Epoch 25 finished \tANN training loss 0.065721\n",
      ">> Epoch 26 finished \tANN training loss 0.062261\n",
      ">> Epoch 27 finished \tANN training loss 0.057246\n",
      ">> Epoch 28 finished \tANN training loss 0.054196\n",
      ">> Epoch 29 finished \tANN training loss 0.050689\n",
      ">> Epoch 30 finished \tANN training loss 0.048315\n",
      ">> Epoch 31 finished \tANN training loss 0.044980\n",
      ">> Epoch 32 finished \tANN training loss 0.041336\n",
      ">> Epoch 33 finished \tANN training loss 0.039494\n",
      ">> Epoch 34 finished \tANN training loss 0.037713\n",
      ">> Epoch 35 finished \tANN training loss 0.035863\n",
      ">> Epoch 36 finished \tANN training loss 0.034670\n",
      ">> Epoch 37 finished \tANN training loss 0.031809\n",
      ">> Epoch 38 finished \tANN training loss 0.030921\n",
      ">> Epoch 39 finished \tANN training loss 0.030657\n",
      ">> Epoch 40 finished \tANN training loss 0.027944\n",
      ">> Epoch 41 finished \tANN training loss 0.025821\n",
      ">> Epoch 42 finished \tANN training loss 0.025341\n",
      ">> Epoch 43 finished \tANN training loss 0.024332\n",
      ">> Epoch 44 finished \tANN training loss 0.022425\n",
      ">> Epoch 45 finished \tANN training loss 0.021186\n",
      ">> Epoch 46 finished \tANN training loss 0.020227\n",
      ">> Epoch 47 finished \tANN training loss 0.019697\n",
      ">> Epoch 48 finished \tANN training loss 0.020635\n",
      ">> Epoch 49 finished \tANN training loss 0.018131\n",
      ">> Epoch 50 finished \tANN training loss 0.017273\n",
      ">> Epoch 51 finished \tANN training loss 0.016298\n",
      ">> Epoch 52 finished \tANN training loss 0.017280\n",
      ">> Epoch 53 finished \tANN training loss 0.015914\n",
      ">> Epoch 54 finished \tANN training loss 0.016130\n",
      ">> Epoch 55 finished \tANN training loss 0.014796\n",
      ">> Epoch 56 finished \tANN training loss 0.014006\n",
      ">> Epoch 57 finished \tANN training loss 0.013298\n",
      ">> Epoch 58 finished \tANN training loss 0.013001\n",
      ">> Epoch 59 finished \tANN training loss 0.012679\n",
      ">> Epoch 60 finished \tANN training loss 0.011761\n",
      ">> Epoch 61 finished \tANN training loss 0.012224\n",
      ">> Epoch 62 finished \tANN training loss 0.011151\n",
      ">> Epoch 63 finished \tANN training loss 0.010751\n",
      ">> Epoch 64 finished \tANN training loss 0.011168\n",
      ">> Epoch 65 finished \tANN training loss 0.010703\n",
      ">> Epoch 66 finished \tANN training loss 0.010149\n",
      ">> Epoch 67 finished \tANN training loss 0.009659\n",
      ">> Epoch 68 finished \tANN training loss 0.009157\n",
      ">> Epoch 69 finished \tANN training loss 0.008897\n",
      ">> Epoch 70 finished \tANN training loss 0.008683\n",
      ">> Epoch 71 finished \tANN training loss 0.008385\n",
      ">> Epoch 72 finished \tANN training loss 0.008611\n",
      ">> Epoch 73 finished \tANN training loss 0.008355\n",
      ">> Epoch 74 finished \tANN training loss 0.007766\n",
      ">> Epoch 75 finished \tANN training loss 0.007492\n",
      ">> Epoch 76 finished \tANN training loss 0.007223\n",
      ">> Epoch 77 finished \tANN training loss 0.007085\n",
      ">> Epoch 78 finished \tANN training loss 0.006592\n",
      ">> Epoch 79 finished \tANN training loss 0.006890\n",
      ">> Epoch 80 finished \tANN training loss 0.006434\n",
      ">> Epoch 81 finished \tANN training loss 0.006338\n",
      ">> Epoch 82 finished \tANN training loss 0.006357\n",
      ">> Epoch 83 finished \tANN training loss 0.005957\n",
      ">> Epoch 84 finished \tANN training loss 0.005783\n",
      ">> Epoch 85 finished \tANN training loss 0.005643\n",
      ">> Epoch 86 finished \tANN training loss 0.005788\n",
      ">> Epoch 87 finished \tANN training loss 0.005514\n",
      ">> Epoch 88 finished \tANN training loss 0.005403\n",
      ">> Epoch 89 finished \tANN training loss 0.005462\n",
      ">> Epoch 90 finished \tANN training loss 0.005035\n",
      ">> Epoch 91 finished \tANN training loss 0.005009\n",
      ">> Epoch 92 finished \tANN training loss 0.004970\n",
      ">> Epoch 93 finished \tANN training loss 0.004813\n",
      ">> Epoch 94 finished \tANN training loss 0.004701\n",
      ">> Epoch 95 finished \tANN training loss 0.004578\n",
      ">> Epoch 96 finished \tANN training loss 0.004513\n",
      ">> Epoch 97 finished \tANN training loss 0.004193\n",
      ">> Epoch 98 finished \tANN training loss 0.004197\n",
      ">> Epoch 99 finished \tANN training loss 0.004290\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.920000\n"
     ]
    }
   ],
   "source": [
    "onelayer_acc[4] = deep_belief_net(hidden_layers_structure=[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.92\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(onelayer_acc[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.91, 0.915, 0.91, 0.925, 0.92]\n",
      "Most accurate 1-layer setting is Setting 4\n"
     ]
    }
   ],
   "source": [
    "# Collated results\n",
    "print(onelayer_acc)\n",
    "print('Most accurate 1-layer setting is Setting ' + str(onelayer_acc.index(max(onelayer_acc)) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create bar graph\n",
    "plt.clf()\n",
    "objects = ('[100]', '[200]', '[300]', '[400]', '[500]')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [0.9,0.91,0.91,0.905,0.885]\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.xlabel('Layer Settings')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('One-layer Settings')\n",
    " \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different 2-layer structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store accuracies\n",
    "twolayer_acc = [0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing layer_1=100 performance:\n",
    "\n",
    "Setting 1  (100) on 200\n",
    "\n",
    "hidden_layers_structure=[100, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 79.374649\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 67.140266\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 62.985001\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 75.959839\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 86.681122\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 55.794071\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 61.128662\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 59.591640\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 77.757370\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 70.530952\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 267.905792\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 341.416626\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 348.704132\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 510.415222\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 408.542725\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 493.022064\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 440.875000\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 531.033386\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 607.113220\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 495.345459\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.678231\n",
      ">> Epoch 1 finished \tANN training loss 0.534832\n",
      ">> Epoch 2 finished \tANN training loss 0.413190\n",
      ">> Epoch 3 finished \tANN training loss 0.367403\n",
      ">> Epoch 4 finished \tANN training loss 0.308878\n",
      ">> Epoch 5 finished \tANN training loss 0.268009\n",
      ">> Epoch 6 finished \tANN training loss 0.262234\n",
      ">> Epoch 7 finished \tANN training loss 0.237101\n",
      ">> Epoch 8 finished \tANN training loss 0.212039\n",
      ">> Epoch 9 finished \tANN training loss 0.182842\n",
      ">> Epoch 10 finished \tANN training loss 0.165388\n",
      ">> Epoch 11 finished \tANN training loss 0.150437\n",
      ">> Epoch 12 finished \tANN training loss 0.136103\n",
      ">> Epoch 13 finished \tANN training loss 0.133416\n",
      ">> Epoch 14 finished \tANN training loss 0.115399\n",
      ">> Epoch 15 finished \tANN training loss 0.116453\n",
      ">> Epoch 16 finished \tANN training loss 0.096186\n",
      ">> Epoch 17 finished \tANN training loss 0.090626\n",
      ">> Epoch 18 finished \tANN training loss 0.080112\n",
      ">> Epoch 19 finished \tANN training loss 0.070163\n",
      ">> Epoch 20 finished \tANN training loss 0.065557\n",
      ">> Epoch 21 finished \tANN training loss 0.064237\n",
      ">> Epoch 22 finished \tANN training loss 0.058382\n",
      ">> Epoch 23 finished \tANN training loss 0.052497\n",
      ">> Epoch 24 finished \tANN training loss 0.046720\n",
      ">> Epoch 25 finished \tANN training loss 0.044644\n",
      ">> Epoch 26 finished \tANN training loss 0.046889\n",
      ">> Epoch 27 finished \tANN training loss 0.035678\n",
      ">> Epoch 28 finished \tANN training loss 0.036833\n",
      ">> Epoch 29 finished \tANN training loss 0.037246\n",
      ">> Epoch 30 finished \tANN training loss 0.030282\n",
      ">> Epoch 31 finished \tANN training loss 0.027454\n",
      ">> Epoch 32 finished \tANN training loss 0.023638\n",
      ">> Epoch 33 finished \tANN training loss 0.027469\n",
      ">> Epoch 34 finished \tANN training loss 0.022819\n",
      ">> Epoch 35 finished \tANN training loss 0.024739\n",
      ">> Epoch 36 finished \tANN training loss 0.025703\n",
      ">> Epoch 37 finished \tANN training loss 0.018306\n",
      ">> Epoch 38 finished \tANN training loss 0.018302\n",
      ">> Epoch 39 finished \tANN training loss 0.017651\n",
      ">> Epoch 40 finished \tANN training loss 0.015268\n",
      ">> Epoch 41 finished \tANN training loss 0.013655\n",
      ">> Epoch 42 finished \tANN training loss 0.013784\n",
      ">> Epoch 43 finished \tANN training loss 0.012985\n",
      ">> Epoch 44 finished \tANN training loss 0.013676\n",
      ">> Epoch 45 finished \tANN training loss 0.010109\n",
      ">> Epoch 46 finished \tANN training loss 0.011957\n",
      ">> Epoch 47 finished \tANN training loss 0.010025\n",
      ">> Epoch 48 finished \tANN training loss 0.010394\n",
      ">> Epoch 49 finished \tANN training loss 0.010651\n",
      ">> Epoch 50 finished \tANN training loss 0.009239\n",
      ">> Epoch 51 finished \tANN training loss 0.009096\n",
      ">> Epoch 52 finished \tANN training loss 0.008305\n",
      ">> Epoch 53 finished \tANN training loss 0.007778\n",
      ">> Epoch 54 finished \tANN training loss 0.007623\n",
      ">> Epoch 55 finished \tANN training loss 0.008029\n",
      ">> Epoch 56 finished \tANN training loss 0.006515\n",
      ">> Epoch 57 finished \tANN training loss 0.007940\n",
      ">> Epoch 58 finished \tANN training loss 0.007083\n",
      ">> Epoch 59 finished \tANN training loss 0.006657\n",
      ">> Epoch 60 finished \tANN training loss 0.005659\n",
      ">> Epoch 61 finished \tANN training loss 0.005713\n",
      ">> Epoch 62 finished \tANN training loss 0.005602\n",
      ">> Epoch 63 finished \tANN training loss 0.004602\n",
      ">> Epoch 64 finished \tANN training loss 0.004061\n",
      ">> Epoch 65 finished \tANN training loss 0.004606\n",
      ">> Epoch 66 finished \tANN training loss 0.003558\n",
      ">> Epoch 67 finished \tANN training loss 0.004126\n",
      ">> Epoch 68 finished \tANN training loss 0.004720\n",
      ">> Epoch 69 finished \tANN training loss 0.003774\n",
      ">> Epoch 70 finished \tANN training loss 0.003213\n",
      ">> Epoch 71 finished \tANN training loss 0.004063\n",
      ">> Epoch 72 finished \tANN training loss 0.004298\n",
      ">> Epoch 73 finished \tANN training loss 0.003465\n",
      ">> Epoch 74 finished \tANN training loss 0.003252\n",
      ">> Epoch 75 finished \tANN training loss 0.004233\n",
      ">> Epoch 76 finished \tANN training loss 0.003266\n",
      ">> Epoch 77 finished \tANN training loss 0.002727\n",
      ">> Epoch 78 finished \tANN training loss 0.002284\n",
      ">> Epoch 79 finished \tANN training loss 0.002393\n",
      ">> Epoch 80 finished \tANN training loss 0.003297\n",
      ">> Epoch 81 finished \tANN training loss 0.002788\n",
      ">> Epoch 82 finished \tANN training loss 0.002249\n",
      ">> Epoch 83 finished \tANN training loss 0.002132\n",
      ">> Epoch 84 finished \tANN training loss 0.002154\n",
      ">> Epoch 85 finished \tANN training loss 0.002170\n",
      ">> Epoch 86 finished \tANN training loss 0.001731\n",
      ">> Epoch 87 finished \tANN training loss 0.001930\n",
      ">> Epoch 88 finished \tANN training loss 0.001708\n",
      ">> Epoch 89 finished \tANN training loss 0.001665\n",
      ">> Epoch 90 finished \tANN training loss 0.001693\n",
      ">> Epoch 91 finished \tANN training loss 0.001914\n",
      ">> Epoch 92 finished \tANN training loss 0.002027\n",
      ">> Epoch 93 finished \tANN training loss 0.001719\n",
      ">> Epoch 94 finished \tANN training loss 0.001769\n",
      ">> Epoch 95 finished \tANN training loss 0.001749\n",
      ">> Epoch 96 finished \tANN training loss 0.001628\n",
      ">> Epoch 97 finished \tANN training loss 0.001885\n",
      ">> Epoch 98 finished \tANN training loss 0.001687\n",
      ">> Epoch 99 finished \tANN training loss 0.001623\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.915000\n"
     ]
    }
   ],
   "source": [
    "twolayer_acc[0] = deep_belief_net(hidden_layers_structure=[100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.915\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(twolayer_acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 2  (100) on 500\n",
    "\n",
    "hidden_layers_structure=[100, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 46.900723\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 58.844337\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 43.177505\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 51.222061\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 61.009644\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 48.352509\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 56.135422\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 62.211838\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 54.295658\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 60.772980\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 267.859772\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 225.061951\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 188.880936\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 295.289886\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 238.730621\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 196.554642\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 390.598724\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 300.621643\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 192.369415\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 241.131882\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.655090\n",
      ">> Epoch 1 finished \tANN training loss 0.519671\n",
      ">> Epoch 2 finished \tANN training loss 0.424776\n",
      ">> Epoch 3 finished \tANN training loss 0.344556\n",
      ">> Epoch 4 finished \tANN training loss 0.294002\n",
      ">> Epoch 5 finished \tANN training loss 0.259555\n",
      ">> Epoch 6 finished \tANN training loss 0.227636\n",
      ">> Epoch 7 finished \tANN training loss 0.226134\n",
      ">> Epoch 8 finished \tANN training loss 0.192655\n",
      ">> Epoch 9 finished \tANN training loss 0.170256\n",
      ">> Epoch 10 finished \tANN training loss 0.151603\n",
      ">> Epoch 11 finished \tANN training loss 0.148299\n",
      ">> Epoch 12 finished \tANN training loss 0.130624\n",
      ">> Epoch 13 finished \tANN training loss 0.122690\n",
      ">> Epoch 14 finished \tANN training loss 0.105357\n",
      ">> Epoch 15 finished \tANN training loss 0.094765\n",
      ">> Epoch 16 finished \tANN training loss 0.091541\n",
      ">> Epoch 17 finished \tANN training loss 0.078635\n",
      ">> Epoch 18 finished \tANN training loss 0.075306\n",
      ">> Epoch 19 finished \tANN training loss 0.069898\n",
      ">> Epoch 20 finished \tANN training loss 0.060313\n",
      ">> Epoch 21 finished \tANN training loss 0.058699\n",
      ">> Epoch 22 finished \tANN training loss 0.059849\n",
      ">> Epoch 23 finished \tANN training loss 0.047306\n",
      ">> Epoch 24 finished \tANN training loss 0.040965\n",
      ">> Epoch 25 finished \tANN training loss 0.044648\n",
      ">> Epoch 26 finished \tANN training loss 0.039472\n",
      ">> Epoch 27 finished \tANN training loss 0.035901\n",
      ">> Epoch 28 finished \tANN training loss 0.035944\n",
      ">> Epoch 29 finished \tANN training loss 0.027704\n",
      ">> Epoch 30 finished \tANN training loss 0.028995\n",
      ">> Epoch 31 finished \tANN training loss 0.029808\n",
      ">> Epoch 32 finished \tANN training loss 0.027962\n",
      ">> Epoch 33 finished \tANN training loss 0.024353\n",
      ">> Epoch 34 finished \tANN training loss 0.023927\n",
      ">> Epoch 35 finished \tANN training loss 0.022628\n",
      ">> Epoch 36 finished \tANN training loss 0.021696\n",
      ">> Epoch 37 finished \tANN training loss 0.018049\n",
      ">> Epoch 38 finished \tANN training loss 0.016429\n",
      ">> Epoch 39 finished \tANN training loss 0.015042\n",
      ">> Epoch 40 finished \tANN training loss 0.015295\n",
      ">> Epoch 41 finished \tANN training loss 0.013337\n",
      ">> Epoch 42 finished \tANN training loss 0.012755\n",
      ">> Epoch 43 finished \tANN training loss 0.012167\n",
      ">> Epoch 44 finished \tANN training loss 0.011956\n",
      ">> Epoch 45 finished \tANN training loss 0.010398\n",
      ">> Epoch 46 finished \tANN training loss 0.010038\n",
      ">> Epoch 47 finished \tANN training loss 0.010485\n",
      ">> Epoch 48 finished \tANN training loss 0.010167\n",
      ">> Epoch 49 finished \tANN training loss 0.009580\n",
      ">> Epoch 50 finished \tANN training loss 0.008789\n",
      ">> Epoch 51 finished \tANN training loss 0.008090\n",
      ">> Epoch 52 finished \tANN training loss 0.007969\n",
      ">> Epoch 53 finished \tANN training loss 0.007156\n",
      ">> Epoch 54 finished \tANN training loss 0.006996\n",
      ">> Epoch 55 finished \tANN training loss 0.006067\n",
      ">> Epoch 56 finished \tANN training loss 0.005709\n",
      ">> Epoch 57 finished \tANN training loss 0.006361\n",
      ">> Epoch 58 finished \tANN training loss 0.005719\n",
      ">> Epoch 59 finished \tANN training loss 0.005650\n",
      ">> Epoch 60 finished \tANN training loss 0.005276\n",
      ">> Epoch 61 finished \tANN training loss 0.006485\n",
      ">> Epoch 62 finished \tANN training loss 0.006583\n",
      ">> Epoch 63 finished \tANN training loss 0.004637\n",
      ">> Epoch 64 finished \tANN training loss 0.004141\n",
      ">> Epoch 65 finished \tANN training loss 0.004181\n",
      ">> Epoch 66 finished \tANN training loss 0.004513\n",
      ">> Epoch 67 finished \tANN training loss 0.003850\n",
      ">> Epoch 68 finished \tANN training loss 0.004143\n",
      ">> Epoch 69 finished \tANN training loss 0.004499\n",
      ">> Epoch 70 finished \tANN training loss 0.003854\n",
      ">> Epoch 71 finished \tANN training loss 0.004295\n",
      ">> Epoch 72 finished \tANN training loss 0.003509\n",
      ">> Epoch 73 finished \tANN training loss 0.003128\n",
      ">> Epoch 74 finished \tANN training loss 0.003751\n",
      ">> Epoch 75 finished \tANN training loss 0.003114\n",
      ">> Epoch 76 finished \tANN training loss 0.003320\n",
      ">> Epoch 77 finished \tANN training loss 0.004103\n",
      ">> Epoch 78 finished \tANN training loss 0.003369\n",
      ">> Epoch 79 finished \tANN training loss 0.003081\n",
      ">> Epoch 80 finished \tANN training loss 0.003164\n",
      ">> Epoch 81 finished \tANN training loss 0.002609\n",
      ">> Epoch 82 finished \tANN training loss 0.003658\n",
      ">> Epoch 83 finished \tANN training loss 0.002517\n",
      ">> Epoch 84 finished \tANN training loss 0.002544\n",
      ">> Epoch 85 finished \tANN training loss 0.002970\n",
      ">> Epoch 86 finished \tANN training loss 0.002467\n",
      ">> Epoch 87 finished \tANN training loss 0.002379\n",
      ">> Epoch 88 finished \tANN training loss 0.002195\n",
      ">> Epoch 89 finished \tANN training loss 0.002379\n",
      ">> Epoch 90 finished \tANN training loss 0.002187\n",
      ">> Epoch 91 finished \tANN training loss 0.002067\n",
      ">> Epoch 92 finished \tANN training loss 0.001939\n",
      ">> Epoch 93 finished \tANN training loss 0.001989\n",
      ">> Epoch 94 finished \tANN training loss 0.002001\n",
      ">> Epoch 95 finished \tANN training loss 0.001831\n",
      ">> Epoch 96 finished \tANN training loss 0.001825\n",
      ">> Epoch 97 finished \tANN training loss 0.001718\n",
      ">> Epoch 98 finished \tANN training loss 0.001775\n",
      ">> Epoch 99 finished \tANN training loss 0.001570\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.915000\n"
     ]
    }
   ],
   "source": [
    "twolayer_acc[1] = deep_belief_net(hidden_layers_structure=[100, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.915\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(twolayer_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing layer_1=500 performance:\n",
    "\n",
    "Setting 3  (500) on 200\n",
    "\n",
    "hidden_layers_structure=[500, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 66.616905\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 41.794498\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 40.064198\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 49.759266\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 45.772545\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 51.655350\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 34.008072\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 40.369316\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 47.192348\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 32.536373\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 139.700043\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 139.479324\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 154.507370\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 149.979294\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 177.211273\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 171.639984\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 284.741333\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 255.479370\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 256.613159\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 259.745453\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.657874\n",
      ">> Epoch 1 finished \tANN training loss 0.490750\n",
      ">> Epoch 2 finished \tANN training loss 0.418871\n",
      ">> Epoch 3 finished \tANN training loss 0.371595\n",
      ">> Epoch 4 finished \tANN training loss 0.303596\n",
      ">> Epoch 5 finished \tANN training loss 0.254828\n",
      ">> Epoch 6 finished \tANN training loss 0.228769\n",
      ">> Epoch 7 finished \tANN training loss 0.190201\n",
      ">> Epoch 8 finished \tANN training loss 0.183969\n",
      ">> Epoch 9 finished \tANN training loss 0.174994\n",
      ">> Epoch 10 finished \tANN training loss 0.137839\n",
      ">> Epoch 11 finished \tANN training loss 0.135722\n",
      ">> Epoch 12 finished \tANN training loss 0.112763\n",
      ">> Epoch 13 finished \tANN training loss 0.107191\n",
      ">> Epoch 14 finished \tANN training loss 0.100026\n",
      ">> Epoch 15 finished \tANN training loss 0.081416\n",
      ">> Epoch 16 finished \tANN training loss 0.081809\n",
      ">> Epoch 17 finished \tANN training loss 0.070781\n",
      ">> Epoch 18 finished \tANN training loss 0.064466\n",
      ">> Epoch 19 finished \tANN training loss 0.060430\n",
      ">> Epoch 20 finished \tANN training loss 0.056110\n",
      ">> Epoch 21 finished \tANN training loss 0.049476\n",
      ">> Epoch 22 finished \tANN training loss 0.048510\n",
      ">> Epoch 23 finished \tANN training loss 0.040786\n",
      ">> Epoch 24 finished \tANN training loss 0.038424\n",
      ">> Epoch 25 finished \tANN training loss 0.036943\n",
      ">> Epoch 26 finished \tANN training loss 0.033430\n",
      ">> Epoch 27 finished \tANN training loss 0.027127\n",
      ">> Epoch 28 finished \tANN training loss 0.029400\n",
      ">> Epoch 29 finished \tANN training loss 0.026401\n",
      ">> Epoch 30 finished \tANN training loss 0.023717\n",
      ">> Epoch 31 finished \tANN training loss 0.022168\n",
      ">> Epoch 32 finished \tANN training loss 0.022820\n",
      ">> Epoch 33 finished \tANN training loss 0.021149\n",
      ">> Epoch 34 finished \tANN training loss 0.020552\n",
      ">> Epoch 35 finished \tANN training loss 0.015224\n",
      ">> Epoch 36 finished \tANN training loss 0.019159\n",
      ">> Epoch 37 finished \tANN training loss 0.014946\n",
      ">> Epoch 38 finished \tANN training loss 0.015595\n",
      ">> Epoch 39 finished \tANN training loss 0.012482\n",
      ">> Epoch 40 finished \tANN training loss 0.012273\n",
      ">> Epoch 41 finished \tANN training loss 0.011136\n",
      ">> Epoch 42 finished \tANN training loss 0.011097\n",
      ">> Epoch 43 finished \tANN training loss 0.010602\n",
      ">> Epoch 44 finished \tANN training loss 0.008973\n",
      ">> Epoch 45 finished \tANN training loss 0.008432\n",
      ">> Epoch 46 finished \tANN training loss 0.008714\n",
      ">> Epoch 47 finished \tANN training loss 0.008342\n",
      ">> Epoch 48 finished \tANN training loss 0.008721\n",
      ">> Epoch 49 finished \tANN training loss 0.007404\n",
      ">> Epoch 50 finished \tANN training loss 0.008252\n",
      ">> Epoch 51 finished \tANN training loss 0.006198\n",
      ">> Epoch 52 finished \tANN training loss 0.006139\n",
      ">> Epoch 53 finished \tANN training loss 0.006267\n",
      ">> Epoch 54 finished \tANN training loss 0.005249\n",
      ">> Epoch 55 finished \tANN training loss 0.005675\n",
      ">> Epoch 56 finished \tANN training loss 0.007387\n",
      ">> Epoch 57 finished \tANN training loss 0.005238\n",
      ">> Epoch 58 finished \tANN training loss 0.005009\n",
      ">> Epoch 59 finished \tANN training loss 0.004082\n",
      ">> Epoch 60 finished \tANN training loss 0.004925\n",
      ">> Epoch 61 finished \tANN training loss 0.004018\n",
      ">> Epoch 62 finished \tANN training loss 0.003953\n",
      ">> Epoch 63 finished \tANN training loss 0.004372\n",
      ">> Epoch 64 finished \tANN training loss 0.003574\n",
      ">> Epoch 65 finished \tANN training loss 0.003137\n",
      ">> Epoch 66 finished \tANN training loss 0.003196\n",
      ">> Epoch 67 finished \tANN training loss 0.003031\n",
      ">> Epoch 68 finished \tANN training loss 0.002775\n",
      ">> Epoch 69 finished \tANN training loss 0.003340\n",
      ">> Epoch 70 finished \tANN training loss 0.002560\n",
      ">> Epoch 71 finished \tANN training loss 0.002965\n",
      ">> Epoch 72 finished \tANN training loss 0.002645\n",
      ">> Epoch 73 finished \tANN training loss 0.003353\n",
      ">> Epoch 74 finished \tANN training loss 0.002821\n",
      ">> Epoch 75 finished \tANN training loss 0.002891\n",
      ">> Epoch 76 finished \tANN training loss 0.002221\n",
      ">> Epoch 77 finished \tANN training loss 0.001979\n",
      ">> Epoch 78 finished \tANN training loss 0.002301\n",
      ">> Epoch 79 finished \tANN training loss 0.001860\n",
      ">> Epoch 80 finished \tANN training loss 0.001911\n",
      ">> Epoch 81 finished \tANN training loss 0.001802\n",
      ">> Epoch 82 finished \tANN training loss 0.001864\n",
      ">> Epoch 83 finished \tANN training loss 0.001685\n",
      ">> Epoch 84 finished \tANN training loss 0.001816\n",
      ">> Epoch 85 finished \tANN training loss 0.001712\n",
      ">> Epoch 86 finished \tANN training loss 0.001780\n",
      ">> Epoch 87 finished \tANN training loss 0.001511\n",
      ">> Epoch 88 finished \tANN training loss 0.001829\n",
      ">> Epoch 89 finished \tANN training loss 0.001786\n",
      ">> Epoch 90 finished \tANN training loss 0.001788\n",
      ">> Epoch 91 finished \tANN training loss 0.001396\n",
      ">> Epoch 92 finished \tANN training loss 0.001532\n",
      ">> Epoch 93 finished \tANN training loss 0.001492\n",
      ">> Epoch 94 finished \tANN training loss 0.001431\n",
      ">> Epoch 95 finished \tANN training loss 0.001299\n",
      ">> Epoch 96 finished \tANN training loss 0.001569\n",
      ">> Epoch 97 finished \tANN training loss 0.001452\n",
      ">> Epoch 98 finished \tANN training loss 0.001459\n",
      ">> Epoch 99 finished \tANN training loss 0.001264\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.920000\n"
     ]
    }
   ],
   "source": [
    "twolayer_acc[2] = deep_belief_net(hidden_layers_structure=[500, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.92\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(twolayer_acc[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 4  (500) on 500\n",
    "\n",
    "hidden_layers_structure=[500, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 71.661751\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 57.248215\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 51.039303\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 36.137207\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 37.001850\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 32.274765\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 41.225483\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 27.219765\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 45.696922\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 38.599415\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 124.731964\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 75.605820\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 180.343048\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 186.135315\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 172.192368\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 125.186134\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 184.999771\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 188.884750\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 131.298843\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 148.734741\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.660491\n",
      ">> Epoch 1 finished \tANN training loss 0.489562\n",
      ">> Epoch 2 finished \tANN training loss 0.392955\n",
      ">> Epoch 3 finished \tANN training loss 0.367938\n",
      ">> Epoch 4 finished \tANN training loss 0.293265\n",
      ">> Epoch 5 finished \tANN training loss 0.250625\n",
      ">> Epoch 6 finished \tANN training loss 0.210597\n",
      ">> Epoch 7 finished \tANN training loss 0.181460\n",
      ">> Epoch 8 finished \tANN training loss 0.167114\n",
      ">> Epoch 9 finished \tANN training loss 0.160760\n",
      ">> Epoch 10 finished \tANN training loss 0.132280\n",
      ">> Epoch 11 finished \tANN training loss 0.110675\n",
      ">> Epoch 12 finished \tANN training loss 0.104748\n",
      ">> Epoch 13 finished \tANN training loss 0.096836\n",
      ">> Epoch 14 finished \tANN training loss 0.084379\n",
      ">> Epoch 15 finished \tANN training loss 0.092434\n",
      ">> Epoch 16 finished \tANN training loss 0.072570\n",
      ">> Epoch 17 finished \tANN training loss 0.061137\n",
      ">> Epoch 18 finished \tANN training loss 0.071292\n",
      ">> Epoch 19 finished \tANN training loss 0.052609\n",
      ">> Epoch 20 finished \tANN training loss 0.052656\n",
      ">> Epoch 21 finished \tANN training loss 0.040762\n",
      ">> Epoch 22 finished \tANN training loss 0.047511\n",
      ">> Epoch 23 finished \tANN training loss 0.034841\n",
      ">> Epoch 24 finished \tANN training loss 0.029626\n",
      ">> Epoch 25 finished \tANN training loss 0.029027\n",
      ">> Epoch 26 finished \tANN training loss 0.027281\n",
      ">> Epoch 27 finished \tANN training loss 0.031472\n",
      ">> Epoch 28 finished \tANN training loss 0.022944\n",
      ">> Epoch 29 finished \tANN training loss 0.022094\n",
      ">> Epoch 30 finished \tANN training loss 0.021607\n",
      ">> Epoch 31 finished \tANN training loss 0.018819\n",
      ">> Epoch 32 finished \tANN training loss 0.018088\n",
      ">> Epoch 33 finished \tANN training loss 0.015578\n",
      ">> Epoch 34 finished \tANN training loss 0.014821\n",
      ">> Epoch 35 finished \tANN training loss 0.014754\n",
      ">> Epoch 36 finished \tANN training loss 0.013215\n",
      ">> Epoch 37 finished \tANN training loss 0.013180\n",
      ">> Epoch 38 finished \tANN training loss 0.012334\n",
      ">> Epoch 39 finished \tANN training loss 0.011340\n",
      ">> Epoch 40 finished \tANN training loss 0.010903\n",
      ">> Epoch 41 finished \tANN training loss 0.009551\n",
      ">> Epoch 42 finished \tANN training loss 0.009597\n",
      ">> Epoch 43 finished \tANN training loss 0.008119\n",
      ">> Epoch 44 finished \tANN training loss 0.007534\n",
      ">> Epoch 45 finished \tANN training loss 0.007236\n",
      ">> Epoch 46 finished \tANN training loss 0.008007\n",
      ">> Epoch 47 finished \tANN training loss 0.007704\n",
      ">> Epoch 48 finished \tANN training loss 0.007291\n",
      ">> Epoch 49 finished \tANN training loss 0.006352\n",
      ">> Epoch 50 finished \tANN training loss 0.006105\n",
      ">> Epoch 51 finished \tANN training loss 0.006388\n",
      ">> Epoch 52 finished \tANN training loss 0.005272\n",
      ">> Epoch 53 finished \tANN training loss 0.005689\n",
      ">> Epoch 54 finished \tANN training loss 0.005405\n",
      ">> Epoch 55 finished \tANN training loss 0.004688\n",
      ">> Epoch 56 finished \tANN training loss 0.004148\n",
      ">> Epoch 57 finished \tANN training loss 0.004590\n",
      ">> Epoch 58 finished \tANN training loss 0.004238\n",
      ">> Epoch 59 finished \tANN training loss 0.004169\n",
      ">> Epoch 60 finished \tANN training loss 0.003835\n",
      ">> Epoch 61 finished \tANN training loss 0.003647\n",
      ">> Epoch 62 finished \tANN training loss 0.003732\n",
      ">> Epoch 63 finished \tANN training loss 0.003312\n",
      ">> Epoch 64 finished \tANN training loss 0.003365\n",
      ">> Epoch 65 finished \tANN training loss 0.003090\n",
      ">> Epoch 66 finished \tANN training loss 0.003446\n",
      ">> Epoch 67 finished \tANN training loss 0.002781\n",
      ">> Epoch 68 finished \tANN training loss 0.003004\n",
      ">> Epoch 69 finished \tANN training loss 0.003034\n",
      ">> Epoch 70 finished \tANN training loss 0.002909\n",
      ">> Epoch 71 finished \tANN training loss 0.002802\n",
      ">> Epoch 72 finished \tANN training loss 0.002567\n",
      ">> Epoch 73 finished \tANN training loss 0.002420\n",
      ">> Epoch 74 finished \tANN training loss 0.002339\n",
      ">> Epoch 75 finished \tANN training loss 0.002678\n",
      ">> Epoch 76 finished \tANN training loss 0.002441\n",
      ">> Epoch 77 finished \tANN training loss 0.002296\n",
      ">> Epoch 78 finished \tANN training loss 0.002319\n",
      ">> Epoch 79 finished \tANN training loss 0.002183\n",
      ">> Epoch 80 finished \tANN training loss 0.002513\n",
      ">> Epoch 81 finished \tANN training loss 0.001895\n",
      ">> Epoch 82 finished \tANN training loss 0.002353\n",
      ">> Epoch 83 finished \tANN training loss 0.001722\n",
      ">> Epoch 84 finished \tANN training loss 0.001619\n",
      ">> Epoch 85 finished \tANN training loss 0.001785\n",
      ">> Epoch 86 finished \tANN training loss 0.001882\n",
      ">> Epoch 87 finished \tANN training loss 0.001897\n",
      ">> Epoch 88 finished \tANN training loss 0.001458\n",
      ">> Epoch 89 finished \tANN training loss 0.001540\n",
      ">> Epoch 90 finished \tANN training loss 0.001412\n",
      ">> Epoch 91 finished \tANN training loss 0.001483\n",
      ">> Epoch 92 finished \tANN training loss 0.001313\n",
      ">> Epoch 93 finished \tANN training loss 0.001155\n",
      ">> Epoch 94 finished \tANN training loss 0.001166\n",
      ">> Epoch 95 finished \tANN training loss 0.001079\n",
      ">> Epoch 96 finished \tANN training loss 0.001114\n",
      ">> Epoch 97 finished \tANN training loss 0.000990\n",
      ">> Epoch 98 finished \tANN training loss 0.001224\n",
      ">> Epoch 99 finished \tANN training loss 0.001181\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.930000\n"
     ]
    }
   ],
   "source": [
    "twolayer_acc[3] = deep_belief_net(hidden_layers_structure=[500, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.93\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(twolayer_acc[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.915, 0.915, 0.92, 0.93, 0, 0, 0, 0]\n",
      "Most accurate 2-layer setting is Setting 4\n"
     ]
    }
   ],
   "source": [
    "# Collated results\n",
    "print(twolayer_acc)\n",
    "print('Most accurate 2-layer setting is Setting ' + str(twolayer_acc.index(max(twolayer_acc)) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create bar graph\n",
    "plt.clf()\n",
    "objects = ('[100, 200]', '[100, 500]', '[500, 200]', '[500, 500]')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [0.905,0.92,0.89,0.89]\n",
    "\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.xlabel('Layer Settings')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Two-layer Settings')\n",
    " \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different 3-layer structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store accuracies\n",
    "threelayer_acc = [0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 1  Baseline\n",
    "\n",
    "hidden_layers_structure=[100, 200, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 69.211891\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 54.715435\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 65.167702\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 84.242981\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 57.315968\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 100.032951\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 93.880310\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 126.621506\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 103.421570\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 102.365608\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 346.209290\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 374.893646\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 491.747742\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 729.175598\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 617.200256\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 800.803772\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 777.229614\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 772.667480\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 727.426392\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 745.967529\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 11459.107422\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 18461.373047\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 36919.609375\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 50508.015625\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 50366.011719\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 55213.621094\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 70355.671875\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 71196.914062\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 75190.921875\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 81700.390625\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.580584\n",
      ">> Epoch 1 finished \tANN training loss 1.194760\n",
      ">> Epoch 2 finished \tANN training loss 0.925701\n",
      ">> Epoch 3 finished \tANN training loss 0.783431\n",
      ">> Epoch 4 finished \tANN training loss 0.678023\n",
      ">> Epoch 5 finished \tANN training loss 0.565544\n",
      ">> Epoch 6 finished \tANN training loss 0.499443\n",
      ">> Epoch 7 finished \tANN training loss 0.440925\n",
      ">> Epoch 8 finished \tANN training loss 0.374729\n",
      ">> Epoch 9 finished \tANN training loss 0.324563\n",
      ">> Epoch 10 finished \tANN training loss 0.286204\n",
      ">> Epoch 11 finished \tANN training loss 0.260415\n",
      ">> Epoch 12 finished \tANN training loss 0.257141\n",
      ">> Epoch 13 finished \tANN training loss 0.244947\n",
      ">> Epoch 14 finished \tANN training loss 0.219555\n",
      ">> Epoch 15 finished \tANN training loss 0.179129\n",
      ">> Epoch 16 finished \tANN training loss 0.210747\n",
      ">> Epoch 17 finished \tANN training loss 0.179460\n",
      ">> Epoch 18 finished \tANN training loss 0.154251\n",
      ">> Epoch 19 finished \tANN training loss 0.167423\n",
      ">> Epoch 20 finished \tANN training loss 0.122682\n",
      ">> Epoch 21 finished \tANN training loss 0.119368\n",
      ">> Epoch 22 finished \tANN training loss 0.122256\n",
      ">> Epoch 23 finished \tANN training loss 0.090236\n",
      ">> Epoch 24 finished \tANN training loss 0.090379\n",
      ">> Epoch 25 finished \tANN training loss 0.093834\n",
      ">> Epoch 26 finished \tANN training loss 0.066902\n",
      ">> Epoch 27 finished \tANN training loss 0.067678\n",
      ">> Epoch 28 finished \tANN training loss 0.058022\n",
      ">> Epoch 29 finished \tANN training loss 0.053510\n",
      ">> Epoch 30 finished \tANN training loss 0.059847\n",
      ">> Epoch 31 finished \tANN training loss 0.050838\n",
      ">> Epoch 32 finished \tANN training loss 0.053311\n",
      ">> Epoch 33 finished \tANN training loss 0.046493\n",
      ">> Epoch 34 finished \tANN training loss 0.054898\n",
      ">> Epoch 35 finished \tANN training loss 0.034509\n",
      ">> Epoch 36 finished \tANN training loss 0.045126\n",
      ">> Epoch 37 finished \tANN training loss 0.038577\n",
      ">> Epoch 38 finished \tANN training loss 0.037325\n",
      ">> Epoch 39 finished \tANN training loss 0.048366\n",
      ">> Epoch 40 finished \tANN training loss 0.025499\n",
      ">> Epoch 41 finished \tANN training loss 0.030443\n",
      ">> Epoch 42 finished \tANN training loss 0.029826\n",
      ">> Epoch 43 finished \tANN training loss 0.026014\n",
      ">> Epoch 44 finished \tANN training loss 0.025371\n",
      ">> Epoch 45 finished \tANN training loss 0.022160\n",
      ">> Epoch 46 finished \tANN training loss 0.020218\n",
      ">> Epoch 47 finished \tANN training loss 0.031927\n",
      ">> Epoch 48 finished \tANN training loss 0.021658\n",
      ">> Epoch 49 finished \tANN training loss 0.020687\n",
      ">> Epoch 50 finished \tANN training loss 0.028883\n",
      ">> Epoch 51 finished \tANN training loss 0.025723\n",
      ">> Epoch 52 finished \tANN training loss 0.023819\n",
      ">> Epoch 53 finished \tANN training loss 0.019441\n",
      ">> Epoch 54 finished \tANN training loss 0.020782\n",
      ">> Epoch 55 finished \tANN training loss 0.023676\n",
      ">> Epoch 56 finished \tANN training loss 0.024203\n",
      ">> Epoch 57 finished \tANN training loss 0.017896\n",
      ">> Epoch 58 finished \tANN training loss 0.017443\n",
      ">> Epoch 59 finished \tANN training loss 0.011332\n",
      ">> Epoch 60 finished \tANN training loss 0.011712\n",
      ">> Epoch 61 finished \tANN training loss 0.013364\n",
      ">> Epoch 62 finished \tANN training loss 0.012547\n",
      ">> Epoch 63 finished \tANN training loss 0.012622\n",
      ">> Epoch 64 finished \tANN training loss 0.013190\n",
      ">> Epoch 65 finished \tANN training loss 0.012341\n",
      ">> Epoch 66 finished \tANN training loss 0.016282\n",
      ">> Epoch 67 finished \tANN training loss 0.010343\n",
      ">> Epoch 68 finished \tANN training loss 0.009073\n",
      ">> Epoch 69 finished \tANN training loss 0.009885\n",
      ">> Epoch 70 finished \tANN training loss 0.009054\n",
      ">> Epoch 71 finished \tANN training loss 0.008841\n",
      ">> Epoch 72 finished \tANN training loss 0.009537\n",
      ">> Epoch 73 finished \tANN training loss 0.012629\n",
      ">> Epoch 74 finished \tANN training loss 0.012985\n",
      ">> Epoch 75 finished \tANN training loss 0.011929\n",
      ">> Epoch 76 finished \tANN training loss 0.010100\n",
      ">> Epoch 77 finished \tANN training loss 0.005749\n",
      ">> Epoch 78 finished \tANN training loss 0.010081\n",
      ">> Epoch 79 finished \tANN training loss 0.005840\n",
      ">> Epoch 80 finished \tANN training loss 0.007349\n",
      ">> Epoch 81 finished \tANN training loss 0.005691\n",
      ">> Epoch 82 finished \tANN training loss 0.004833\n",
      ">> Epoch 83 finished \tANN training loss 0.005270\n",
      ">> Epoch 84 finished \tANN training loss 0.005854\n",
      ">> Epoch 85 finished \tANN training loss 0.008042\n",
      ">> Epoch 86 finished \tANN training loss 0.004677\n",
      ">> Epoch 87 finished \tANN training loss 0.006828\n",
      ">> Epoch 88 finished \tANN training loss 0.006649\n",
      ">> Epoch 89 finished \tANN training loss 0.005791\n",
      ">> Epoch 90 finished \tANN training loss 0.007586\n",
      ">> Epoch 91 finished \tANN training loss 0.007884\n",
      ">> Epoch 92 finished \tANN training loss 0.009073\n",
      ">> Epoch 93 finished \tANN training loss 0.007698\n",
      ">> Epoch 94 finished \tANN training loss 0.005238\n",
      ">> Epoch 95 finished \tANN training loss 0.005730\n",
      ">> Epoch 96 finished \tANN training loss 0.006825\n",
      ">> Epoch 97 finished \tANN training loss 0.004438\n",
      ">> Epoch 98 finished \tANN training loss 0.004480\n",
      ">> Epoch 99 finished \tANN training loss 0.005031\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.905000\n"
     ]
    }
   ],
   "source": [
    "threelayer_acc[0] = deep_belief_net(hidden_layers_structure=[100, 200, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.905\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(threelayer_acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 2  1st Layer = 500\n",
    "\n",
    "hidden_layers_structure=[500, 200, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 64.890564\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 64.352226\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 66.013908\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 43.504204\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 37.202518\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 34.903088\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 32.947681\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 32.995579\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 38.589195\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 53.157158\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 265.494537\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 293.698242\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 319.521240\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 491.695862\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 325.683319\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 387.864807\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 347.527710\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 288.222107\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 441.854614\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 365.168640\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 7704.830078\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 8756.724609\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 14747.218750\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 13782.094727\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 20590.453125\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 17249.955078\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 23698.279297\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 22634.957031\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 35549.710938\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 33558.386719\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.148739\n",
      ">> Epoch 1 finished \tANN training loss 0.770206\n",
      ">> Epoch 2 finished \tANN training loss 0.570758\n",
      ">> Epoch 3 finished \tANN training loss 0.423928\n",
      ">> Epoch 4 finished \tANN training loss 0.373262\n",
      ">> Epoch 5 finished \tANN training loss 0.294912\n",
      ">> Epoch 6 finished \tANN training loss 0.236317\n",
      ">> Epoch 7 finished \tANN training loss 0.256555\n",
      ">> Epoch 8 finished \tANN training loss 0.181814\n",
      ">> Epoch 9 finished \tANN training loss 0.156014\n",
      ">> Epoch 10 finished \tANN training loss 0.157873\n",
      ">> Epoch 11 finished \tANN training loss 0.112084\n",
      ">> Epoch 12 finished \tANN training loss 0.102756\n",
      ">> Epoch 13 finished \tANN training loss 0.106336\n",
      ">> Epoch 14 finished \tANN training loss 0.084706\n",
      ">> Epoch 15 finished \tANN training loss 0.081579\n",
      ">> Epoch 16 finished \tANN training loss 0.069330\n",
      ">> Epoch 17 finished \tANN training loss 0.066856\n",
      ">> Epoch 18 finished \tANN training loss 0.056585\n",
      ">> Epoch 19 finished \tANN training loss 0.072584\n",
      ">> Epoch 20 finished \tANN training loss 0.058569\n",
      ">> Epoch 21 finished \tANN training loss 0.054657\n",
      ">> Epoch 22 finished \tANN training loss 0.050575\n",
      ">> Epoch 23 finished \tANN training loss 0.040919\n",
      ">> Epoch 24 finished \tANN training loss 0.036358\n",
      ">> Epoch 25 finished \tANN training loss 0.033245\n",
      ">> Epoch 26 finished \tANN training loss 0.032542\n",
      ">> Epoch 27 finished \tANN training loss 0.033480\n",
      ">> Epoch 28 finished \tANN training loss 0.029375\n",
      ">> Epoch 29 finished \tANN training loss 0.019737\n",
      ">> Epoch 30 finished \tANN training loss 0.021683\n",
      ">> Epoch 31 finished \tANN training loss 0.019821\n",
      ">> Epoch 32 finished \tANN training loss 0.017043\n",
      ">> Epoch 33 finished \tANN training loss 0.015202\n",
      ">> Epoch 34 finished \tANN training loss 0.016184\n",
      ">> Epoch 35 finished \tANN training loss 0.014409\n",
      ">> Epoch 36 finished \tANN training loss 0.015413\n",
      ">> Epoch 37 finished \tANN training loss 0.010383\n",
      ">> Epoch 38 finished \tANN training loss 0.012022\n",
      ">> Epoch 39 finished \tANN training loss 0.011709\n",
      ">> Epoch 40 finished \tANN training loss 0.011447\n",
      ">> Epoch 41 finished \tANN training loss 0.007633\n",
      ">> Epoch 42 finished \tANN training loss 0.007799\n",
      ">> Epoch 43 finished \tANN training loss 0.006248\n",
      ">> Epoch 44 finished \tANN training loss 0.006012\n",
      ">> Epoch 45 finished \tANN training loss 0.004822\n",
      ">> Epoch 46 finished \tANN training loss 0.006340\n",
      ">> Epoch 47 finished \tANN training loss 0.004101\n",
      ">> Epoch 48 finished \tANN training loss 0.003524\n",
      ">> Epoch 49 finished \tANN training loss 0.004045\n",
      ">> Epoch 50 finished \tANN training loss 0.004038\n",
      ">> Epoch 51 finished \tANN training loss 0.002426\n",
      ">> Epoch 52 finished \tANN training loss 0.001912\n",
      ">> Epoch 53 finished \tANN training loss 0.002844\n",
      ">> Epoch 54 finished \tANN training loss 0.005721\n",
      ">> Epoch 55 finished \tANN training loss 0.002943\n",
      ">> Epoch 56 finished \tANN training loss 0.002222\n",
      ">> Epoch 57 finished \tANN training loss 0.002616\n",
      ">> Epoch 58 finished \tANN training loss 0.002046\n",
      ">> Epoch 59 finished \tANN training loss 0.001357\n",
      ">> Epoch 60 finished \tANN training loss 0.001450\n",
      ">> Epoch 61 finished \tANN training loss 0.001482\n",
      ">> Epoch 62 finished \tANN training loss 0.001142\n",
      ">> Epoch 63 finished \tANN training loss 0.001222\n",
      ">> Epoch 64 finished \tANN training loss 0.001581\n",
      ">> Epoch 65 finished \tANN training loss 0.002686\n",
      ">> Epoch 66 finished \tANN training loss 0.001035\n",
      ">> Epoch 67 finished \tANN training loss 0.001389\n",
      ">> Epoch 68 finished \tANN training loss 0.001359\n",
      ">> Epoch 69 finished \tANN training loss 0.000933\n",
      ">> Epoch 70 finished \tANN training loss 0.001213\n",
      ">> Epoch 71 finished \tANN training loss 0.001068\n",
      ">> Epoch 72 finished \tANN training loss 0.000659\n",
      ">> Epoch 73 finished \tANN training loss 0.000580\n",
      ">> Epoch 74 finished \tANN training loss 0.000587\n",
      ">> Epoch 75 finished \tANN training loss 0.000927\n",
      ">> Epoch 76 finished \tANN training loss 0.021917\n",
      ">> Epoch 77 finished \tANN training loss 0.002161\n",
      ">> Epoch 78 finished \tANN training loss 0.001041\n",
      ">> Epoch 79 finished \tANN training loss 0.001683\n",
      ">> Epoch 80 finished \tANN training loss 0.001074\n",
      ">> Epoch 81 finished \tANN training loss 0.000763\n",
      ">> Epoch 82 finished \tANN training loss 0.000818\n",
      ">> Epoch 83 finished \tANN training loss 0.000978\n",
      ">> Epoch 84 finished \tANN training loss 0.001042\n",
      ">> Epoch 85 finished \tANN training loss 0.000871\n",
      ">> Epoch 86 finished \tANN training loss 0.001035\n",
      ">> Epoch 87 finished \tANN training loss 0.001564\n",
      ">> Epoch 88 finished \tANN training loss 0.000981\n",
      ">> Epoch 89 finished \tANN training loss 0.000645\n",
      ">> Epoch 90 finished \tANN training loss 0.000562\n",
      ">> Epoch 91 finished \tANN training loss 0.000525\n",
      ">> Epoch 92 finished \tANN training loss 0.000495\n",
      ">> Epoch 93 finished \tANN training loss 0.000774\n",
      ">> Epoch 94 finished \tANN training loss 0.000486\n",
      ">> Epoch 95 finished \tANN training loss 0.000467\n",
      ">> Epoch 96 finished \tANN training loss 0.000382\n",
      ">> Epoch 97 finished \tANN training loss 0.000421\n",
      ">> Epoch 98 finished \tANN training loss 0.011721\n",
      ">> Epoch 99 finished \tANN training loss 0.000538\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.915000\n"
     ]
    }
   ],
   "source": [
    "threelayer_acc[1] = deep_belief_net(hidden_layers_structure=[500, 200, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.915\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(threelayer_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 3  2nd Layer = 500\n",
    "\n",
    "hidden_layers_structure=[100, 500, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 46.037357\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 90.269768\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 54.883526\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 90.055466\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 48.924706\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 59.731728\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 68.522606\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 76.011360\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 67.557816\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 77.150909\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 693.408936\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 475.906860\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 517.780701\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 512.426208\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 403.576843\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 414.917175\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 432.829529\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 677.364441\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 506.351013\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 512.856201\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 20082.324219\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 19763.488281\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 26652.082031\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 35193.226562\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 43998.519531\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 54127.585938\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 55989.539062\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 77291.226562\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 93704.960938\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 90798.687500\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.368032\n",
      ">> Epoch 1 finished \tANN training loss 0.965836\n",
      ">> Epoch 2 finished \tANN training loss 0.824202\n",
      ">> Epoch 3 finished \tANN training loss 0.637604\n",
      ">> Epoch 4 finished \tANN training loss 0.625319\n",
      ">> Epoch 5 finished \tANN training loss 0.492506\n",
      ">> Epoch 6 finished \tANN training loss 0.464184\n",
      ">> Epoch 7 finished \tANN training loss 0.369553\n",
      ">> Epoch 8 finished \tANN training loss 0.331659\n",
      ">> Epoch 9 finished \tANN training loss 0.307416\n",
      ">> Epoch 10 finished \tANN training loss 0.248093\n",
      ">> Epoch 11 finished \tANN training loss 0.245124\n",
      ">> Epoch 12 finished \tANN training loss 0.221395\n",
      ">> Epoch 13 finished \tANN training loss 0.204238\n",
      ">> Epoch 14 finished \tANN training loss 0.201492\n",
      ">> Epoch 15 finished \tANN training loss 0.163309\n",
      ">> Epoch 16 finished \tANN training loss 0.155370\n",
      ">> Epoch 17 finished \tANN training loss 0.141065\n",
      ">> Epoch 18 finished \tANN training loss 0.117621\n",
      ">> Epoch 19 finished \tANN training loss 0.110302\n",
      ">> Epoch 20 finished \tANN training loss 0.138922\n",
      ">> Epoch 21 finished \tANN training loss 0.147155\n",
      ">> Epoch 22 finished \tANN training loss 0.107724\n",
      ">> Epoch 23 finished \tANN training loss 0.086279\n",
      ">> Epoch 24 finished \tANN training loss 0.083002\n",
      ">> Epoch 25 finished \tANN training loss 0.072218\n",
      ">> Epoch 26 finished \tANN training loss 0.066615\n",
      ">> Epoch 27 finished \tANN training loss 0.066570\n",
      ">> Epoch 28 finished \tANN training loss 0.064705\n",
      ">> Epoch 29 finished \tANN training loss 0.051858\n",
      ">> Epoch 30 finished \tANN training loss 0.047943\n",
      ">> Epoch 31 finished \tANN training loss 0.058598\n",
      ">> Epoch 32 finished \tANN training loss 0.048704\n",
      ">> Epoch 33 finished \tANN training loss 0.053961\n",
      ">> Epoch 34 finished \tANN training loss 0.042221\n",
      ">> Epoch 35 finished \tANN training loss 0.037252\n",
      ">> Epoch 36 finished \tANN training loss 0.039174\n",
      ">> Epoch 37 finished \tANN training loss 0.031617\n",
      ">> Epoch 38 finished \tANN training loss 0.031995\n",
      ">> Epoch 39 finished \tANN training loss 0.028332\n",
      ">> Epoch 40 finished \tANN training loss 0.025115\n",
      ">> Epoch 41 finished \tANN training loss 0.023724\n",
      ">> Epoch 42 finished \tANN training loss 0.022677\n",
      ">> Epoch 43 finished \tANN training loss 0.023166\n",
      ">> Epoch 44 finished \tANN training loss 0.025624\n",
      ">> Epoch 45 finished \tANN training loss 0.020415\n",
      ">> Epoch 46 finished \tANN training loss 0.024133\n",
      ">> Epoch 47 finished \tANN training loss 0.016326\n",
      ">> Epoch 48 finished \tANN training loss 0.016199\n",
      ">> Epoch 49 finished \tANN training loss 0.016581\n",
      ">> Epoch 50 finished \tANN training loss 0.015776\n",
      ">> Epoch 51 finished \tANN training loss 0.012655\n",
      ">> Epoch 52 finished \tANN training loss 0.013531\n",
      ">> Epoch 53 finished \tANN training loss 0.014778\n",
      ">> Epoch 54 finished \tANN training loss 0.014632\n",
      ">> Epoch 55 finished \tANN training loss 0.012877\n",
      ">> Epoch 56 finished \tANN training loss 0.012540\n",
      ">> Epoch 57 finished \tANN training loss 0.009692\n",
      ">> Epoch 58 finished \tANN training loss 0.011601\n",
      ">> Epoch 59 finished \tANN training loss 0.012456\n",
      ">> Epoch 60 finished \tANN training loss 0.008918\n",
      ">> Epoch 61 finished \tANN training loss 0.009407\n",
      ">> Epoch 62 finished \tANN training loss 0.011221\n",
      ">> Epoch 63 finished \tANN training loss 0.010915\n",
      ">> Epoch 64 finished \tANN training loss 0.012434\n",
      ">> Epoch 65 finished \tANN training loss 0.009233\n",
      ">> Epoch 66 finished \tANN training loss 0.011796\n",
      ">> Epoch 67 finished \tANN training loss 0.008571\n",
      ">> Epoch 68 finished \tANN training loss 0.010558\n",
      ">> Epoch 69 finished \tANN training loss 0.013745\n",
      ">> Epoch 70 finished \tANN training loss 0.007248\n",
      ">> Epoch 71 finished \tANN training loss 0.007332\n",
      ">> Epoch 72 finished \tANN training loss 0.006959\n",
      ">> Epoch 73 finished \tANN training loss 0.008246\n",
      ">> Epoch 74 finished \tANN training loss 0.005313\n",
      ">> Epoch 75 finished \tANN training loss 0.005633\n",
      ">> Epoch 76 finished \tANN training loss 0.011271\n",
      ">> Epoch 77 finished \tANN training loss 0.005037\n",
      ">> Epoch 78 finished \tANN training loss 0.004283\n",
      ">> Epoch 79 finished \tANN training loss 0.003606\n",
      ">> Epoch 80 finished \tANN training loss 0.003277\n",
      ">> Epoch 81 finished \tANN training loss 0.005034\n",
      ">> Epoch 82 finished \tANN training loss 0.005416\n",
      ">> Epoch 83 finished \tANN training loss 0.004572\n",
      ">> Epoch 84 finished \tANN training loss 0.003049\n",
      ">> Epoch 85 finished \tANN training loss 0.003888\n",
      ">> Epoch 86 finished \tANN training loss 0.004188\n",
      ">> Epoch 87 finished \tANN training loss 0.004338\n",
      ">> Epoch 88 finished \tANN training loss 0.002636\n",
      ">> Epoch 89 finished \tANN training loss 0.004734\n",
      ">> Epoch 90 finished \tANN training loss 0.004708\n",
      ">> Epoch 91 finished \tANN training loss 0.003443\n",
      ">> Epoch 92 finished \tANN training loss 0.003586\n",
      ">> Epoch 93 finished \tANN training loss 0.002316\n",
      ">> Epoch 94 finished \tANN training loss 0.002712\n",
      ">> Epoch 95 finished \tANN training loss 0.003219\n",
      ">> Epoch 96 finished \tANN training loss 0.002898\n",
      ">> Epoch 97 finished \tANN training loss 0.001889\n",
      ">> Epoch 98 finished \tANN training loss 0.005270\n",
      ">> Epoch 99 finished \tANN training loss 0.004183\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.905000\n"
     ]
    }
   ],
   "source": [
    "threelayer_acc[2] = deep_belief_net(hidden_layers_structure=[100, 500, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.905\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(threelayer_acc[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 4  3rd Layer = 500\n",
    "\n",
    "hidden_layers_structure=[100, 200, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 58.408165\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 69.195312\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 71.488205\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 42.269623\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 61.457363\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 79.939407\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 101.641739\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 84.693886\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 96.495857\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 97.584984\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 374.778687\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 617.015137\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 481.513519\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 1032.019043\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 872.648376\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 882.761902\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 1119.452759\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 1263.858398\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 1300.469727\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 1353.029663\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 13897.667969\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 26535.505859\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 50756.683594\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 53545.750000\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 38055.984375\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 82117.398438\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 76303.906250\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 58658.738281\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 71391.468750\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 89981.617188\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.119905\n",
      ">> Epoch 1 finished \tANN training loss 0.909399\n",
      ">> Epoch 2 finished \tANN training loss 0.682351\n",
      ">> Epoch 3 finished \tANN training loss 0.569214\n",
      ">> Epoch 4 finished \tANN training loss 0.482639\n",
      ">> Epoch 5 finished \tANN training loss 0.371544\n",
      ">> Epoch 6 finished \tANN training loss 0.334282\n",
      ">> Epoch 7 finished \tANN training loss 0.308384\n",
      ">> Epoch 8 finished \tANN training loss 0.230581\n",
      ">> Epoch 9 finished \tANN training loss 0.207830\n",
      ">> Epoch 10 finished \tANN training loss 0.205981\n",
      ">> Epoch 11 finished \tANN training loss 0.145182\n",
      ">> Epoch 12 finished \tANN training loss 0.126699\n",
      ">> Epoch 13 finished \tANN training loss 0.111036\n",
      ">> Epoch 14 finished \tANN training loss 0.122788\n",
      ">> Epoch 15 finished \tANN training loss 0.108345\n",
      ">> Epoch 16 finished \tANN training loss 0.094774\n",
      ">> Epoch 17 finished \tANN training loss 0.075767\n",
      ">> Epoch 18 finished \tANN training loss 0.064752\n",
      ">> Epoch 19 finished \tANN training loss 0.072787\n",
      ">> Epoch 20 finished \tANN training loss 0.053335\n",
      ">> Epoch 21 finished \tANN training loss 0.065728\n",
      ">> Epoch 22 finished \tANN training loss 0.066765\n",
      ">> Epoch 23 finished \tANN training loss 0.059071\n",
      ">> Epoch 24 finished \tANN training loss 0.042531\n",
      ">> Epoch 25 finished \tANN training loss 0.037095\n",
      ">> Epoch 26 finished \tANN training loss 0.044392\n",
      ">> Epoch 27 finished \tANN training loss 0.029427\n",
      ">> Epoch 28 finished \tANN training loss 0.026206\n",
      ">> Epoch 29 finished \tANN training loss 0.034820\n",
      ">> Epoch 30 finished \tANN training loss 0.031917\n",
      ">> Epoch 31 finished \tANN training loss 0.019695\n",
      ">> Epoch 32 finished \tANN training loss 0.026772\n",
      ">> Epoch 33 finished \tANN training loss 0.021182\n",
      ">> Epoch 34 finished \tANN training loss 0.019129\n",
      ">> Epoch 35 finished \tANN training loss 0.019079\n",
      ">> Epoch 36 finished \tANN training loss 0.017632\n",
      ">> Epoch 37 finished \tANN training loss 0.012927\n",
      ">> Epoch 38 finished \tANN training loss 0.019722\n",
      ">> Epoch 39 finished \tANN training loss 0.015793\n",
      ">> Epoch 40 finished \tANN training loss 0.016437\n",
      ">> Epoch 41 finished \tANN training loss 0.014072\n",
      ">> Epoch 42 finished \tANN training loss 0.019872\n",
      ">> Epoch 43 finished \tANN training loss 0.011392\n",
      ">> Epoch 44 finished \tANN training loss 0.013529\n",
      ">> Epoch 45 finished \tANN training loss 0.007584\n",
      ">> Epoch 46 finished \tANN training loss 0.009698\n",
      ">> Epoch 47 finished \tANN training loss 0.009805\n",
      ">> Epoch 48 finished \tANN training loss 0.008928\n",
      ">> Epoch 49 finished \tANN training loss 0.005349\n",
      ">> Epoch 50 finished \tANN training loss 0.005724\n",
      ">> Epoch 51 finished \tANN training loss 0.007827\n",
      ">> Epoch 52 finished \tANN training loss 0.005161\n",
      ">> Epoch 53 finished \tANN training loss 0.006110\n",
      ">> Epoch 54 finished \tANN training loss 0.008521\n",
      ">> Epoch 55 finished \tANN training loss 0.008896\n",
      ">> Epoch 56 finished \tANN training loss 0.009951\n",
      ">> Epoch 57 finished \tANN training loss 0.005751\n",
      ">> Epoch 58 finished \tANN training loss 0.004731\n",
      ">> Epoch 59 finished \tANN training loss 0.004270\n",
      ">> Epoch 60 finished \tANN training loss 0.008654\n",
      ">> Epoch 61 finished \tANN training loss 0.005985\n",
      ">> Epoch 62 finished \tANN training loss 0.007761\n",
      ">> Epoch 63 finished \tANN training loss 0.008057\n",
      ">> Epoch 64 finished \tANN training loss 0.004299\n",
      ">> Epoch 65 finished \tANN training loss 0.004801\n",
      ">> Epoch 66 finished \tANN training loss 0.003753\n",
      ">> Epoch 67 finished \tANN training loss 0.004286\n",
      ">> Epoch 68 finished \tANN training loss 0.004568\n",
      ">> Epoch 69 finished \tANN training loss 0.004588\n",
      ">> Epoch 70 finished \tANN training loss 0.003805\n",
      ">> Epoch 71 finished \tANN training loss 0.005524\n",
      ">> Epoch 72 finished \tANN training loss 0.002980\n",
      ">> Epoch 73 finished \tANN training loss 0.003374\n",
      ">> Epoch 74 finished \tANN training loss 0.003371\n",
      ">> Epoch 75 finished \tANN training loss 0.003845\n",
      ">> Epoch 76 finished \tANN training loss 0.004075\n",
      ">> Epoch 77 finished \tANN training loss 0.005952\n",
      ">> Epoch 78 finished \tANN training loss 0.004317\n",
      ">> Epoch 79 finished \tANN training loss 0.004397\n",
      ">> Epoch 80 finished \tANN training loss 0.003549\n",
      ">> Epoch 81 finished \tANN training loss 0.005461\n",
      ">> Epoch 82 finished \tANN training loss 0.003525\n",
      ">> Epoch 83 finished \tANN training loss 0.003562\n",
      ">> Epoch 84 finished \tANN training loss 0.004389\n",
      ">> Epoch 85 finished \tANN training loss 0.002285\n",
      ">> Epoch 86 finished \tANN training loss 0.003203\n",
      ">> Epoch 87 finished \tANN training loss 0.002455\n",
      ">> Epoch 88 finished \tANN training loss 0.002817\n",
      ">> Epoch 89 finished \tANN training loss 0.005536\n",
      ">> Epoch 90 finished \tANN training loss 0.002001\n",
      ">> Epoch 91 finished \tANN training loss 0.002743\n",
      ">> Epoch 92 finished \tANN training loss 0.001895\n",
      ">> Epoch 93 finished \tANN training loss 0.001581\n",
      ">> Epoch 94 finished \tANN training loss 0.001748\n",
      ">> Epoch 95 finished \tANN training loss 0.001589\n",
      ">> Epoch 96 finished \tANN training loss 0.001303\n",
      ">> Epoch 97 finished \tANN training loss 0.001715\n",
      ">> Epoch 98 finished \tANN training loss 0.002865\n",
      ">> Epoch 99 finished \tANN training loss 0.002316\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.910000\n"
     ]
    }
   ],
   "source": [
    "threelayer_acc[3] = deep_belief_net(hidden_layers_structure=[100, 200, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.91\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(threelayer_acc[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 5  1st Layer = 500, 2nd Layer = 500\n",
    "\n",
    "hidden_layers_structure=[500, 500, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 53.484531\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 49.617989\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 28.611094\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 46.947323\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 26.846586\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 36.614113\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 23.046961\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 43.888760\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 37.438507\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 20.884056\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 44.684120\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 40.053890\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 34.717075\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 58.272724\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 19.762663\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 48.689640\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 41.754356\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 35.288944\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 33.576466\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 58.882156\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 302.683289\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 494.964233\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 459.660461\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 486.647186\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 530.887207\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 494.535339\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 635.821899\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 487.871063\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 576.013428\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 580.948303\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.618450\n",
      ">> Epoch 1 finished \tANN training loss 0.420340\n",
      ">> Epoch 2 finished \tANN training loss 0.371379\n",
      ">> Epoch 3 finished \tANN training loss 0.272224\n",
      ">> Epoch 4 finished \tANN training loss 0.231351\n",
      ">> Epoch 5 finished \tANN training loss 0.187486\n",
      ">> Epoch 6 finished \tANN training loss 0.186028\n",
      ">> Epoch 7 finished \tANN training loss 0.156209\n",
      ">> Epoch 8 finished \tANN training loss 0.127174\n",
      ">> Epoch 9 finished \tANN training loss 0.124143\n",
      ">> Epoch 10 finished \tANN training loss 0.101501\n",
      ">> Epoch 11 finished \tANN training loss 0.099455\n",
      ">> Epoch 12 finished \tANN training loss 0.067864\n",
      ">> Epoch 13 finished \tANN training loss 0.066500\n",
      ">> Epoch 14 finished \tANN training loss 0.057930\n",
      ">> Epoch 15 finished \tANN training loss 0.052993\n",
      ">> Epoch 16 finished \tANN training loss 0.051266\n",
      ">> Epoch 17 finished \tANN training loss 0.042247\n",
      ">> Epoch 18 finished \tANN training loss 0.049701\n",
      ">> Epoch 19 finished \tANN training loss 0.037037\n",
      ">> Epoch 20 finished \tANN training loss 0.035621\n",
      ">> Epoch 21 finished \tANN training loss 0.028847\n",
      ">> Epoch 22 finished \tANN training loss 0.026278\n",
      ">> Epoch 23 finished \tANN training loss 0.024064\n",
      ">> Epoch 24 finished \tANN training loss 0.023868\n",
      ">> Epoch 25 finished \tANN training loss 0.021595\n",
      ">> Epoch 26 finished \tANN training loss 0.022288\n",
      ">> Epoch 27 finished \tANN training loss 0.017450\n",
      ">> Epoch 28 finished \tANN training loss 0.021444\n",
      ">> Epoch 29 finished \tANN training loss 0.014531\n",
      ">> Epoch 30 finished \tANN training loss 0.014011\n",
      ">> Epoch 31 finished \tANN training loss 0.015504\n",
      ">> Epoch 32 finished \tANN training loss 0.011549\n",
      ">> Epoch 33 finished \tANN training loss 0.011439\n",
      ">> Epoch 34 finished \tANN training loss 0.011122\n",
      ">> Epoch 35 finished \tANN training loss 0.008329\n",
      ">> Epoch 36 finished \tANN training loss 0.008541\n",
      ">> Epoch 37 finished \tANN training loss 0.007277\n",
      ">> Epoch 38 finished \tANN training loss 0.008585\n",
      ">> Epoch 39 finished \tANN training loss 0.005722\n",
      ">> Epoch 40 finished \tANN training loss 0.007295\n",
      ">> Epoch 41 finished \tANN training loss 0.005362\n",
      ">> Epoch 42 finished \tANN training loss 0.005623\n",
      ">> Epoch 43 finished \tANN training loss 0.004942\n",
      ">> Epoch 44 finished \tANN training loss 0.003925\n",
      ">> Epoch 45 finished \tANN training loss 0.004307\n",
      ">> Epoch 46 finished \tANN training loss 0.003705\n",
      ">> Epoch 47 finished \tANN training loss 0.003339\n",
      ">> Epoch 48 finished \tANN training loss 0.002857\n",
      ">> Epoch 49 finished \tANN training loss 0.004071\n",
      ">> Epoch 50 finished \tANN training loss 0.003528\n",
      ">> Epoch 51 finished \tANN training loss 0.002837\n",
      ">> Epoch 52 finished \tANN training loss 0.002737\n",
      ">> Epoch 53 finished \tANN training loss 0.002371\n",
      ">> Epoch 54 finished \tANN training loss 0.001788\n",
      ">> Epoch 55 finished \tANN training loss 0.001632\n",
      ">> Epoch 56 finished \tANN training loss 0.002442\n",
      ">> Epoch 57 finished \tANN training loss 0.002046\n",
      ">> Epoch 58 finished \tANN training loss 0.001969\n",
      ">> Epoch 59 finished \tANN training loss 0.001716\n",
      ">> Epoch 60 finished \tANN training loss 0.001657\n",
      ">> Epoch 61 finished \tANN training loss 0.001847\n",
      ">> Epoch 62 finished \tANN training loss 0.001518\n",
      ">> Epoch 63 finished \tANN training loss 0.001185\n",
      ">> Epoch 64 finished \tANN training loss 0.001212\n",
      ">> Epoch 65 finished \tANN training loss 0.003035\n",
      ">> Epoch 66 finished \tANN training loss 0.001409\n",
      ">> Epoch 67 finished \tANN training loss 0.001151\n",
      ">> Epoch 68 finished \tANN training loss 0.001186\n",
      ">> Epoch 69 finished \tANN training loss 0.000978\n",
      ">> Epoch 70 finished \tANN training loss 0.000953\n",
      ">> Epoch 71 finished \tANN training loss 0.000965\n",
      ">> Epoch 72 finished \tANN training loss 0.000900\n",
      ">> Epoch 73 finished \tANN training loss 0.000799\n",
      ">> Epoch 74 finished \tANN training loss 0.000662\n",
      ">> Epoch 75 finished \tANN training loss 0.000717\n",
      ">> Epoch 76 finished \tANN training loss 0.000637\n",
      ">> Epoch 77 finished \tANN training loss 0.000770\n",
      ">> Epoch 78 finished \tANN training loss 0.000790\n",
      ">> Epoch 79 finished \tANN training loss 0.000887\n",
      ">> Epoch 80 finished \tANN training loss 0.000701\n",
      ">> Epoch 81 finished \tANN training loss 0.000633\n",
      ">> Epoch 82 finished \tANN training loss 0.000643\n",
      ">> Epoch 83 finished \tANN training loss 0.000557\n",
      ">> Epoch 84 finished \tANN training loss 0.000585\n",
      ">> Epoch 85 finished \tANN training loss 0.000480\n",
      ">> Epoch 86 finished \tANN training loss 0.000438\n",
      ">> Epoch 87 finished \tANN training loss 0.000466\n",
      ">> Epoch 88 finished \tANN training loss 0.000410\n",
      ">> Epoch 89 finished \tANN training loss 0.000461\n",
      ">> Epoch 90 finished \tANN training loss 0.000627\n",
      ">> Epoch 91 finished \tANN training loss 0.000476\n",
      ">> Epoch 92 finished \tANN training loss 0.000444\n",
      ">> Epoch 93 finished \tANN training loss 0.000492\n",
      ">> Epoch 94 finished \tANN training loss 0.001626\n",
      ">> Epoch 95 finished \tANN training loss 0.000596\n",
      ">> Epoch 96 finished \tANN training loss 0.000460\n",
      ">> Epoch 97 finished \tANN training loss 0.000393\n",
      ">> Epoch 98 finished \tANN training loss 0.000369\n",
      ">> Epoch 99 finished \tANN training loss 0.000291\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.925000\n"
     ]
    }
   ],
   "source": [
    "threelayer_acc[4] = deep_belief_net(hidden_layers_structure=[500, 500, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.925\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(threelayer_acc[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 6  1st Layer = 500, 3rd Layer = 500\n",
    "\n",
    "hidden_layers_structure=[500, 200, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 47.435226\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 65.411766\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 40.892231\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 29.415909\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 52.996983\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 21.949629\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 24.500845\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 27.317327\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 49.982723\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 47.436172\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 271.394684\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 90.636230\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 165.255737\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 186.123322\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 264.922668\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 203.708313\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 174.804886\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 199.951172\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 273.661652\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 195.511536\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 2191.128418\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 2868.894287\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 4583.454590\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 4213.567871\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 4605.455566\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 4974.555664\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 5970.238770\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 5938.312012\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 6314.316895\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 6525.642578\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.705950\n",
      ">> Epoch 1 finished \tANN training loss 0.501846\n",
      ">> Epoch 2 finished \tANN training loss 0.384075\n",
      ">> Epoch 3 finished \tANN training loss 0.317613\n",
      ">> Epoch 4 finished \tANN training loss 0.285993\n",
      ">> Epoch 5 finished \tANN training loss 0.218323\n",
      ">> Epoch 6 finished \tANN training loss 0.220026\n",
      ">> Epoch 7 finished \tANN training loss 0.171106\n",
      ">> Epoch 8 finished \tANN training loss 0.170636\n",
      ">> Epoch 9 finished \tANN training loss 0.119489\n",
      ">> Epoch 10 finished \tANN training loss 0.116837\n",
      ">> Epoch 11 finished \tANN training loss 0.112188\n",
      ">> Epoch 12 finished \tANN training loss 0.077588\n",
      ">> Epoch 13 finished \tANN training loss 0.093665\n",
      ">> Epoch 14 finished \tANN training loss 0.059614\n",
      ">> Epoch 15 finished \tANN training loss 0.052859\n",
      ">> Epoch 16 finished \tANN training loss 0.045846\n",
      ">> Epoch 17 finished \tANN training loss 0.046793\n",
      ">> Epoch 18 finished \tANN training loss 0.038132\n",
      ">> Epoch 19 finished \tANN training loss 0.030542\n",
      ">> Epoch 20 finished \tANN training loss 0.029776\n",
      ">> Epoch 21 finished \tANN training loss 0.027568\n",
      ">> Epoch 22 finished \tANN training loss 0.026064\n",
      ">> Epoch 23 finished \tANN training loss 0.021152\n",
      ">> Epoch 24 finished \tANN training loss 0.018378\n",
      ">> Epoch 25 finished \tANN training loss 0.019202\n",
      ">> Epoch 26 finished \tANN training loss 0.018589\n",
      ">> Epoch 27 finished \tANN training loss 0.019108\n",
      ">> Epoch 28 finished \tANN training loss 0.012930\n",
      ">> Epoch 29 finished \tANN training loss 0.012161\n",
      ">> Epoch 30 finished \tANN training loss 0.010855\n",
      ">> Epoch 31 finished \tANN training loss 0.017628\n",
      ">> Epoch 32 finished \tANN training loss 0.009240\n",
      ">> Epoch 33 finished \tANN training loss 0.007286\n",
      ">> Epoch 34 finished \tANN training loss 0.006707\n",
      ">> Epoch 35 finished \tANN training loss 0.005772\n",
      ">> Epoch 36 finished \tANN training loss 0.005389\n",
      ">> Epoch 37 finished \tANN training loss 0.005200\n",
      ">> Epoch 38 finished \tANN training loss 0.004847\n",
      ">> Epoch 39 finished \tANN training loss 0.005911\n",
      ">> Epoch 40 finished \tANN training loss 0.004184\n",
      ">> Epoch 41 finished \tANN training loss 0.003993\n",
      ">> Epoch 42 finished \tANN training loss 0.003854\n",
      ">> Epoch 43 finished \tANN training loss 0.004542\n",
      ">> Epoch 44 finished \tANN training loss 0.003205\n",
      ">> Epoch 45 finished \tANN training loss 0.003073\n",
      ">> Epoch 46 finished \tANN training loss 0.004969\n",
      ">> Epoch 47 finished \tANN training loss 0.003377\n",
      ">> Epoch 48 finished \tANN training loss 0.002288\n",
      ">> Epoch 49 finished \tANN training loss 0.001990\n",
      ">> Epoch 50 finished \tANN training loss 0.002185\n",
      ">> Epoch 51 finished \tANN training loss 0.002228\n",
      ">> Epoch 52 finished \tANN training loss 0.001598\n",
      ">> Epoch 53 finished \tANN training loss 0.002845\n",
      ">> Epoch 54 finished \tANN training loss 0.001946\n",
      ">> Epoch 55 finished \tANN training loss 0.001628\n",
      ">> Epoch 56 finished \tANN training loss 0.002171\n",
      ">> Epoch 57 finished \tANN training loss 0.001924\n",
      ">> Epoch 58 finished \tANN training loss 0.001438\n",
      ">> Epoch 59 finished \tANN training loss 0.001108\n",
      ">> Epoch 60 finished \tANN training loss 0.001005\n",
      ">> Epoch 61 finished \tANN training loss 0.001268\n",
      ">> Epoch 62 finished \tANN training loss 0.001319\n",
      ">> Epoch 63 finished \tANN training loss 0.000848\n",
      ">> Epoch 64 finished \tANN training loss 0.001764\n",
      ">> Epoch 65 finished \tANN training loss 0.000830\n",
      ">> Epoch 66 finished \tANN training loss 0.000916\n",
      ">> Epoch 67 finished \tANN training loss 0.001221\n",
      ">> Epoch 68 finished \tANN training loss 0.000744\n",
      ">> Epoch 69 finished \tANN training loss 0.001084\n",
      ">> Epoch 70 finished \tANN training loss 0.000744\n",
      ">> Epoch 71 finished \tANN training loss 0.000532\n",
      ">> Epoch 72 finished \tANN training loss 0.000845\n",
      ">> Epoch 73 finished \tANN training loss 0.001255\n",
      ">> Epoch 74 finished \tANN training loss 0.001226\n",
      ">> Epoch 75 finished \tANN training loss 0.000830\n",
      ">> Epoch 76 finished \tANN training loss 0.000964\n",
      ">> Epoch 77 finished \tANN training loss 0.000761\n",
      ">> Epoch 78 finished \tANN training loss 0.000797\n",
      ">> Epoch 79 finished \tANN training loss 0.000592\n",
      ">> Epoch 80 finished \tANN training loss 0.000542\n",
      ">> Epoch 81 finished \tANN training loss 0.000728\n",
      ">> Epoch 82 finished \tANN training loss 0.000507\n",
      ">> Epoch 83 finished \tANN training loss 0.000683\n",
      ">> Epoch 84 finished \tANN training loss 0.000737\n",
      ">> Epoch 85 finished \tANN training loss 0.000369\n",
      ">> Epoch 86 finished \tANN training loss 0.000815\n",
      ">> Epoch 87 finished \tANN training loss 0.001076\n",
      ">> Epoch 88 finished \tANN training loss 0.000593\n",
      ">> Epoch 89 finished \tANN training loss 0.000633\n",
      ">> Epoch 90 finished \tANN training loss 0.000600\n",
      ">> Epoch 91 finished \tANN training loss 0.000652\n",
      ">> Epoch 92 finished \tANN training loss 0.000585\n",
      ">> Epoch 93 finished \tANN training loss 0.000347\n",
      ">> Epoch 94 finished \tANN training loss 0.000321\n",
      ">> Epoch 95 finished \tANN training loss 0.000256\n",
      ">> Epoch 96 finished \tANN training loss 0.000236\n",
      ">> Epoch 97 finished \tANN training loss 0.000312\n",
      ">> Epoch 98 finished \tANN training loss 0.001116\n",
      ">> Epoch 99 finished \tANN training loss 0.000288\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.920000\n"
     ]
    }
   ],
   "source": [
    "threelayer_acc[5] = deep_belief_net(hidden_layers_structure=[500, 200, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.92\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(threelayer_acc[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 7  2nd Layer = 500, 3rd Layer = 500\n",
    "\n",
    "hidden_layers_structure=[100, 500, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 74.895760\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 85.589020\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 75.232468\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 70.381668\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 78.885025\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 81.908279\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 71.334549\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 73.247963\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 85.922073\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 90.483337\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 287.300110\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 563.882263\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 699.517517\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 413.560638\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 516.107971\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 698.975769\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 453.494598\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 722.497742\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 656.940063\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 627.395447\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 22307.554688\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 26080.412109\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 29038.660156\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 31783.369141\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 40547.296875\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 45807.648438\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 49722.925781\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 50296.851562\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 44169.890625\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 58293.144531\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.142354\n",
      ">> Epoch 1 finished \tANN training loss 0.741675\n",
      ">> Epoch 2 finished \tANN training loss 0.618078\n",
      ">> Epoch 3 finished \tANN training loss 0.503561\n",
      ">> Epoch 4 finished \tANN training loss 0.400187\n",
      ">> Epoch 5 finished \tANN training loss 0.368017\n",
      ">> Epoch 6 finished \tANN training loss 0.335961\n",
      ">> Epoch 7 finished \tANN training loss 0.253038\n",
      ">> Epoch 8 finished \tANN training loss 0.244035\n",
      ">> Epoch 9 finished \tANN training loss 0.227803\n",
      ">> Epoch 10 finished \tANN training loss 0.206103\n",
      ">> Epoch 11 finished \tANN training loss 0.198385\n",
      ">> Epoch 12 finished \tANN training loss 0.184478\n",
      ">> Epoch 13 finished \tANN training loss 0.134006\n",
      ">> Epoch 14 finished \tANN training loss 0.123082\n",
      ">> Epoch 15 finished \tANN training loss 0.120894\n",
      ">> Epoch 16 finished \tANN training loss 0.114036\n",
      ">> Epoch 17 finished \tANN training loss 0.108684\n",
      ">> Epoch 18 finished \tANN training loss 0.088408\n",
      ">> Epoch 19 finished \tANN training loss 0.084145\n",
      ">> Epoch 20 finished \tANN training loss 0.079510\n",
      ">> Epoch 21 finished \tANN training loss 0.068226\n",
      ">> Epoch 22 finished \tANN training loss 0.069401\n",
      ">> Epoch 23 finished \tANN training loss 0.066966\n",
      ">> Epoch 24 finished \tANN training loss 0.059350\n",
      ">> Epoch 25 finished \tANN training loss 0.058920\n",
      ">> Epoch 26 finished \tANN training loss 0.061407\n",
      ">> Epoch 27 finished \tANN training loss 0.049875\n",
      ">> Epoch 28 finished \tANN training loss 0.062418\n",
      ">> Epoch 29 finished \tANN training loss 0.049559\n",
      ">> Epoch 30 finished \tANN training loss 0.051677\n",
      ">> Epoch 31 finished \tANN training loss 0.043280\n",
      ">> Epoch 32 finished \tANN training loss 0.037446\n",
      ">> Epoch 33 finished \tANN training loss 0.039280\n",
      ">> Epoch 34 finished \tANN training loss 0.037099\n",
      ">> Epoch 35 finished \tANN training loss 0.038508\n",
      ">> Epoch 36 finished \tANN training loss 0.030147\n",
      ">> Epoch 37 finished \tANN training loss 0.035359\n",
      ">> Epoch 38 finished \tANN training loss 0.030200\n",
      ">> Epoch 39 finished \tANN training loss 0.028446\n",
      ">> Epoch 40 finished \tANN training loss 0.035360\n",
      ">> Epoch 41 finished \tANN training loss 0.026857\n",
      ">> Epoch 42 finished \tANN training loss 0.022487\n",
      ">> Epoch 43 finished \tANN training loss 0.023180\n",
      ">> Epoch 44 finished \tANN training loss 0.019519\n",
      ">> Epoch 45 finished \tANN training loss 0.019643\n",
      ">> Epoch 46 finished \tANN training loss 0.020917\n",
      ">> Epoch 47 finished \tANN training loss 0.019906\n",
      ">> Epoch 48 finished \tANN training loss 0.021182\n",
      ">> Epoch 49 finished \tANN training loss 0.019993\n",
      ">> Epoch 50 finished \tANN training loss 0.019495\n",
      ">> Epoch 51 finished \tANN training loss 0.012777\n",
      ">> Epoch 52 finished \tANN training loss 0.017926\n",
      ">> Epoch 53 finished \tANN training loss 0.013932\n",
      ">> Epoch 54 finished \tANN training loss 0.011079\n",
      ">> Epoch 55 finished \tANN training loss 0.013040\n",
      ">> Epoch 56 finished \tANN training loss 0.010757\n",
      ">> Epoch 57 finished \tANN training loss 0.011092\n",
      ">> Epoch 58 finished \tANN training loss 0.011699\n",
      ">> Epoch 59 finished \tANN training loss 0.018164\n",
      ">> Epoch 60 finished \tANN training loss 0.015491\n",
      ">> Epoch 61 finished \tANN training loss 0.011143\n",
      ">> Epoch 62 finished \tANN training loss 0.011673\n",
      ">> Epoch 63 finished \tANN training loss 0.006940\n",
      ">> Epoch 64 finished \tANN training loss 0.006353\n",
      ">> Epoch 65 finished \tANN training loss 0.010197\n",
      ">> Epoch 66 finished \tANN training loss 0.008399\n",
      ">> Epoch 67 finished \tANN training loss 0.007237\n",
      ">> Epoch 68 finished \tANN training loss 0.005689\n",
      ">> Epoch 69 finished \tANN training loss 0.006715\n",
      ">> Epoch 70 finished \tANN training loss 0.005677\n",
      ">> Epoch 71 finished \tANN training loss 0.004988\n",
      ">> Epoch 72 finished \tANN training loss 0.004328\n",
      ">> Epoch 73 finished \tANN training loss 0.005476\n",
      ">> Epoch 74 finished \tANN training loss 0.005665\n",
      ">> Epoch 75 finished \tANN training loss 0.007465\n",
      ">> Epoch 76 finished \tANN training loss 0.006398\n",
      ">> Epoch 77 finished \tANN training loss 0.005059\n",
      ">> Epoch 78 finished \tANN training loss 0.003641\n",
      ">> Epoch 79 finished \tANN training loss 0.003520\n",
      ">> Epoch 80 finished \tANN training loss 0.004537\n",
      ">> Epoch 81 finished \tANN training loss 0.004653\n",
      ">> Epoch 82 finished \tANN training loss 0.003046\n",
      ">> Epoch 83 finished \tANN training loss 0.004958\n",
      ">> Epoch 84 finished \tANN training loss 0.003206\n",
      ">> Epoch 85 finished \tANN training loss 0.003467\n",
      ">> Epoch 86 finished \tANN training loss 0.003210\n",
      ">> Epoch 87 finished \tANN training loss 0.003064\n",
      ">> Epoch 88 finished \tANN training loss 0.003068\n",
      ">> Epoch 89 finished \tANN training loss 0.003391\n",
      ">> Epoch 90 finished \tANN training loss 0.002906\n",
      ">> Epoch 91 finished \tANN training loss 0.002788\n",
      ">> Epoch 92 finished \tANN training loss 0.003122\n",
      ">> Epoch 93 finished \tANN training loss 0.002835\n",
      ">> Epoch 94 finished \tANN training loss 0.002755\n",
      ">> Epoch 95 finished \tANN training loss 0.002521\n",
      ">> Epoch 96 finished \tANN training loss 0.002588\n",
      ">> Epoch 97 finished \tANN training loss 0.002841\n",
      ">> Epoch 98 finished \tANN training loss 0.002459\n",
      ">> Epoch 99 finished \tANN training loss 0.001963\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.910000\n"
     ]
    }
   ],
   "source": [
    "threelayer_acc[6] = deep_belief_net(hidden_layers_structure=[100, 500, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.91\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(threelayer_acc[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.905, 0.915, 0.905, 0.91, 0.925, 0.92, 0.91]\n",
      "Most accurate 3-layer setting is Setting 5\n"
     ]
    }
   ],
   "source": [
    "#Collated results\n",
    "print(threelayer_acc)\n",
    "print('Most accurate 3-layer setting is Setting ' + str(threelayer_acc.index(max(threelayer_acc)) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create bar graph\n",
    "plt.clf()\n",
    "objects = ('[100, 200, 300]', '[500, 200, 300]', '[100, 500, 300]', '[100, 200, 500]', '[500, 500, 300]', '[500, 200, 500]', '[100, 500, 500]')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [0.9,0.925,0.895,0.87, 0.91, 0.875, 0.87]\n",
    "\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5, width=0.5)\n",
    "plt.xticks(y_pos, objects, rotation=30)\n",
    "plt.xlabel('Layer Settings')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Three-layer Settings')\n",
    "\n",
    " \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Pre-training (RBM) Learning Rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store accuracies\n",
    "rbm_acc = [0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 1\n",
    "\n",
    "learning_rate_rbm=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 48.676327\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 44.983673\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 37.189789\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 32.108334\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 29.079042\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 26.841358\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 25.027222\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 23.722651\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 22.607672\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 21.722998\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 25.245600\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 18.901453\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 10.708242\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 7.466425\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 5.650034\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 4.540646\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 3.837914\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 3.387722\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 2.928828\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 2.709045\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.031388\n",
      ">> Epoch 1 finished \tANN training loss 0.698853\n",
      ">> Epoch 2 finished \tANN training loss 0.578371\n",
      ">> Epoch 3 finished \tANN training loss 0.471742\n",
      ">> Epoch 4 finished \tANN training loss 0.438442\n",
      ">> Epoch 5 finished \tANN training loss 0.378788\n",
      ">> Epoch 6 finished \tANN training loss 0.329750\n",
      ">> Epoch 7 finished \tANN training loss 0.296313\n",
      ">> Epoch 8 finished \tANN training loss 0.287987\n",
      ">> Epoch 9 finished \tANN training loss 0.236950\n",
      ">> Epoch 10 finished \tANN training loss 0.274150\n",
      ">> Epoch 11 finished \tANN training loss 0.195219\n",
      ">> Epoch 12 finished \tANN training loss 0.198504\n",
      ">> Epoch 13 finished \tANN training loss 0.161178\n",
      ">> Epoch 14 finished \tANN training loss 0.153865\n",
      ">> Epoch 15 finished \tANN training loss 0.149770\n",
      ">> Epoch 16 finished \tANN training loss 0.125424\n",
      ">> Epoch 17 finished \tANN training loss 0.110138\n",
      ">> Epoch 18 finished \tANN training loss 0.120597\n",
      ">> Epoch 19 finished \tANN training loss 0.087653\n",
      ">> Epoch 20 finished \tANN training loss 0.083731\n",
      ">> Epoch 21 finished \tANN training loss 0.071885\n",
      ">> Epoch 22 finished \tANN training loss 0.070991\n",
      ">> Epoch 23 finished \tANN training loss 0.062647\n",
      ">> Epoch 24 finished \tANN training loss 0.063783\n",
      ">> Epoch 25 finished \tANN training loss 0.050657\n",
      ">> Epoch 26 finished \tANN training loss 0.046961\n",
      ">> Epoch 27 finished \tANN training loss 0.041112\n",
      ">> Epoch 28 finished \tANN training loss 0.039394\n",
      ">> Epoch 29 finished \tANN training loss 0.030572\n",
      ">> Epoch 30 finished \tANN training loss 0.032830\n",
      ">> Epoch 31 finished \tANN training loss 0.027804\n",
      ">> Epoch 32 finished \tANN training loss 0.022658\n",
      ">> Epoch 33 finished \tANN training loss 0.023533\n",
      ">> Epoch 34 finished \tANN training loss 0.040336\n",
      ">> Epoch 35 finished \tANN training loss 0.021596\n",
      ">> Epoch 36 finished \tANN training loss 0.017419\n",
      ">> Epoch 37 finished \tANN training loss 0.018029\n",
      ">> Epoch 38 finished \tANN training loss 0.021564\n",
      ">> Epoch 39 finished \tANN training loss 0.015428\n",
      ">> Epoch 40 finished \tANN training loss 0.016025\n",
      ">> Epoch 41 finished \tANN training loss 0.013834\n",
      ">> Epoch 42 finished \tANN training loss 0.012360\n",
      ">> Epoch 43 finished \tANN training loss 0.012491\n",
      ">> Epoch 44 finished \tANN training loss 0.009236\n",
      ">> Epoch 45 finished \tANN training loss 0.009869\n",
      ">> Epoch 46 finished \tANN training loss 0.013663\n",
      ">> Epoch 47 finished \tANN training loss 0.008555\n",
      ">> Epoch 48 finished \tANN training loss 0.009036\n",
      ">> Epoch 49 finished \tANN training loss 0.007324\n",
      ">> Epoch 50 finished \tANN training loss 0.007412\n",
      ">> Epoch 51 finished \tANN training loss 0.005840\n",
      ">> Epoch 52 finished \tANN training loss 0.006838\n",
      ">> Epoch 53 finished \tANN training loss 0.006508\n",
      ">> Epoch 54 finished \tANN training loss 0.005439\n",
      ">> Epoch 55 finished \tANN training loss 0.006234\n",
      ">> Epoch 56 finished \tANN training loss 0.005158\n",
      ">> Epoch 57 finished \tANN training loss 0.007660\n",
      ">> Epoch 58 finished \tANN training loss 0.004611\n",
      ">> Epoch 59 finished \tANN training loss 0.004291\n",
      ">> Epoch 60 finished \tANN training loss 0.004049\n",
      ">> Epoch 61 finished \tANN training loss 0.003814\n",
      ">> Epoch 62 finished \tANN training loss 0.003774\n",
      ">> Epoch 63 finished \tANN training loss 0.003721\n",
      ">> Epoch 64 finished \tANN training loss 0.003675\n",
      ">> Epoch 65 finished \tANN training loss 0.003255\n",
      ">> Epoch 66 finished \tANN training loss 0.003487\n",
      ">> Epoch 67 finished \tANN training loss 0.003269\n",
      ">> Epoch 68 finished \tANN training loss 0.003429\n",
      ">> Epoch 69 finished \tANN training loss 0.002832\n",
      ">> Epoch 70 finished \tANN training loss 0.003182\n",
      ">> Epoch 71 finished \tANN training loss 0.002601\n",
      ">> Epoch 72 finished \tANN training loss 0.002468\n",
      ">> Epoch 73 finished \tANN training loss 0.002914\n",
      ">> Epoch 74 finished \tANN training loss 0.003348\n",
      ">> Epoch 75 finished \tANN training loss 0.002607\n",
      ">> Epoch 76 finished \tANN training loss 0.003349\n",
      ">> Epoch 77 finished \tANN training loss 0.002008\n",
      ">> Epoch 78 finished \tANN training loss 0.002095\n",
      ">> Epoch 79 finished \tANN training loss 0.001980\n",
      ">> Epoch 80 finished \tANN training loss 0.001866\n",
      ">> Epoch 81 finished \tANN training loss 0.002689\n",
      ">> Epoch 82 finished \tANN training loss 0.002575\n",
      ">> Epoch 83 finished \tANN training loss 0.001595\n",
      ">> Epoch 84 finished \tANN training loss 0.001677\n",
      ">> Epoch 85 finished \tANN training loss 0.001653\n",
      ">> Epoch 86 finished \tANN training loss 0.001344\n",
      ">> Epoch 87 finished \tANN training loss 0.001573\n",
      ">> Epoch 88 finished \tANN training loss 0.001442\n",
      ">> Epoch 89 finished \tANN training loss 0.001547\n",
      ">> Epoch 90 finished \tANN training loss 0.001407\n",
      ">> Epoch 91 finished \tANN training loss 0.001592\n",
      ">> Epoch 92 finished \tANN training loss 0.001101\n",
      ">> Epoch 93 finished \tANN training loss 0.001561\n",
      ">> Epoch 94 finished \tANN training loss 0.001251\n",
      ">> Epoch 95 finished \tANN training loss 0.001467\n",
      ">> Epoch 96 finished \tANN training loss 0.001001\n",
      ">> Epoch 97 finished \tANN training loss 0.001051\n",
      ">> Epoch 98 finished \tANN training loss 0.001021\n",
      ">> Epoch 99 finished \tANN training loss 0.001340\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.895000\n"
     ]
    }
   ],
   "source": [
    "rbm_acc[0] = deep_belief_net(learning_rate_rbm=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.895\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(rbm_acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 2\n",
    "\n",
    "learning_rate_rbm=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 94.117859\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 52.268402\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 64.728737\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 81.388184\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 48.622433\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 34.124168\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 47.252640\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 66.236252\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 42.664062\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 28.159138\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 70.359612\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 78.932182\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 69.850471\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 102.608810\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 102.450813\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 117.915955\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 100.429787\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 128.298401\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 98.683792\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 138.419083\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.697164\n",
      ">> Epoch 1 finished \tANN training loss 0.511610\n",
      ">> Epoch 2 finished \tANN training loss 0.402958\n",
      ">> Epoch 3 finished \tANN training loss 0.354107\n",
      ">> Epoch 4 finished \tANN training loss 0.306825\n",
      ">> Epoch 5 finished \tANN training loss 0.306450\n",
      ">> Epoch 6 finished \tANN training loss 0.240411\n",
      ">> Epoch 7 finished \tANN training loss 0.218182\n",
      ">> Epoch 8 finished \tANN training loss 0.218609\n",
      ">> Epoch 9 finished \tANN training loss 0.171138\n",
      ">> Epoch 10 finished \tANN training loss 0.156822\n",
      ">> Epoch 11 finished \tANN training loss 0.142558\n",
      ">> Epoch 12 finished \tANN training loss 0.118076\n",
      ">> Epoch 13 finished \tANN training loss 0.109248\n",
      ">> Epoch 14 finished \tANN training loss 0.099005\n",
      ">> Epoch 15 finished \tANN training loss 0.102111\n",
      ">> Epoch 16 finished \tANN training loss 0.084690\n",
      ">> Epoch 17 finished \tANN training loss 0.076107\n",
      ">> Epoch 18 finished \tANN training loss 0.069730\n",
      ">> Epoch 19 finished \tANN training loss 0.060799\n",
      ">> Epoch 20 finished \tANN training loss 0.056368\n",
      ">> Epoch 21 finished \tANN training loss 0.051745\n",
      ">> Epoch 22 finished \tANN training loss 0.048154\n",
      ">> Epoch 23 finished \tANN training loss 0.046710\n",
      ">> Epoch 24 finished \tANN training loss 0.045579\n",
      ">> Epoch 25 finished \tANN training loss 0.034644\n",
      ">> Epoch 26 finished \tANN training loss 0.036664\n",
      ">> Epoch 27 finished \tANN training loss 0.031476\n",
      ">> Epoch 28 finished \tANN training loss 0.029145\n",
      ">> Epoch 29 finished \tANN training loss 0.026279\n",
      ">> Epoch 30 finished \tANN training loss 0.024618\n",
      ">> Epoch 31 finished \tANN training loss 0.022609\n",
      ">> Epoch 32 finished \tANN training loss 0.021729\n",
      ">> Epoch 33 finished \tANN training loss 0.018747\n",
      ">> Epoch 34 finished \tANN training loss 0.017236\n",
      ">> Epoch 35 finished \tANN training loss 0.016656\n",
      ">> Epoch 36 finished \tANN training loss 0.016192\n",
      ">> Epoch 37 finished \tANN training loss 0.015062\n",
      ">> Epoch 38 finished \tANN training loss 0.013273\n",
      ">> Epoch 39 finished \tANN training loss 0.011154\n",
      ">> Epoch 40 finished \tANN training loss 0.012872\n",
      ">> Epoch 41 finished \tANN training loss 0.011676\n",
      ">> Epoch 42 finished \tANN training loss 0.010933\n",
      ">> Epoch 43 finished \tANN training loss 0.010014\n",
      ">> Epoch 44 finished \tANN training loss 0.010499\n",
      ">> Epoch 45 finished \tANN training loss 0.011103\n",
      ">> Epoch 46 finished \tANN training loss 0.009723\n",
      ">> Epoch 47 finished \tANN training loss 0.008240\n",
      ">> Epoch 48 finished \tANN training loss 0.008232\n",
      ">> Epoch 49 finished \tANN training loss 0.006863\n",
      ">> Epoch 50 finished \tANN training loss 0.007137\n",
      ">> Epoch 51 finished \tANN training loss 0.006950\n",
      ">> Epoch 52 finished \tANN training loss 0.007001\n",
      ">> Epoch 53 finished \tANN training loss 0.005637\n",
      ">> Epoch 54 finished \tANN training loss 0.005751\n",
      ">> Epoch 55 finished \tANN training loss 0.005118\n",
      ">> Epoch 56 finished \tANN training loss 0.005571\n",
      ">> Epoch 57 finished \tANN training loss 0.004007\n",
      ">> Epoch 58 finished \tANN training loss 0.004096\n",
      ">> Epoch 59 finished \tANN training loss 0.004607\n",
      ">> Epoch 60 finished \tANN training loss 0.005163\n",
      ">> Epoch 61 finished \tANN training loss 0.004935\n",
      ">> Epoch 62 finished \tANN training loss 0.004361\n",
      ">> Epoch 63 finished \tANN training loss 0.003653\n",
      ">> Epoch 64 finished \tANN training loss 0.003530\n",
      ">> Epoch 65 finished \tANN training loss 0.003372\n",
      ">> Epoch 66 finished \tANN training loss 0.003626\n",
      ">> Epoch 67 finished \tANN training loss 0.002988\n",
      ">> Epoch 68 finished \tANN training loss 0.002660\n",
      ">> Epoch 69 finished \tANN training loss 0.002819\n",
      ">> Epoch 70 finished \tANN training loss 0.002478\n",
      ">> Epoch 71 finished \tANN training loss 0.002772\n",
      ">> Epoch 72 finished \tANN training loss 0.002560\n",
      ">> Epoch 73 finished \tANN training loss 0.002443\n",
      ">> Epoch 74 finished \tANN training loss 0.002414\n",
      ">> Epoch 75 finished \tANN training loss 0.002151\n",
      ">> Epoch 76 finished \tANN training loss 0.002132\n",
      ">> Epoch 77 finished \tANN training loss 0.002391\n",
      ">> Epoch 78 finished \tANN training loss 0.002215\n",
      ">> Epoch 79 finished \tANN training loss 0.002274\n",
      ">> Epoch 80 finished \tANN training loss 0.001956\n",
      ">> Epoch 81 finished \tANN training loss 0.001756\n",
      ">> Epoch 82 finished \tANN training loss 0.001879\n",
      ">> Epoch 83 finished \tANN training loss 0.001628\n",
      ">> Epoch 84 finished \tANN training loss 0.001651\n",
      ">> Epoch 85 finished \tANN training loss 0.001509\n",
      ">> Epoch 86 finished \tANN training loss 0.001369\n",
      ">> Epoch 87 finished \tANN training loss 0.001396\n",
      ">> Epoch 88 finished \tANN training loss 0.001494\n",
      ">> Epoch 89 finished \tANN training loss 0.001334\n",
      ">> Epoch 90 finished \tANN training loss 0.001288\n",
      ">> Epoch 91 finished \tANN training loss 0.001407\n",
      ">> Epoch 92 finished \tANN training loss 0.001189\n",
      ">> Epoch 93 finished \tANN training loss 0.001230\n",
      ">> Epoch 94 finished \tANN training loss 0.001383\n",
      ">> Epoch 95 finished \tANN training loss 0.001098\n",
      ">> Epoch 96 finished \tANN training loss 0.001277\n",
      ">> Epoch 97 finished \tANN training loss 0.001462\n",
      ">> Epoch 98 finished \tANN training loss 0.001170\n",
      ">> Epoch 99 finished \tANN training loss 0.001121\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.925000\n"
     ]
    }
   ],
   "source": [
    "rbm_acc[1] = deep_belief_net(learning_rate_rbm=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.925\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(rbm_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 3\n",
    "\n",
    "learning_rate_rbm=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 160.164536\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 210.062515\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 200.079437\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 248.465836\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 249.755737\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 249.164856\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 224.799500\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 185.062149\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 232.932480\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 266.111877\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 5655.453125\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 2792.553711\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 5171.014160\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 7408.390137\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 7781.333984\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 12144.424805\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 16212.991211\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 20276.886719\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 12120.339844\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 18856.796875\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.055440\n",
      ">> Epoch 1 finished \tANN training loss 0.808861\n",
      ">> Epoch 2 finished \tANN training loss 0.639016\n",
      ">> Epoch 3 finished \tANN training loss 0.563351\n",
      ">> Epoch 4 finished \tANN training loss 0.483953\n",
      ">> Epoch 5 finished \tANN training loss 0.408092\n",
      ">> Epoch 6 finished \tANN training loss 0.375929\n",
      ">> Epoch 7 finished \tANN training loss 0.328771\n",
      ">> Epoch 8 finished \tANN training loss 0.322075\n",
      ">> Epoch 9 finished \tANN training loss 0.271771\n",
      ">> Epoch 10 finished \tANN training loss 0.244280\n",
      ">> Epoch 11 finished \tANN training loss 0.241296\n",
      ">> Epoch 12 finished \tANN training loss 0.245327\n",
      ">> Epoch 13 finished \tANN training loss 0.194926\n",
      ">> Epoch 14 finished \tANN training loss 0.183459\n",
      ">> Epoch 15 finished \tANN training loss 0.180053\n",
      ">> Epoch 16 finished \tANN training loss 0.149320\n",
      ">> Epoch 17 finished \tANN training loss 0.148266\n",
      ">> Epoch 18 finished \tANN training loss 0.135523\n",
      ">> Epoch 19 finished \tANN training loss 0.131037\n",
      ">> Epoch 20 finished \tANN training loss 0.120590\n",
      ">> Epoch 21 finished \tANN training loss 0.109792\n",
      ">> Epoch 22 finished \tANN training loss 0.105820\n",
      ">> Epoch 23 finished \tANN training loss 0.094101\n",
      ">> Epoch 24 finished \tANN training loss 0.084638\n",
      ">> Epoch 25 finished \tANN training loss 0.080262\n",
      ">> Epoch 26 finished \tANN training loss 0.081477\n",
      ">> Epoch 27 finished \tANN training loss 0.069037\n",
      ">> Epoch 28 finished \tANN training loss 0.072296\n",
      ">> Epoch 29 finished \tANN training loss 0.060090\n",
      ">> Epoch 30 finished \tANN training loss 0.056916\n",
      ">> Epoch 31 finished \tANN training loss 0.054997\n",
      ">> Epoch 32 finished \tANN training loss 0.053393\n",
      ">> Epoch 33 finished \tANN training loss 0.049004\n",
      ">> Epoch 34 finished \tANN training loss 0.041385\n",
      ">> Epoch 35 finished \tANN training loss 0.041481\n",
      ">> Epoch 36 finished \tANN training loss 0.046173\n",
      ">> Epoch 37 finished \tANN training loss 0.035106\n",
      ">> Epoch 38 finished \tANN training loss 0.035385\n",
      ">> Epoch 39 finished \tANN training loss 0.032396\n",
      ">> Epoch 40 finished \tANN training loss 0.031652\n",
      ">> Epoch 41 finished \tANN training loss 0.032979\n",
      ">> Epoch 42 finished \tANN training loss 0.027082\n",
      ">> Epoch 43 finished \tANN training loss 0.028412\n",
      ">> Epoch 44 finished \tANN training loss 0.029043\n",
      ">> Epoch 45 finished \tANN training loss 0.024924\n",
      ">> Epoch 46 finished \tANN training loss 0.022635\n",
      ">> Epoch 47 finished \tANN training loss 0.021207\n",
      ">> Epoch 48 finished \tANN training loss 0.023568\n",
      ">> Epoch 49 finished \tANN training loss 0.021258\n",
      ">> Epoch 50 finished \tANN training loss 0.018640\n",
      ">> Epoch 51 finished \tANN training loss 0.017819\n",
      ">> Epoch 52 finished \tANN training loss 0.016779\n",
      ">> Epoch 53 finished \tANN training loss 0.015532\n",
      ">> Epoch 54 finished \tANN training loss 0.015907\n",
      ">> Epoch 55 finished \tANN training loss 0.015925\n",
      ">> Epoch 56 finished \tANN training loss 0.013016\n",
      ">> Epoch 57 finished \tANN training loss 0.012353\n",
      ">> Epoch 58 finished \tANN training loss 0.012906\n",
      ">> Epoch 59 finished \tANN training loss 0.010998\n",
      ">> Epoch 60 finished \tANN training loss 0.011406\n",
      ">> Epoch 61 finished \tANN training loss 0.010510\n",
      ">> Epoch 62 finished \tANN training loss 0.011793\n",
      ">> Epoch 63 finished \tANN training loss 0.011529\n",
      ">> Epoch 64 finished \tANN training loss 0.011751\n",
      ">> Epoch 65 finished \tANN training loss 0.010251\n",
      ">> Epoch 66 finished \tANN training loss 0.008513\n",
      ">> Epoch 67 finished \tANN training loss 0.008053\n",
      ">> Epoch 68 finished \tANN training loss 0.009782\n",
      ">> Epoch 69 finished \tANN training loss 0.007693\n",
      ">> Epoch 70 finished \tANN training loss 0.007948\n",
      ">> Epoch 71 finished \tANN training loss 0.007992\n",
      ">> Epoch 72 finished \tANN training loss 0.008252\n",
      ">> Epoch 73 finished \tANN training loss 0.007088\n",
      ">> Epoch 74 finished \tANN training loss 0.006534\n",
      ">> Epoch 75 finished \tANN training loss 0.006166\n",
      ">> Epoch 76 finished \tANN training loss 0.005775\n",
      ">> Epoch 77 finished \tANN training loss 0.005136\n",
      ">> Epoch 78 finished \tANN training loss 0.004990\n",
      ">> Epoch 79 finished \tANN training loss 0.003969\n",
      ">> Epoch 80 finished \tANN training loss 0.004551\n",
      ">> Epoch 81 finished \tANN training loss 0.003993\n",
      ">> Epoch 82 finished \tANN training loss 0.004105\n",
      ">> Epoch 83 finished \tANN training loss 0.004157\n",
      ">> Epoch 84 finished \tANN training loss 0.004661\n",
      ">> Epoch 85 finished \tANN training loss 0.004663\n",
      ">> Epoch 86 finished \tANN training loss 0.003927\n",
      ">> Epoch 87 finished \tANN training loss 0.003849\n",
      ">> Epoch 88 finished \tANN training loss 0.003416\n",
      ">> Epoch 89 finished \tANN training loss 0.003235\n",
      ">> Epoch 90 finished \tANN training loss 0.003278\n",
      ">> Epoch 91 finished \tANN training loss 0.003998\n",
      ">> Epoch 92 finished \tANN training loss 0.004016\n",
      ">> Epoch 93 finished \tANN training loss 0.003774\n",
      ">> Epoch 94 finished \tANN training loss 0.004134\n",
      ">> Epoch 95 finished \tANN training loss 0.004541\n",
      ">> Epoch 96 finished \tANN training loss 0.004077\n",
      ">> Epoch 97 finished \tANN training loss 0.003132\n",
      ">> Epoch 98 finished \tANN training loss 0.002965\n",
      ">> Epoch 99 finished \tANN training loss 0.003258\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.925000\n"
     ]
    }
   ],
   "source": [
    "rbm_acc[2] = deep_belief_net(learning_rate_rbm=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.925\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(rbm_acc[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 4\n",
    "\n",
    "learning_rate_rbm=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 659944.875000\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 787932.250000\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 966067.687500\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 851286.875000\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 768199.187500\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 872080.062500\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 1002583.375000\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 942753.437500\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 845925.625000\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 747044.625000\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 17573135515648.000000\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 35479558815744.000000\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 20757310603264.000000\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 33263219376128.000000\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 20254757486592.000000\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 33756924608512.000000\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 16201875980288.000000\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 32528901603328.000000\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 20109468893184.000000\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 24747140186112.000000\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 2.302652\n",
      ">> Epoch 1 finished \tANN training loss 2.302344\n",
      ">> Epoch 2 finished \tANN training loss 2.302136\n",
      ">> Epoch 3 finished \tANN training loss 2.301996\n",
      ">> Epoch 4 finished \tANN training loss 2.301924\n",
      ">> Epoch 5 finished \tANN training loss 2.301873\n",
      ">> Epoch 6 finished \tANN training loss 2.301846\n",
      ">> Epoch 7 finished \tANN training loss 2.301834\n",
      ">> Epoch 8 finished \tANN training loss 2.301826\n",
      ">> Epoch 9 finished \tANN training loss 2.301821\n",
      ">> Epoch 10 finished \tANN training loss 2.301816\n",
      ">> Epoch 11 finished \tANN training loss 2.301815\n",
      ">> Epoch 12 finished \tANN training loss 2.301812\n",
      ">> Epoch 13 finished \tANN training loss 2.301810\n",
      ">> Epoch 14 finished \tANN training loss 2.301809\n",
      ">> Epoch 15 finished \tANN training loss 2.301808\n",
      ">> Epoch 16 finished \tANN training loss 2.301805\n",
      ">> Epoch 17 finished \tANN training loss 2.301807\n",
      ">> Epoch 18 finished \tANN training loss 2.301808\n",
      ">> Epoch 19 finished \tANN training loss 2.301808\n",
      ">> Epoch 20 finished \tANN training loss 2.301806\n",
      ">> Epoch 21 finished \tANN training loss 2.301806\n",
      ">> Epoch 22 finished \tANN training loss 2.301807\n",
      ">> Epoch 23 finished \tANN training loss 2.301808\n",
      ">> Epoch 24 finished \tANN training loss 2.301808\n",
      ">> Epoch 25 finished \tANN training loss 2.301807\n",
      ">> Epoch 26 finished \tANN training loss 2.301807\n",
      ">> Epoch 27 finished \tANN training loss 2.301806\n",
      ">> Epoch 28 finished \tANN training loss 2.301807\n",
      ">> Epoch 29 finished \tANN training loss 2.301807\n",
      ">> Epoch 30 finished \tANN training loss 2.301808\n",
      ">> Epoch 31 finished \tANN training loss 2.301806\n",
      ">> Epoch 32 finished \tANN training loss 2.301806\n",
      ">> Epoch 33 finished \tANN training loss 2.301806\n",
      ">> Epoch 34 finished \tANN training loss 2.301807\n",
      ">> Epoch 35 finished \tANN training loss 2.301805\n",
      ">> Epoch 36 finished \tANN training loss 2.301806\n",
      ">> Epoch 37 finished \tANN training loss 2.301806\n",
      ">> Epoch 38 finished \tANN training loss 2.301808\n",
      ">> Epoch 39 finished \tANN training loss 2.301807\n",
      ">> Epoch 40 finished \tANN training loss 2.301809\n",
      ">> Epoch 41 finished \tANN training loss 2.301808\n",
      ">> Epoch 42 finished \tANN training loss 2.301808\n",
      ">> Epoch 43 finished \tANN training loss 2.301809\n",
      ">> Epoch 44 finished \tANN training loss 2.301807\n",
      ">> Epoch 45 finished \tANN training loss 2.301810\n",
      ">> Epoch 46 finished \tANN training loss 2.301808\n",
      ">> Epoch 47 finished \tANN training loss 2.301808\n",
      ">> Epoch 48 finished \tANN training loss 2.301811\n",
      ">> Epoch 49 finished \tANN training loss 2.301811\n",
      ">> Epoch 50 finished \tANN training loss 2.301810\n",
      ">> Epoch 51 finished \tANN training loss 2.301807\n",
      ">> Epoch 52 finished \tANN training loss 2.301809\n",
      ">> Epoch 53 finished \tANN training loss 2.301808\n",
      ">> Epoch 54 finished \tANN training loss 2.301808\n",
      ">> Epoch 55 finished \tANN training loss 2.301809\n",
      ">> Epoch 56 finished \tANN training loss 2.301810\n",
      ">> Epoch 57 finished \tANN training loss 2.301809\n",
      ">> Epoch 58 finished \tANN training loss 2.301807\n",
      ">> Epoch 59 finished \tANN training loss 2.301806\n",
      ">> Epoch 60 finished \tANN training loss 2.301808\n",
      ">> Epoch 61 finished \tANN training loss 2.301808\n",
      ">> Epoch 62 finished \tANN training loss 2.301808\n",
      ">> Epoch 63 finished \tANN training loss 2.301809\n",
      ">> Epoch 64 finished \tANN training loss 2.301808\n",
      ">> Epoch 65 finished \tANN training loss 2.301807\n",
      ">> Epoch 66 finished \tANN training loss 2.301806\n",
      ">> Epoch 67 finished \tANN training loss 2.301806\n",
      ">> Epoch 68 finished \tANN training loss 2.301806\n",
      ">> Epoch 69 finished \tANN training loss 2.301807\n",
      ">> Epoch 70 finished \tANN training loss 2.301806\n",
      ">> Epoch 71 finished \tANN training loss 2.301807\n",
      ">> Epoch 72 finished \tANN training loss 2.301809\n",
      ">> Epoch 73 finished \tANN training loss 2.301808\n",
      ">> Epoch 74 finished \tANN training loss 2.301806\n",
      ">> Epoch 75 finished \tANN training loss 2.301807\n",
      ">> Epoch 76 finished \tANN training loss 2.301807\n",
      ">> Epoch 77 finished \tANN training loss 2.301805\n",
      ">> Epoch 78 finished \tANN training loss 2.301806\n",
      ">> Epoch 79 finished \tANN training loss 2.301805\n",
      ">> Epoch 80 finished \tANN training loss 2.301805\n",
      ">> Epoch 81 finished \tANN training loss 2.301805\n",
      ">> Epoch 82 finished \tANN training loss 2.301808\n",
      ">> Epoch 83 finished \tANN training loss 2.301808\n",
      ">> Epoch 84 finished \tANN training loss 2.301810\n",
      ">> Epoch 85 finished \tANN training loss 2.301808\n",
      ">> Epoch 86 finished \tANN training loss 2.301808\n",
      ">> Epoch 87 finished \tANN training loss 2.301808\n",
      ">> Epoch 88 finished \tANN training loss 2.301806\n",
      ">> Epoch 89 finished \tANN training loss 2.301808\n",
      ">> Epoch 90 finished \tANN training loss 2.301809\n",
      ">> Epoch 91 finished \tANN training loss 2.301808\n",
      ">> Epoch 92 finished \tANN training loss 2.301808\n",
      ">> Epoch 93 finished \tANN training loss 2.301809\n",
      ">> Epoch 94 finished \tANN training loss 2.301811\n",
      ">> Epoch 95 finished \tANN training loss 2.301808\n",
      ">> Epoch 96 finished \tANN training loss 2.301809\n",
      ">> Epoch 97 finished \tANN training loss 2.301805\n",
      ">> Epoch 98 finished \tANN training loss 2.301808\n",
      ">> Epoch 99 finished \tANN training loss 2.301808\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.070000\n"
     ]
    }
   ],
   "source": [
    "rbm_acc[3] = deep_belief_net(learning_rate_rbm=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.07\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(rbm_acc[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 5\n",
    "\n",
    "learning_rate_rbm=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 64719360.000000\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 52101428.000000\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 73726448.000000\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 78145640.000000\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 78830472.000000\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 61287808.000000\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 56961512.000000\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 57858744.000000\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 58905796.000000\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 60400040.000000\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 158461615795077120000.000000\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 117894105145398001664.000000\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 129137271246384267264.000000\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 137155209103289614336.000000\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 106870621467641380864.000000\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 121985801328724541440.000000\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 133737522343882784768.000000\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 136982005235589316608.000000\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 132234516336992059392.000000\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 118541972580855709696.000000\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 2.303047\n",
      ">> Epoch 1 finished \tANN training loss 2.302773\n",
      ">> Epoch 2 finished \tANN training loss 2.302444\n",
      ">> Epoch 3 finished \tANN training loss 2.302197\n",
      ">> Epoch 4 finished \tANN training loss 2.302058\n",
      ">> Epoch 5 finished \tANN training loss 2.301948\n",
      ">> Epoch 6 finished \tANN training loss 2.301892\n",
      ">> Epoch 7 finished \tANN training loss 2.301873\n",
      ">> Epoch 8 finished \tANN training loss 2.301846\n",
      ">> Epoch 9 finished \tANN training loss 2.301833\n",
      ">> Epoch 10 finished \tANN training loss 2.301822\n",
      ">> Epoch 11 finished \tANN training loss 2.301822\n",
      ">> Epoch 12 finished \tANN training loss 2.301822\n",
      ">> Epoch 13 finished \tANN training loss 2.301818\n",
      ">> Epoch 14 finished \tANN training loss 2.301816\n",
      ">> Epoch 15 finished \tANN training loss 2.301815\n",
      ">> Epoch 16 finished \tANN training loss 2.301813\n",
      ">> Epoch 17 finished \tANN training loss 2.301807\n",
      ">> Epoch 18 finished \tANN training loss 2.301844\n",
      ">> Epoch 19 finished \tANN training loss 2.301825\n",
      ">> Epoch 20 finished \tANN training loss 2.301814\n",
      ">> Epoch 21 finished \tANN training loss 2.301814\n",
      ">> Epoch 22 finished \tANN training loss 2.301811\n",
      ">> Epoch 23 finished \tANN training loss 2.301810\n",
      ">> Epoch 24 finished \tANN training loss 2.301813\n",
      ">> Epoch 25 finished \tANN training loss 2.301808\n",
      ">> Epoch 26 finished \tANN training loss 2.301809\n",
      ">> Epoch 27 finished \tANN training loss 2.301810\n",
      ">> Epoch 28 finished \tANN training loss 2.301809\n",
      ">> Epoch 29 finished \tANN training loss 2.301807\n",
      ">> Epoch 30 finished \tANN training loss 2.301805\n",
      ">> Epoch 31 finished \tANN training loss 2.300120\n",
      ">> Epoch 32 finished \tANN training loss 2.301818\n",
      ">> Epoch 33 finished \tANN training loss 2.301810\n",
      ">> Epoch 34 finished \tANN training loss 2.301826\n",
      ">> Epoch 35 finished \tANN training loss 2.301830\n",
      ">> Epoch 36 finished \tANN training loss 2.301830\n",
      ">> Epoch 37 finished \tANN training loss 2.301830\n",
      ">> Epoch 38 finished \tANN training loss 2.301839\n",
      ">> Epoch 39 finished \tANN training loss 2.301849\n",
      ">> Epoch 40 finished \tANN training loss 2.301862\n",
      ">> Epoch 41 finished \tANN training loss 2.301884\n",
      ">> Epoch 42 finished \tANN training loss 2.301927\n",
      ">> Epoch 43 finished \tANN training loss 2.301968\n",
      ">> Epoch 44 finished \tANN training loss 2.302024\n",
      ">> Epoch 45 finished \tANN training loss 2.302189\n",
      ">> Epoch 46 finished \tANN training loss 2.302360\n",
      ">> Epoch 47 finished \tANN training loss 2.302679\n",
      ">> Epoch 48 finished \tANN training loss 2.293964\n",
      ">> Epoch 49 finished \tANN training loss 2.303125\n",
      ">> Epoch 50 finished \tANN training loss 2.303401\n",
      ">> Epoch 51 finished \tANN training loss 2.303262\n",
      ">> Epoch 52 finished \tANN training loss 2.303241\n",
      ">> Epoch 53 finished \tANN training loss 2.303434\n",
      ">> Epoch 54 finished \tANN training loss 2.303587\n",
      ">> Epoch 55 finished \tANN training loss 2.303831\n",
      ">> Epoch 56 finished \tANN training loss 2.303696\n",
      ">> Epoch 57 finished \tANN training loss 2.303664\n",
      ">> Epoch 58 finished \tANN training loss 2.303646\n",
      ">> Epoch 59 finished \tANN training loss 2.303725\n",
      ">> Epoch 60 finished \tANN training loss 2.303984\n",
      ">> Epoch 61 finished \tANN training loss 2.303880\n",
      ">> Epoch 62 finished \tANN training loss 2.304004\n",
      ">> Epoch 63 finished \tANN training loss 2.304679\n",
      ">> Epoch 64 finished \tANN training loss 2.304989\n",
      ">> Epoch 65 finished \tANN training loss 2.304720\n",
      ">> Epoch 66 finished \tANN training loss 2.304526\n",
      ">> Epoch 67 finished \tANN training loss 2.304274\n",
      ">> Epoch 68 finished \tANN training loss 2.304306\n",
      ">> Epoch 69 finished \tANN training loss 2.304354\n",
      ">> Epoch 70 finished \tANN training loss 2.304672\n",
      ">> Epoch 71 finished \tANN training loss 2.304369\n",
      ">> Epoch 72 finished \tANN training loss 2.304423\n",
      ">> Epoch 73 finished \tANN training loss 2.304207\n",
      ">> Epoch 74 finished \tANN training loss 2.304089\n",
      ">> Epoch 75 finished \tANN training loss 2.304088\n",
      ">> Epoch 76 finished \tANN training loss 2.304371\n",
      ">> Epoch 77 finished \tANN training loss 2.304699\n",
      ">> Epoch 78 finished \tANN training loss 2.304737\n",
      ">> Epoch 79 finished \tANN training loss 2.304569\n",
      ">> Epoch 80 finished \tANN training loss 2.304573\n",
      ">> Epoch 81 finished \tANN training loss 2.304322\n",
      ">> Epoch 82 finished \tANN training loss 2.304173\n",
      ">> Epoch 83 finished \tANN training loss 2.304614\n",
      ">> Epoch 84 finished \tANN training loss 2.304327\n",
      ">> Epoch 85 finished \tANN training loss 2.304223\n",
      ">> Epoch 86 finished \tANN training loss 2.304059\n",
      ">> Epoch 87 finished \tANN training loss 2.303887\n",
      ">> Epoch 88 finished \tANN training loss 2.304258\n",
      ">> Epoch 89 finished \tANN training loss 2.304428\n",
      ">> Epoch 90 finished \tANN training loss 2.304466\n",
      ">> Epoch 91 finished \tANN training loss 2.304476\n",
      ">> Epoch 92 finished \tANN training loss 2.304698\n",
      ">> Epoch 93 finished \tANN training loss 2.304370\n",
      ">> Epoch 94 finished \tANN training loss 2.304367\n",
      ">> Epoch 95 finished \tANN training loss 2.304404\n",
      ">> Epoch 96 finished \tANN training loss 2.304243\n",
      ">> Epoch 97 finished \tANN training loss 2.304262\n",
      ">> Epoch 98 finished \tANN training loss 2.304664\n",
      ">> Epoch 99 finished \tANN training loss 2.304306\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.085000\n"
     ]
    }
   ],
   "source": [
    "rbm_acc[4] = deep_belief_net(learning_rate_rbm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.085\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(rbm_acc[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.895, 0.925, 0.925, 0.07, 0.085]\n",
      "Most accurate rbm learning_rate setting is Setting 2\n"
     ]
    }
   ],
   "source": [
    "# Collated results\n",
    "print(rbm_acc)\n",
    "print('Most accurate rbm learning_rate setting is Setting ' + str(rbm_acc.index(max(rbm_acc)) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create bar graph\n",
    "plt.clf()\n",
    "objects = ('0.01', '0.05', '0.10', '0.50', '1.0')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [0.895,0.91,0.885,0.595, 0.215]\n",
    "\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.8)\n",
    "plt.xticks(y_pos, objects, rotation=30)\n",
    "plt.xlabel('Settings')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('learning_rate_rbm Settings')\n",
    " \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Learning Rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store accuracies\n",
    "learning_rate_acc = [0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 1\n",
    "\n",
    "learning_rate=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 69.933800\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 86.468925\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 60.413849\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 37.420773\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 57.553745\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 48.849998\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 46.914040\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 32.126434\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 43.242886\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 46.625622\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 208.352875\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 196.485397\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 153.115433\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 172.141602\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 146.398560\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 135.346878\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 246.178574\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 313.933014\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 209.161133\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 190.459106\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.629365\n",
      ">> Epoch 1 finished \tANN training loss 1.279105\n",
      ">> Epoch 2 finished \tANN training loss 1.059501\n",
      ">> Epoch 3 finished \tANN training loss 0.914509\n",
      ">> Epoch 4 finished \tANN training loss 0.816454\n",
      ">> Epoch 5 finished \tANN training loss 0.744698\n",
      ">> Epoch 6 finished \tANN training loss 0.686117\n",
      ">> Epoch 7 finished \tANN training loss 0.645617\n",
      ">> Epoch 8 finished \tANN training loss 0.608573\n",
      ">> Epoch 9 finished \tANN training loss 0.577660\n",
      ">> Epoch 10 finished \tANN training loss 0.550672\n",
      ">> Epoch 11 finished \tANN training loss 0.532825\n",
      ">> Epoch 12 finished \tANN training loss 0.508937\n",
      ">> Epoch 13 finished \tANN training loss 0.495999\n",
      ">> Epoch 14 finished \tANN training loss 0.477021\n",
      ">> Epoch 15 finished \tANN training loss 0.465330\n",
      ">> Epoch 16 finished \tANN training loss 0.452623\n",
      ">> Epoch 17 finished \tANN training loss 0.440344\n",
      ">> Epoch 18 finished \tANN training loss 0.432176\n",
      ">> Epoch 19 finished \tANN training loss 0.419401\n",
      ">> Epoch 20 finished \tANN training loss 0.409559\n",
      ">> Epoch 21 finished \tANN training loss 0.403169\n",
      ">> Epoch 22 finished \tANN training loss 0.395906\n",
      ">> Epoch 23 finished \tANN training loss 0.388536\n",
      ">> Epoch 24 finished \tANN training loss 0.381234\n",
      ">> Epoch 25 finished \tANN training loss 0.372231\n",
      ">> Epoch 26 finished \tANN training loss 0.365119\n",
      ">> Epoch 27 finished \tANN training loss 0.361248\n",
      ">> Epoch 28 finished \tANN training loss 0.359318\n",
      ">> Epoch 29 finished \tANN training loss 0.348451\n",
      ">> Epoch 30 finished \tANN training loss 0.343703\n",
      ">> Epoch 31 finished \tANN training loss 0.338539\n",
      ">> Epoch 32 finished \tANN training loss 0.332721\n",
      ">> Epoch 33 finished \tANN training loss 0.326364\n",
      ">> Epoch 34 finished \tANN training loss 0.322866\n",
      ">> Epoch 35 finished \tANN training loss 0.315578\n",
      ">> Epoch 36 finished \tANN training loss 0.311409\n",
      ">> Epoch 37 finished \tANN training loss 0.307143\n",
      ">> Epoch 38 finished \tANN training loss 0.301555\n",
      ">> Epoch 39 finished \tANN training loss 0.298471\n",
      ">> Epoch 40 finished \tANN training loss 0.294797\n",
      ">> Epoch 41 finished \tANN training loss 0.291421\n",
      ">> Epoch 42 finished \tANN training loss 0.286068\n",
      ">> Epoch 43 finished \tANN training loss 0.280822\n",
      ">> Epoch 44 finished \tANN training loss 0.281135\n",
      ">> Epoch 45 finished \tANN training loss 0.272934\n",
      ">> Epoch 46 finished \tANN training loss 0.270998\n",
      ">> Epoch 47 finished \tANN training loss 0.265909\n",
      ">> Epoch 48 finished \tANN training loss 0.262369\n",
      ">> Epoch 49 finished \tANN training loss 0.260038\n",
      ">> Epoch 50 finished \tANN training loss 0.255623\n",
      ">> Epoch 51 finished \tANN training loss 0.254204\n",
      ">> Epoch 52 finished \tANN training loss 0.250070\n",
      ">> Epoch 53 finished \tANN training loss 0.250241\n",
      ">> Epoch 54 finished \tANN training loss 0.244649\n",
      ">> Epoch 55 finished \tANN training loss 0.240994\n",
      ">> Epoch 56 finished \tANN training loss 0.238792\n",
      ">> Epoch 57 finished \tANN training loss 0.235977\n",
      ">> Epoch 58 finished \tANN training loss 0.234095\n",
      ">> Epoch 59 finished \tANN training loss 0.230959\n",
      ">> Epoch 60 finished \tANN training loss 0.226960\n",
      ">> Epoch 61 finished \tANN training loss 0.224025\n",
      ">> Epoch 62 finished \tANN training loss 0.221200\n",
      ">> Epoch 63 finished \tANN training loss 0.218461\n",
      ">> Epoch 64 finished \tANN training loss 0.217176\n",
      ">> Epoch 65 finished \tANN training loss 0.212729\n",
      ">> Epoch 66 finished \tANN training loss 0.210462\n",
      ">> Epoch 67 finished \tANN training loss 0.207862\n",
      ">> Epoch 68 finished \tANN training loss 0.206948\n",
      ">> Epoch 69 finished \tANN training loss 0.204125\n",
      ">> Epoch 70 finished \tANN training loss 0.201825\n",
      ">> Epoch 71 finished \tANN training loss 0.200427\n",
      ">> Epoch 72 finished \tANN training loss 0.196757\n",
      ">> Epoch 73 finished \tANN training loss 0.196490\n",
      ">> Epoch 74 finished \tANN training loss 0.192354\n",
      ">> Epoch 75 finished \tANN training loss 0.189790\n",
      ">> Epoch 76 finished \tANN training loss 0.189221\n",
      ">> Epoch 77 finished \tANN training loss 0.187079\n",
      ">> Epoch 78 finished \tANN training loss 0.183960\n",
      ">> Epoch 79 finished \tANN training loss 0.181910\n",
      ">> Epoch 80 finished \tANN training loss 0.178902\n",
      ">> Epoch 81 finished \tANN training loss 0.177419\n",
      ">> Epoch 82 finished \tANN training loss 0.175729\n",
      ">> Epoch 83 finished \tANN training loss 0.173142\n",
      ">> Epoch 84 finished \tANN training loss 0.171444\n",
      ">> Epoch 85 finished \tANN training loss 0.170271\n",
      ">> Epoch 86 finished \tANN training loss 0.169083\n",
      ">> Epoch 87 finished \tANN training loss 0.168768\n",
      ">> Epoch 88 finished \tANN training loss 0.164169\n",
      ">> Epoch 89 finished \tANN training loss 0.162057\n",
      ">> Epoch 90 finished \tANN training loss 0.160132\n",
      ">> Epoch 91 finished \tANN training loss 0.158354\n",
      ">> Epoch 92 finished \tANN training loss 0.157897\n",
      ">> Epoch 93 finished \tANN training loss 0.157818\n",
      ">> Epoch 94 finished \tANN training loss 0.154748\n",
      ">> Epoch 95 finished \tANN training loss 0.152439\n",
      ">> Epoch 96 finished \tANN training loss 0.151026\n",
      ">> Epoch 97 finished \tANN training loss 0.150095\n",
      ">> Epoch 98 finished \tANN training loss 0.147264\n",
      ">> Epoch 99 finished \tANN training loss 0.146539\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.890000\n"
     ]
    }
   ],
   "source": [
    "learning_rate_acc[0] = deep_belief_net(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.89\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(learning_rate_acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 2\n",
    "\n",
    "learning_rate=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 103.053932\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 83.255623\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 67.956223\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 58.407803\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 44.890244\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 61.732590\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 28.954615\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 58.617508\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 46.534317\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 56.053905\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 238.889465\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 256.781281\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 257.730835\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 223.796326\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 225.551773\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 273.075012\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 274.740936\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 248.027222\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 306.634583\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 440.243591\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.782082\n",
      ">> Epoch 1 finished \tANN training loss 0.593705\n",
      ">> Epoch 2 finished \tANN training loss 0.505861\n",
      ">> Epoch 3 finished \tANN training loss 0.477037\n",
      ">> Epoch 4 finished \tANN training loss 0.439587\n",
      ">> Epoch 5 finished \tANN training loss 0.389118\n",
      ">> Epoch 6 finished \tANN training loss 0.359429\n",
      ">> Epoch 7 finished \tANN training loss 0.334283\n",
      ">> Epoch 8 finished \tANN training loss 0.324409\n",
      ">> Epoch 9 finished \tANN training loss 0.290854\n",
      ">> Epoch 10 finished \tANN training loss 0.281184\n",
      ">> Epoch 11 finished \tANN training loss 0.256170\n",
      ">> Epoch 12 finished \tANN training loss 0.253099\n",
      ">> Epoch 13 finished \tANN training loss 0.231107\n",
      ">> Epoch 14 finished \tANN training loss 0.216666\n",
      ">> Epoch 15 finished \tANN training loss 0.204839\n",
      ">> Epoch 16 finished \tANN training loss 0.206297\n",
      ">> Epoch 17 finished \tANN training loss 0.180103\n",
      ">> Epoch 18 finished \tANN training loss 0.165963\n",
      ">> Epoch 19 finished \tANN training loss 0.165647\n",
      ">> Epoch 20 finished \tANN training loss 0.156310\n",
      ">> Epoch 21 finished \tANN training loss 0.146792\n",
      ">> Epoch 22 finished \tANN training loss 0.133899\n",
      ">> Epoch 23 finished \tANN training loss 0.130649\n",
      ">> Epoch 24 finished \tANN training loss 0.122780\n",
      ">> Epoch 25 finished \tANN training loss 0.115134\n",
      ">> Epoch 26 finished \tANN training loss 0.116041\n",
      ">> Epoch 27 finished \tANN training loss 0.104789\n",
      ">> Epoch 28 finished \tANN training loss 0.104240\n",
      ">> Epoch 29 finished \tANN training loss 0.098319\n",
      ">> Epoch 30 finished \tANN training loss 0.098756\n",
      ">> Epoch 31 finished \tANN training loss 0.089121\n",
      ">> Epoch 32 finished \tANN training loss 0.085279\n",
      ">> Epoch 33 finished \tANN training loss 0.082884\n",
      ">> Epoch 34 finished \tANN training loss 0.078826\n",
      ">> Epoch 35 finished \tANN training loss 0.069467\n",
      ">> Epoch 36 finished \tANN training loss 0.067952\n",
      ">> Epoch 37 finished \tANN training loss 0.064318\n",
      ">> Epoch 38 finished \tANN training loss 0.061697\n",
      ">> Epoch 39 finished \tANN training loss 0.063148\n",
      ">> Epoch 40 finished \tANN training loss 0.055854\n",
      ">> Epoch 41 finished \tANN training loss 0.051667\n",
      ">> Epoch 42 finished \tANN training loss 0.052734\n",
      ">> Epoch 43 finished \tANN training loss 0.051653\n",
      ">> Epoch 44 finished \tANN training loss 0.048571\n",
      ">> Epoch 45 finished \tANN training loss 0.045261\n",
      ">> Epoch 46 finished \tANN training loss 0.042220\n",
      ">> Epoch 47 finished \tANN training loss 0.043575\n",
      ">> Epoch 48 finished \tANN training loss 0.039726\n",
      ">> Epoch 49 finished \tANN training loss 0.038059\n",
      ">> Epoch 50 finished \tANN training loss 0.041982\n",
      ">> Epoch 51 finished \tANN training loss 0.036944\n",
      ">> Epoch 52 finished \tANN training loss 0.033312\n",
      ">> Epoch 53 finished \tANN training loss 0.031878\n",
      ">> Epoch 54 finished \tANN training loss 0.034168\n",
      ">> Epoch 55 finished \tANN training loss 0.030390\n",
      ">> Epoch 56 finished \tANN training loss 0.029356\n",
      ">> Epoch 57 finished \tANN training loss 0.027582\n",
      ">> Epoch 58 finished \tANN training loss 0.026400\n",
      ">> Epoch 59 finished \tANN training loss 0.026022\n",
      ">> Epoch 60 finished \tANN training loss 0.026777\n",
      ">> Epoch 61 finished \tANN training loss 0.024125\n",
      ">> Epoch 62 finished \tANN training loss 0.023355\n",
      ">> Epoch 63 finished \tANN training loss 0.021910\n",
      ">> Epoch 64 finished \tANN training loss 0.021456\n",
      ">> Epoch 65 finished \tANN training loss 0.021160\n",
      ">> Epoch 66 finished \tANN training loss 0.020390\n",
      ">> Epoch 67 finished \tANN training loss 0.018512\n",
      ">> Epoch 68 finished \tANN training loss 0.019700\n",
      ">> Epoch 69 finished \tANN training loss 0.017312\n",
      ">> Epoch 70 finished \tANN training loss 0.016398\n",
      ">> Epoch 71 finished \tANN training loss 0.015336\n",
      ">> Epoch 72 finished \tANN training loss 0.015841\n",
      ">> Epoch 73 finished \tANN training loss 0.015287\n",
      ">> Epoch 74 finished \tANN training loss 0.015168\n",
      ">> Epoch 75 finished \tANN training loss 0.014664\n",
      ">> Epoch 76 finished \tANN training loss 0.014059\n",
      ">> Epoch 77 finished \tANN training loss 0.014142\n",
      ">> Epoch 78 finished \tANN training loss 0.013599\n",
      ">> Epoch 79 finished \tANN training loss 0.013549\n",
      ">> Epoch 80 finished \tANN training loss 0.013242\n",
      ">> Epoch 81 finished \tANN training loss 0.012400\n",
      ">> Epoch 82 finished \tANN training loss 0.013169\n",
      ">> Epoch 83 finished \tANN training loss 0.011579\n",
      ">> Epoch 84 finished \tANN training loss 0.011487\n",
      ">> Epoch 85 finished \tANN training loss 0.012151\n",
      ">> Epoch 86 finished \tANN training loss 0.010846\n",
      ">> Epoch 87 finished \tANN training loss 0.010092\n",
      ">> Epoch 88 finished \tANN training loss 0.010239\n",
      ">> Epoch 89 finished \tANN training loss 0.009644\n",
      ">> Epoch 90 finished \tANN training loss 0.009252\n",
      ">> Epoch 91 finished \tANN training loss 0.008728\n",
      ">> Epoch 92 finished \tANN training loss 0.010050\n",
      ">> Epoch 93 finished \tANN training loss 0.009089\n",
      ">> Epoch 94 finished \tANN training loss 0.008579\n",
      ">> Epoch 95 finished \tANN training loss 0.008284\n",
      ">> Epoch 96 finished \tANN training loss 0.008328\n",
      ">> Epoch 97 finished \tANN training loss 0.008808\n",
      ">> Epoch 98 finished \tANN training loss 0.008253\n",
      ">> Epoch 99 finished \tANN training loss 0.008129\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.920000\n"
     ]
    }
   ],
   "source": [
    "learning_rate_acc[1] = deep_belief_net(learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.92\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(learning_rate_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 3\n",
    "\n",
    "learning_rate=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 47.652290\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 55.271156\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 48.212074\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 33.137135\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 39.583824\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 43.962959\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 35.656364\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 36.410442\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 34.619732\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 32.343910\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 55.459496\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 95.580605\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 105.353058\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 130.964539\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 138.146347\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 87.875450\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 118.518761\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 89.706543\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 109.253838\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 135.404419\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.657151\n",
      ">> Epoch 1 finished \tANN training loss 0.493530\n",
      ">> Epoch 2 finished \tANN training loss 0.377674\n",
      ">> Epoch 3 finished \tANN training loss 0.373210\n",
      ">> Epoch 4 finished \tANN training loss 0.274853\n",
      ">> Epoch 5 finished \tANN training loss 0.261747\n",
      ">> Epoch 6 finished \tANN training loss 0.222844\n",
      ">> Epoch 7 finished \tANN training loss 0.206062\n",
      ">> Epoch 8 finished \tANN training loss 0.171021\n",
      ">> Epoch 9 finished \tANN training loss 0.149654\n",
      ">> Epoch 10 finished \tANN training loss 0.137944\n",
      ">> Epoch 11 finished \tANN training loss 0.168323\n",
      ">> Epoch 12 finished \tANN training loss 0.123973\n",
      ">> Epoch 13 finished \tANN training loss 0.095754\n",
      ">> Epoch 14 finished \tANN training loss 0.109095\n",
      ">> Epoch 15 finished \tANN training loss 0.083682\n",
      ">> Epoch 16 finished \tANN training loss 0.072084\n",
      ">> Epoch 17 finished \tANN training loss 0.066666\n",
      ">> Epoch 18 finished \tANN training loss 0.071365\n",
      ">> Epoch 19 finished \tANN training loss 0.055275\n",
      ">> Epoch 20 finished \tANN training loss 0.051152\n",
      ">> Epoch 21 finished \tANN training loss 0.045655\n",
      ">> Epoch 22 finished \tANN training loss 0.042054\n",
      ">> Epoch 23 finished \tANN training loss 0.034709\n",
      ">> Epoch 24 finished \tANN training loss 0.033314\n",
      ">> Epoch 25 finished \tANN training loss 0.032465\n",
      ">> Epoch 26 finished \tANN training loss 0.026967\n",
      ">> Epoch 27 finished \tANN training loss 0.024518\n",
      ">> Epoch 28 finished \tANN training loss 0.025480\n",
      ">> Epoch 29 finished \tANN training loss 0.023100\n",
      ">> Epoch 30 finished \tANN training loss 0.020183\n",
      ">> Epoch 31 finished \tANN training loss 0.019314\n",
      ">> Epoch 32 finished \tANN training loss 0.017291\n",
      ">> Epoch 33 finished \tANN training loss 0.018470\n",
      ">> Epoch 34 finished \tANN training loss 0.016602\n",
      ">> Epoch 35 finished \tANN training loss 0.014977\n",
      ">> Epoch 36 finished \tANN training loss 0.014587\n",
      ">> Epoch 37 finished \tANN training loss 0.013785\n",
      ">> Epoch 38 finished \tANN training loss 0.012783\n",
      ">> Epoch 39 finished \tANN training loss 0.011944\n",
      ">> Epoch 40 finished \tANN training loss 0.011293\n",
      ">> Epoch 41 finished \tANN training loss 0.010660\n",
      ">> Epoch 42 finished \tANN training loss 0.010000\n",
      ">> Epoch 43 finished \tANN training loss 0.009628\n",
      ">> Epoch 44 finished \tANN training loss 0.008598\n",
      ">> Epoch 45 finished \tANN training loss 0.008143\n",
      ">> Epoch 46 finished \tANN training loss 0.007193\n",
      ">> Epoch 47 finished \tANN training loss 0.007968\n",
      ">> Epoch 48 finished \tANN training loss 0.007827\n",
      ">> Epoch 49 finished \tANN training loss 0.007814\n",
      ">> Epoch 50 finished \tANN training loss 0.006888\n",
      ">> Epoch 51 finished \tANN training loss 0.006506\n",
      ">> Epoch 52 finished \tANN training loss 0.005970\n",
      ">> Epoch 53 finished \tANN training loss 0.006577\n",
      ">> Epoch 54 finished \tANN training loss 0.006193\n",
      ">> Epoch 55 finished \tANN training loss 0.004916\n",
      ">> Epoch 56 finished \tANN training loss 0.004508\n",
      ">> Epoch 57 finished \tANN training loss 0.004520\n",
      ">> Epoch 58 finished \tANN training loss 0.004065\n",
      ">> Epoch 59 finished \tANN training loss 0.003666\n",
      ">> Epoch 60 finished \tANN training loss 0.003412\n",
      ">> Epoch 61 finished \tANN training loss 0.004051\n",
      ">> Epoch 62 finished \tANN training loss 0.003390\n",
      ">> Epoch 63 finished \tANN training loss 0.003180\n",
      ">> Epoch 64 finished \tANN training loss 0.003303\n",
      ">> Epoch 65 finished \tANN training loss 0.003750\n",
      ">> Epoch 66 finished \tANN training loss 0.003348\n",
      ">> Epoch 67 finished \tANN training loss 0.003086\n",
      ">> Epoch 68 finished \tANN training loss 0.003153\n",
      ">> Epoch 69 finished \tANN training loss 0.003046\n",
      ">> Epoch 70 finished \tANN training loss 0.003026\n",
      ">> Epoch 71 finished \tANN training loss 0.002603\n",
      ">> Epoch 72 finished \tANN training loss 0.002791\n",
      ">> Epoch 73 finished \tANN training loss 0.002581\n",
      ">> Epoch 74 finished \tANN training loss 0.002312\n",
      ">> Epoch 75 finished \tANN training loss 0.002532\n",
      ">> Epoch 76 finished \tANN training loss 0.002242\n",
      ">> Epoch 77 finished \tANN training loss 0.002637\n",
      ">> Epoch 78 finished \tANN training loss 0.002213\n",
      ">> Epoch 79 finished \tANN training loss 0.001911\n",
      ">> Epoch 80 finished \tANN training loss 0.001898\n",
      ">> Epoch 81 finished \tANN training loss 0.001758\n",
      ">> Epoch 82 finished \tANN training loss 0.001751\n",
      ">> Epoch 83 finished \tANN training loss 0.001590\n",
      ">> Epoch 84 finished \tANN training loss 0.002314\n",
      ">> Epoch 85 finished \tANN training loss 0.001564\n",
      ">> Epoch 86 finished \tANN training loss 0.001448\n",
      ">> Epoch 87 finished \tANN training loss 0.001613\n",
      ">> Epoch 88 finished \tANN training loss 0.003464\n",
      ">> Epoch 89 finished \tANN training loss 0.001611\n",
      ">> Epoch 90 finished \tANN training loss 0.001433\n",
      ">> Epoch 91 finished \tANN training loss 0.001442\n",
      ">> Epoch 92 finished \tANN training loss 0.001296\n",
      ">> Epoch 93 finished \tANN training loss 0.001741\n",
      ">> Epoch 94 finished \tANN training loss 0.001336\n",
      ">> Epoch 95 finished \tANN training loss 0.001406\n",
      ">> Epoch 96 finished \tANN training loss 0.001200\n",
      ">> Epoch 97 finished \tANN training loss 0.001083\n",
      ">> Epoch 98 finished \tANN training loss 0.001213\n",
      ">> Epoch 99 finished \tANN training loss 0.001116\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.925000\n"
     ]
    }
   ],
   "source": [
    "learning_rate_acc[2] = deep_belief_net(learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.925\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(learning_rate_acc[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 4\n",
    "\n",
    "learning_rate=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 66.080261\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 91.634460\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 87.947868\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 60.693130\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 40.038055\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 66.025490\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 32.787392\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 55.634209\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 39.156128\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 40.610870\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 72.047874\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 73.710304\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 126.095909\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 131.791779\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 196.352539\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 100.173981\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 192.914551\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 182.272186\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 205.986252\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 171.381775\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.625497\n",
      ">> Epoch 1 finished \tANN training loss 0.443053\n",
      ">> Epoch 2 finished \tANN training loss 0.266654\n",
      ">> Epoch 3 finished \tANN training loss 0.312450\n",
      ">> Epoch 4 finished \tANN training loss 0.113105\n",
      ">> Epoch 5 finished \tANN training loss 0.087273\n",
      ">> Epoch 6 finished \tANN training loss 0.134224\n",
      ">> Epoch 7 finished \tANN training loss 0.062731\n",
      ">> Epoch 8 finished \tANN training loss 0.049890\n",
      ">> Epoch 9 finished \tANN training loss 0.083968\n",
      ">> Epoch 10 finished \tANN training loss 0.031370\n",
      ">> Epoch 11 finished \tANN training loss 0.027858\n",
      ">> Epoch 12 finished \tANN training loss 0.036975\n",
      ">> Epoch 13 finished \tANN training loss 0.018078\n",
      ">> Epoch 14 finished \tANN training loss 0.025839\n",
      ">> Epoch 15 finished \tANN training loss 0.042755\n",
      ">> Epoch 16 finished \tANN training loss 0.015244\n",
      ">> Epoch 17 finished \tANN training loss 0.010195\n",
      ">> Epoch 18 finished \tANN training loss 0.016056\n",
      ">> Epoch 19 finished \tANN training loss 0.010570\n",
      ">> Epoch 20 finished \tANN training loss 0.005912\n",
      ">> Epoch 21 finished \tANN training loss 0.037383\n",
      ">> Epoch 22 finished \tANN training loss 0.019192\n",
      ">> Epoch 23 finished \tANN training loss 0.005719\n",
      ">> Epoch 24 finished \tANN training loss 0.002678\n",
      ">> Epoch 25 finished \tANN training loss 0.003906\n",
      ">> Epoch 26 finished \tANN training loss 0.003647\n",
      ">> Epoch 27 finished \tANN training loss 0.002898\n",
      ">> Epoch 28 finished \tANN training loss 0.004228\n",
      ">> Epoch 29 finished \tANN training loss 0.002741\n",
      ">> Epoch 30 finished \tANN training loss 0.008772\n",
      ">> Epoch 31 finished \tANN training loss 0.008482\n",
      ">> Epoch 32 finished \tANN training loss 0.004358\n",
      ">> Epoch 33 finished \tANN training loss 0.041635\n",
      ">> Epoch 34 finished \tANN training loss 0.001244\n",
      ">> Epoch 35 finished \tANN training loss 0.001848\n",
      ">> Epoch 36 finished \tANN training loss 0.002650\n",
      ">> Epoch 37 finished \tANN training loss 0.004173\n",
      ">> Epoch 38 finished \tANN training loss 0.011905\n",
      ">> Epoch 39 finished \tANN training loss 0.001364\n",
      ">> Epoch 40 finished \tANN training loss 0.007689\n",
      ">> Epoch 41 finished \tANN training loss 0.001544\n",
      ">> Epoch 42 finished \tANN training loss 0.000782\n",
      ">> Epoch 43 finished \tANN training loss 0.002078\n",
      ">> Epoch 44 finished \tANN training loss 0.000957\n",
      ">> Epoch 45 finished \tANN training loss 0.000402\n",
      ">> Epoch 46 finished \tANN training loss 0.000553\n",
      ">> Epoch 47 finished \tANN training loss 0.004701\n",
      ">> Epoch 48 finished \tANN training loss 0.002965\n",
      ">> Epoch 49 finished \tANN training loss 0.001008\n",
      ">> Epoch 50 finished \tANN training loss 0.000825\n",
      ">> Epoch 51 finished \tANN training loss 0.000679\n",
      ">> Epoch 52 finished \tANN training loss 0.004910\n",
      ">> Epoch 53 finished \tANN training loss 0.002777\n",
      ">> Epoch 54 finished \tANN training loss 0.002326\n",
      ">> Epoch 55 finished \tANN training loss 0.001592\n",
      ">> Epoch 56 finished \tANN training loss 0.001784\n",
      ">> Epoch 57 finished \tANN training loss 0.100423\n",
      ">> Epoch 58 finished \tANN training loss 0.000539\n",
      ">> Epoch 59 finished \tANN training loss 0.000370\n",
      ">> Epoch 60 finished \tANN training loss 0.005256\n",
      ">> Epoch 61 finished \tANN training loss 0.001440\n",
      ">> Epoch 62 finished \tANN training loss 0.000945\n",
      ">> Epoch 63 finished \tANN training loss 0.000955\n",
      ">> Epoch 64 finished \tANN training loss 0.000496\n",
      ">> Epoch 65 finished \tANN training loss 0.000135\n",
      ">> Epoch 66 finished \tANN training loss 0.000102\n",
      ">> Epoch 67 finished \tANN training loss 0.000683\n",
      ">> Epoch 68 finished \tANN training loss 0.003279\n",
      ">> Epoch 69 finished \tANN training loss 0.000170\n",
      ">> Epoch 70 finished \tANN training loss 0.000104\n",
      ">> Epoch 71 finished \tANN training loss 0.000065\n",
      ">> Epoch 72 finished \tANN training loss 0.000058\n",
      ">> Epoch 73 finished \tANN training loss 0.001415\n",
      ">> Epoch 74 finished \tANN training loss 0.000088\n",
      ">> Epoch 75 finished \tANN training loss 0.000364\n",
      ">> Epoch 76 finished \tANN training loss 0.000103\n",
      ">> Epoch 77 finished \tANN training loss 0.000454\n",
      ">> Epoch 78 finished \tANN training loss 0.000215\n",
      ">> Epoch 79 finished \tANN training loss 0.000653\n",
      ">> Epoch 80 finished \tANN training loss 0.000137\n",
      ">> Epoch 81 finished \tANN training loss 0.018381\n",
      ">> Epoch 82 finished \tANN training loss 0.000126\n",
      ">> Epoch 83 finished \tANN training loss 0.000193\n",
      ">> Epoch 84 finished \tANN training loss 0.007115\n",
      ">> Epoch 85 finished \tANN training loss 0.000196\n",
      ">> Epoch 86 finished \tANN training loss 0.001507\n",
      ">> Epoch 87 finished \tANN training loss 0.000209\n",
      ">> Epoch 88 finished \tANN training loss 0.000152\n",
      ">> Epoch 89 finished \tANN training loss 0.001545\n",
      ">> Epoch 90 finished \tANN training loss 0.002522\n",
      ">> Epoch 91 finished \tANN training loss 0.001260\n",
      ">> Epoch 92 finished \tANN training loss 0.000640\n",
      ">> Epoch 93 finished \tANN training loss 0.000399\n",
      ">> Epoch 94 finished \tANN training loss 0.000192\n",
      ">> Epoch 95 finished \tANN training loss 0.000459\n",
      ">> Epoch 96 finished \tANN training loss 0.001434\n",
      ">> Epoch 97 finished \tANN training loss 0.000236\n",
      ">> Epoch 98 finished \tANN training loss 0.000135\n",
      ">> Epoch 99 finished \tANN training loss 0.002107\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.915000\n"
     ]
    }
   ],
   "source": [
    "learning_rate_acc[3] = deep_belief_net(learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.915\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(learning_rate_acc[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 5\n",
    "\n",
    "learning_rate=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 72.270477\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 81.801888\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 46.275944\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 50.945541\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 45.906464\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 52.310688\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 28.218534\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 38.909161\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 37.254112\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 32.678345\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 83.250877\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 104.853477\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 136.333603\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 100.361305\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 133.560608\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 118.718063\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 140.663452\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 126.779884\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 129.488190\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 82.820702\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.784486\n",
      ">> Epoch 1 finished \tANN training loss 1.236545\n",
      ">> Epoch 2 finished \tANN training loss 1.051392\n",
      ">> Epoch 3 finished \tANN training loss 1.002878\n",
      ">> Epoch 4 finished \tANN training loss 0.682209\n",
      ">> Epoch 5 finished \tANN training loss 0.785251\n",
      ">> Epoch 6 finished \tANN training loss 0.514953\n",
      ">> Epoch 7 finished \tANN training loss 0.589532\n",
      ">> Epoch 8 finished \tANN training loss 0.414532\n",
      ">> Epoch 9 finished \tANN training loss 0.413148\n",
      ">> Epoch 10 finished \tANN training loss 0.467909\n",
      ">> Epoch 11 finished \tANN training loss 0.428593\n",
      ">> Epoch 12 finished \tANN training loss 0.469807\n",
      ">> Epoch 13 finished \tANN training loss 0.910338\n",
      ">> Epoch 14 finished \tANN training loss 0.312067\n",
      ">> Epoch 15 finished \tANN training loss 0.220670\n",
      ">> Epoch 16 finished \tANN training loss 0.734210\n",
      ">> Epoch 17 finished \tANN training loss 0.181733\n",
      ">> Epoch 18 finished \tANN training loss 0.142107\n",
      ">> Epoch 19 finished \tANN training loss 0.193240\n",
      ">> Epoch 20 finished \tANN training loss 0.209811\n",
      ">> Epoch 21 finished \tANN training loss 0.108896\n",
      ">> Epoch 22 finished \tANN training loss 0.084323\n",
      ">> Epoch 23 finished \tANN training loss 0.106669\n",
      ">> Epoch 24 finished \tANN training loss 0.110347\n",
      ">> Epoch 25 finished \tANN training loss 0.170654\n",
      ">> Epoch 26 finished \tANN training loss 0.091176\n",
      ">> Epoch 27 finished \tANN training loss 0.159304\n",
      ">> Epoch 28 finished \tANN training loss 0.117409\n",
      ">> Epoch 29 finished \tANN training loss 0.049898\n",
      ">> Epoch 30 finished \tANN training loss 0.044145\n",
      ">> Epoch 31 finished \tANN training loss 0.067317\n",
      ">> Epoch 32 finished \tANN training loss 0.075125\n",
      ">> Epoch 33 finished \tANN training loss 0.044914\n",
      ">> Epoch 34 finished \tANN training loss 0.025610\n",
      ">> Epoch 35 finished \tANN training loss 0.066149\n",
      ">> Epoch 36 finished \tANN training loss 0.052959\n",
      ">> Epoch 37 finished \tANN training loss 0.042279\n",
      ">> Epoch 38 finished \tANN training loss 0.098137\n",
      ">> Epoch 39 finished \tANN training loss 0.051835\n",
      ">> Epoch 40 finished \tANN training loss 0.055665\n",
      ">> Epoch 41 finished \tANN training loss 0.022152\n",
      ">> Epoch 42 finished \tANN training loss 0.041295\n",
      ">> Epoch 43 finished \tANN training loss 0.062445\n",
      ">> Epoch 44 finished \tANN training loss 0.047148\n",
      ">> Epoch 45 finished \tANN training loss 0.038272\n",
      ">> Epoch 46 finished \tANN training loss 0.020030\n",
      ">> Epoch 47 finished \tANN training loss 0.029258\n",
      ">> Epoch 48 finished \tANN training loss 0.026438\n",
      ">> Epoch 49 finished \tANN training loss 0.095146\n",
      ">> Epoch 50 finished \tANN training loss 0.021292\n",
      ">> Epoch 51 finished \tANN training loss 0.036980\n",
      ">> Epoch 52 finished \tANN training loss 0.069420\n",
      ">> Epoch 53 finished \tANN training loss 0.029707\n",
      ">> Epoch 54 finished \tANN training loss 0.011608\n",
      ">> Epoch 55 finished \tANN training loss 0.012328\n",
      ">> Epoch 56 finished \tANN training loss 0.008602\n",
      ">> Epoch 57 finished \tANN training loss 0.010130\n",
      ">> Epoch 58 finished \tANN training loss 0.004296\n",
      ">> Epoch 59 finished \tANN training loss 0.025018\n",
      ">> Epoch 60 finished \tANN training loss 0.033085\n",
      ">> Epoch 61 finished \tANN training loss 0.003564\n",
      ">> Epoch 62 finished \tANN training loss 0.029988\n",
      ">> Epoch 63 finished \tANN training loss 0.006880\n",
      ">> Epoch 64 finished \tANN training loss 0.093447\n",
      ">> Epoch 65 finished \tANN training loss 0.032250\n",
      ">> Epoch 66 finished \tANN training loss 0.032537\n",
      ">> Epoch 67 finished \tANN training loss 0.030323\n",
      ">> Epoch 68 finished \tANN training loss 0.042899\n",
      ">> Epoch 69 finished \tANN training loss 0.024002\n",
      ">> Epoch 70 finished \tANN training loss 0.012982\n",
      ">> Epoch 71 finished \tANN training loss 0.021231\n",
      ">> Epoch 72 finished \tANN training loss 0.028176\n",
      ">> Epoch 73 finished \tANN training loss 0.032740\n",
      ">> Epoch 74 finished \tANN training loss 0.018218\n",
      ">> Epoch 75 finished \tANN training loss 0.016554\n",
      ">> Epoch 76 finished \tANN training loss 0.014098\n",
      ">> Epoch 77 finished \tANN training loss 0.014523\n",
      ">> Epoch 78 finished \tANN training loss 0.023437\n",
      ">> Epoch 79 finished \tANN training loss 0.008770\n",
      ">> Epoch 80 finished \tANN training loss 0.014129\n",
      ">> Epoch 81 finished \tANN training loss 0.004421\n",
      ">> Epoch 82 finished \tANN training loss 0.056209\n",
      ">> Epoch 83 finished \tANN training loss 0.020073\n",
      ">> Epoch 84 finished \tANN training loss 0.029482\n",
      ">> Epoch 85 finished \tANN training loss 0.012197\n",
      ">> Epoch 86 finished \tANN training loss 0.012146\n",
      ">> Epoch 87 finished \tANN training loss 0.007191\n",
      ">> Epoch 88 finished \tANN training loss 0.013365\n",
      ">> Epoch 89 finished \tANN training loss 0.012833\n",
      ">> Epoch 90 finished \tANN training loss 0.027089\n",
      ">> Epoch 91 finished \tANN training loss 0.022890\n",
      ">> Epoch 92 finished \tANN training loss 0.008142\n",
      ">> Epoch 93 finished \tANN training loss 0.009520\n",
      ">> Epoch 94 finished \tANN training loss 0.010478\n",
      ">> Epoch 95 finished \tANN training loss 0.014582\n",
      ">> Epoch 96 finished \tANN training loss 0.009667\n",
      ">> Epoch 97 finished \tANN training loss 0.010653\n",
      ">> Epoch 98 finished \tANN training loss 0.009629\n",
      ">> Epoch 99 finished \tANN training loss 0.011932\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.930000\n"
     ]
    }
   ],
   "source": [
    "learning_rate_acc[4] = deep_belief_net(learning_rate=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.93\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(learning_rate_acc[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89, 0.92, 0.925, 0.915, 0.93]\n",
      "Most accurate learning_rate setting is Setting 5\n"
     ]
    }
   ],
   "source": [
    "# Collated results\n",
    "print(learning_rate_acc)\n",
    "print('Most accurate learning_rate setting is Setting ' + str(learning_rate_acc.index(max(learning_rate_acc)) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create bar graph\n",
    "plt.clf()\n",
    "objects = ('0.01', '0.05', '0.10', '0.50', '1.0')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [0.885,0.895,0.915,0.895, 0.875]\n",
    "\n",
    "plt.bar(y_pos, performance, align='center', alpha=1.)\n",
    "plt.xticks(y_pos, objects, rotation=30)\n",
    "plt.xlabel('Settings')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('learning_rate Settings')\n",
    " \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Pre-training Lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store accuracies\n",
    "epoch_rbm_acc = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 1\n",
    "\n",
    "n_epochs_rbm=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 54.133324\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 186.200073\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 2.176536\n",
      ">> Epoch 1 finished \tANN training loss 1.886552\n",
      ">> Epoch 2 finished \tANN training loss 1.566326\n",
      ">> Epoch 3 finished \tANN training loss 1.279872\n",
      ">> Epoch 4 finished \tANN training loss 1.101892\n",
      ">> Epoch 5 finished \tANN training loss 0.955066\n",
      ">> Epoch 6 finished \tANN training loss 0.858420\n",
      ">> Epoch 7 finished \tANN training loss 0.743757\n",
      ">> Epoch 8 finished \tANN training loss 0.679228\n",
      ">> Epoch 9 finished \tANN training loss 0.560115\n",
      ">> Epoch 10 finished \tANN training loss 0.475118\n",
      ">> Epoch 11 finished \tANN training loss 0.445670\n",
      ">> Epoch 12 finished \tANN training loss 0.390593\n",
      ">> Epoch 13 finished \tANN training loss 0.372699\n",
      ">> Epoch 14 finished \tANN training loss 0.330413\n",
      ">> Epoch 15 finished \tANN training loss 0.298382\n",
      ">> Epoch 16 finished \tANN training loss 0.279288\n",
      ">> Epoch 17 finished \tANN training loss 0.267730\n",
      ">> Epoch 18 finished \tANN training loss 0.257929\n",
      ">> Epoch 19 finished \tANN training loss 0.207795\n",
      ">> Epoch 20 finished \tANN training loss 0.212080\n",
      ">> Epoch 21 finished \tANN training loss 0.188913\n",
      ">> Epoch 22 finished \tANN training loss 0.151036\n",
      ">> Epoch 23 finished \tANN training loss 0.162748\n",
      ">> Epoch 24 finished \tANN training loss 0.148730\n",
      ">> Epoch 25 finished \tANN training loss 0.139550\n",
      ">> Epoch 26 finished \tANN training loss 0.118637\n",
      ">> Epoch 27 finished \tANN training loss 0.107832\n",
      ">> Epoch 28 finished \tANN training loss 0.100572\n",
      ">> Epoch 29 finished \tANN training loss 0.090064\n",
      ">> Epoch 30 finished \tANN training loss 0.093168\n",
      ">> Epoch 31 finished \tANN training loss 0.122952\n",
      ">> Epoch 32 finished \tANN training loss 0.068589\n",
      ">> Epoch 33 finished \tANN training loss 0.073530\n",
      ">> Epoch 34 finished \tANN training loss 0.064477\n",
      ">> Epoch 35 finished \tANN training loss 0.052682\n",
      ">> Epoch 36 finished \tANN training loss 0.053517\n",
      ">> Epoch 37 finished \tANN training loss 0.043368\n",
      ">> Epoch 38 finished \tANN training loss 0.046191\n",
      ">> Epoch 39 finished \tANN training loss 0.040723\n",
      ">> Epoch 40 finished \tANN training loss 0.038071\n",
      ">> Epoch 41 finished \tANN training loss 0.035844\n",
      ">> Epoch 42 finished \tANN training loss 0.032580\n",
      ">> Epoch 43 finished \tANN training loss 0.031440\n",
      ">> Epoch 44 finished \tANN training loss 0.030940\n",
      ">> Epoch 45 finished \tANN training loss 0.025004\n",
      ">> Epoch 46 finished \tANN training loss 0.025485\n",
      ">> Epoch 47 finished \tANN training loss 0.034077\n",
      ">> Epoch 48 finished \tANN training loss 0.018855\n",
      ">> Epoch 49 finished \tANN training loss 0.026004\n",
      ">> Epoch 50 finished \tANN training loss 0.018260\n",
      ">> Epoch 51 finished \tANN training loss 0.015772\n",
      ">> Epoch 52 finished \tANN training loss 0.014571\n",
      ">> Epoch 53 finished \tANN training loss 0.017756\n",
      ">> Epoch 54 finished \tANN training loss 0.014397\n",
      ">> Epoch 55 finished \tANN training loss 0.014987\n",
      ">> Epoch 56 finished \tANN training loss 0.017929\n",
      ">> Epoch 57 finished \tANN training loss 0.013392\n",
      ">> Epoch 58 finished \tANN training loss 0.013023\n",
      ">> Epoch 59 finished \tANN training loss 0.013521\n",
      ">> Epoch 60 finished \tANN training loss 0.012485\n",
      ">> Epoch 61 finished \tANN training loss 0.008115\n",
      ">> Epoch 62 finished \tANN training loss 0.008428\n",
      ">> Epoch 63 finished \tANN training loss 0.009117\n",
      ">> Epoch 64 finished \tANN training loss 0.009082\n",
      ">> Epoch 65 finished \tANN training loss 0.007596\n",
      ">> Epoch 66 finished \tANN training loss 0.008884\n",
      ">> Epoch 67 finished \tANN training loss 0.007591\n",
      ">> Epoch 68 finished \tANN training loss 0.006304\n",
      ">> Epoch 69 finished \tANN training loss 0.005949\n",
      ">> Epoch 70 finished \tANN training loss 0.006765\n",
      ">> Epoch 71 finished \tANN training loss 0.005489\n",
      ">> Epoch 72 finished \tANN training loss 0.006013\n",
      ">> Epoch 73 finished \tANN training loss 0.005303\n",
      ">> Epoch 74 finished \tANN training loss 0.004564\n",
      ">> Epoch 75 finished \tANN training loss 0.005381\n",
      ">> Epoch 76 finished \tANN training loss 0.004588\n",
      ">> Epoch 77 finished \tANN training loss 0.005255\n",
      ">> Epoch 78 finished \tANN training loss 0.004246\n",
      ">> Epoch 79 finished \tANN training loss 0.003941\n",
      ">> Epoch 80 finished \tANN training loss 0.003553\n",
      ">> Epoch 81 finished \tANN training loss 0.003588\n",
      ">> Epoch 82 finished \tANN training loss 0.003404\n",
      ">> Epoch 83 finished \tANN training loss 0.002582\n",
      ">> Epoch 84 finished \tANN training loss 0.002499\n",
      ">> Epoch 85 finished \tANN training loss 0.005473\n",
      ">> Epoch 86 finished \tANN training loss 0.003155\n",
      ">> Epoch 87 finished \tANN training loss 0.003976\n",
      ">> Epoch 88 finished \tANN training loss 0.003612\n",
      ">> Epoch 89 finished \tANN training loss 0.002397\n",
      ">> Epoch 90 finished \tANN training loss 0.002645\n",
      ">> Epoch 91 finished \tANN training loss 0.002445\n",
      ">> Epoch 92 finished \tANN training loss 0.002145\n",
      ">> Epoch 93 finished \tANN training loss 0.002753\n",
      ">> Epoch 94 finished \tANN training loss 0.002109\n",
      ">> Epoch 95 finished \tANN training loss 0.001884\n",
      ">> Epoch 96 finished \tANN training loss 0.001634\n",
      ">> Epoch 97 finished \tANN training loss 0.002355\n",
      ">> Epoch 98 finished \tANN training loss 0.001735\n",
      ">> Epoch 99 finished \tANN training loss 0.002307\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.880000\n"
     ]
    }
   ],
   "source": [
    "epoch_rbm_acc[0] = deep_belief_net(n_epochs_rbm=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.88\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(epoch_rbm_acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 2\n",
    "\n",
    "n_epochs_rbm=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 75.499611\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 51.906788\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 311.572845\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 163.014496\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.406206\n",
      ">> Epoch 1 finished \tANN training loss 1.033983\n",
      ">> Epoch 2 finished \tANN training loss 0.859397\n",
      ">> Epoch 3 finished \tANN training loss 0.742048\n",
      ">> Epoch 4 finished \tANN training loss 0.672420\n",
      ">> Epoch 5 finished \tANN training loss 0.604227\n",
      ">> Epoch 6 finished \tANN training loss 0.509879\n",
      ">> Epoch 7 finished \tANN training loss 0.451311\n",
      ">> Epoch 8 finished \tANN training loss 0.393184\n",
      ">> Epoch 9 finished \tANN training loss 0.342961\n",
      ">> Epoch 10 finished \tANN training loss 0.300598\n",
      ">> Epoch 11 finished \tANN training loss 0.278065\n",
      ">> Epoch 12 finished \tANN training loss 0.252543\n",
      ">> Epoch 13 finished \tANN training loss 0.237767\n",
      ">> Epoch 14 finished \tANN training loss 0.215873\n",
      ">> Epoch 15 finished \tANN training loss 0.191894\n",
      ">> Epoch 16 finished \tANN training loss 0.183744\n",
      ">> Epoch 17 finished \tANN training loss 0.163935\n",
      ">> Epoch 18 finished \tANN training loss 0.145984\n",
      ">> Epoch 19 finished \tANN training loss 0.127589\n",
      ">> Epoch 20 finished \tANN training loss 0.138214\n",
      ">> Epoch 21 finished \tANN training loss 0.113581\n",
      ">> Epoch 22 finished \tANN training loss 0.099124\n",
      ">> Epoch 23 finished \tANN training loss 0.087590\n",
      ">> Epoch 24 finished \tANN training loss 0.089463\n",
      ">> Epoch 25 finished \tANN training loss 0.081815\n",
      ">> Epoch 26 finished \tANN training loss 0.079989\n",
      ">> Epoch 27 finished \tANN training loss 0.070151\n",
      ">> Epoch 28 finished \tANN training loss 0.056714\n",
      ">> Epoch 29 finished \tANN training loss 0.055306\n",
      ">> Epoch 30 finished \tANN training loss 0.051711\n",
      ">> Epoch 31 finished \tANN training loss 0.046722\n",
      ">> Epoch 32 finished \tANN training loss 0.046583\n",
      ">> Epoch 33 finished \tANN training loss 0.042314\n",
      ">> Epoch 34 finished \tANN training loss 0.052939\n",
      ">> Epoch 35 finished \tANN training loss 0.030549\n",
      ">> Epoch 36 finished \tANN training loss 0.027775\n",
      ">> Epoch 37 finished \tANN training loss 0.024974\n",
      ">> Epoch 38 finished \tANN training loss 0.025100\n",
      ">> Epoch 39 finished \tANN training loss 0.022156\n",
      ">> Epoch 40 finished \tANN training loss 0.017354\n",
      ">> Epoch 41 finished \tANN training loss 0.020108\n",
      ">> Epoch 42 finished \tANN training loss 0.017079\n",
      ">> Epoch 43 finished \tANN training loss 0.016311\n",
      ">> Epoch 44 finished \tANN training loss 0.015735\n",
      ">> Epoch 45 finished \tANN training loss 0.017427\n",
      ">> Epoch 46 finished \tANN training loss 0.014546\n",
      ">> Epoch 47 finished \tANN training loss 0.015083\n",
      ">> Epoch 48 finished \tANN training loss 0.010674\n",
      ">> Epoch 49 finished \tANN training loss 0.010786\n",
      ">> Epoch 50 finished \tANN training loss 0.010343\n",
      ">> Epoch 51 finished \tANN training loss 0.009560\n",
      ">> Epoch 52 finished \tANN training loss 0.008939\n",
      ">> Epoch 53 finished \tANN training loss 0.013078\n",
      ">> Epoch 54 finished \tANN training loss 0.009162\n",
      ">> Epoch 55 finished \tANN training loss 0.009893\n",
      ">> Epoch 56 finished \tANN training loss 0.007682\n",
      ">> Epoch 57 finished \tANN training loss 0.006834\n",
      ">> Epoch 58 finished \tANN training loss 0.006762\n",
      ">> Epoch 59 finished \tANN training loss 0.007925\n",
      ">> Epoch 60 finished \tANN training loss 0.008234\n",
      ">> Epoch 61 finished \tANN training loss 0.007150\n",
      ">> Epoch 62 finished \tANN training loss 0.007815\n",
      ">> Epoch 63 finished \tANN training loss 0.005335\n",
      ">> Epoch 64 finished \tANN training loss 0.006160\n",
      ">> Epoch 65 finished \tANN training loss 0.004428\n",
      ">> Epoch 66 finished \tANN training loss 0.004696\n",
      ">> Epoch 67 finished \tANN training loss 0.006126\n",
      ">> Epoch 68 finished \tANN training loss 0.006380\n",
      ">> Epoch 69 finished \tANN training loss 0.005669\n",
      ">> Epoch 70 finished \tANN training loss 0.004266\n",
      ">> Epoch 71 finished \tANN training loss 0.005353\n",
      ">> Epoch 72 finished \tANN training loss 0.004110\n",
      ">> Epoch 73 finished \tANN training loss 0.004347\n",
      ">> Epoch 74 finished \tANN training loss 0.003618\n",
      ">> Epoch 75 finished \tANN training loss 0.004209\n",
      ">> Epoch 76 finished \tANN training loss 0.003468\n",
      ">> Epoch 77 finished \tANN training loss 0.004476\n",
      ">> Epoch 78 finished \tANN training loss 0.003011\n",
      ">> Epoch 79 finished \tANN training loss 0.002798\n",
      ">> Epoch 80 finished \tANN training loss 0.002408\n",
      ">> Epoch 81 finished \tANN training loss 0.002430\n",
      ">> Epoch 82 finished \tANN training loss 0.002424\n",
      ">> Epoch 83 finished \tANN training loss 0.003720\n",
      ">> Epoch 84 finished \tANN training loss 0.002028\n",
      ">> Epoch 85 finished \tANN training loss 0.001976\n",
      ">> Epoch 86 finished \tANN training loss 0.002047\n",
      ">> Epoch 87 finished \tANN training loss 0.001772\n",
      ">> Epoch 88 finished \tANN training loss 0.002159\n",
      ">> Epoch 89 finished \tANN training loss 0.001816\n",
      ">> Epoch 90 finished \tANN training loss 0.002121\n",
      ">> Epoch 91 finished \tANN training loss 0.001646\n",
      ">> Epoch 92 finished \tANN training loss 0.001465\n",
      ">> Epoch 93 finished \tANN training loss 0.001892\n",
      ">> Epoch 94 finished \tANN training loss 0.001550\n",
      ">> Epoch 95 finished \tANN training loss 0.001412\n",
      ">> Epoch 96 finished \tANN training loss 0.001299\n",
      ">> Epoch 97 finished \tANN training loss 0.001636\n",
      ">> Epoch 98 finished \tANN training loss 0.001239\n",
      ">> Epoch 99 finished \tANN training loss 0.001126\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.915000\n"
     ]
    }
   ],
   "source": [
    "epoch_rbm_acc[1] = deep_belief_net(n_epochs_rbm=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.915\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(epoch_rbm_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 3\n",
    "\n",
    "n_epochs_rbm=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 60.762997\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 45.667091\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 52.430508\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 50.332523\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 50.899242\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 308.078918\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 218.658325\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 354.645477\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 232.560944\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 259.198486\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.847052\n",
      ">> Epoch 1 finished \tANN training loss 0.596296\n",
      ">> Epoch 2 finished \tANN training loss 0.506592\n",
      ">> Epoch 3 finished \tANN training loss 0.430068\n",
      ">> Epoch 4 finished \tANN training loss 0.378345\n",
      ">> Epoch 5 finished \tANN training loss 0.326063\n",
      ">> Epoch 6 finished \tANN training loss 0.295778\n",
      ">> Epoch 7 finished \tANN training loss 0.257471\n",
      ">> Epoch 8 finished \tANN training loss 0.238699\n",
      ">> Epoch 9 finished \tANN training loss 0.200042\n",
      ">> Epoch 10 finished \tANN training loss 0.187703\n",
      ">> Epoch 11 finished \tANN training loss 0.169778\n",
      ">> Epoch 12 finished \tANN training loss 0.154833\n",
      ">> Epoch 13 finished \tANN training loss 0.145112\n",
      ">> Epoch 14 finished \tANN training loss 0.126320\n",
      ">> Epoch 15 finished \tANN training loss 0.123864\n",
      ">> Epoch 16 finished \tANN training loss 0.112698\n",
      ">> Epoch 17 finished \tANN training loss 0.103344\n",
      ">> Epoch 18 finished \tANN training loss 0.093607\n",
      ">> Epoch 19 finished \tANN training loss 0.085473\n",
      ">> Epoch 20 finished \tANN training loss 0.096053\n",
      ">> Epoch 21 finished \tANN training loss 0.079637\n",
      ">> Epoch 22 finished \tANN training loss 0.064363\n",
      ">> Epoch 23 finished \tANN training loss 0.058853\n",
      ">> Epoch 24 finished \tANN training loss 0.055402\n",
      ">> Epoch 25 finished \tANN training loss 0.048572\n",
      ">> Epoch 26 finished \tANN training loss 0.045404\n",
      ">> Epoch 27 finished \tANN training loss 0.045625\n",
      ">> Epoch 28 finished \tANN training loss 0.042693\n",
      ">> Epoch 29 finished \tANN training loss 0.039445\n",
      ">> Epoch 30 finished \tANN training loss 0.032827\n",
      ">> Epoch 31 finished \tANN training loss 0.030562\n",
      ">> Epoch 32 finished \tANN training loss 0.025914\n",
      ">> Epoch 33 finished \tANN training loss 0.028416\n",
      ">> Epoch 34 finished \tANN training loss 0.024119\n",
      ">> Epoch 35 finished \tANN training loss 0.022955\n",
      ">> Epoch 36 finished \tANN training loss 0.021726\n",
      ">> Epoch 37 finished \tANN training loss 0.019045\n",
      ">> Epoch 38 finished \tANN training loss 0.019607\n",
      ">> Epoch 39 finished \tANN training loss 0.016411\n",
      ">> Epoch 40 finished \tANN training loss 0.016079\n",
      ">> Epoch 41 finished \tANN training loss 0.015336\n",
      ">> Epoch 42 finished \tANN training loss 0.014267\n",
      ">> Epoch 43 finished \tANN training loss 0.013012\n",
      ">> Epoch 44 finished \tANN training loss 0.011176\n",
      ">> Epoch 45 finished \tANN training loss 0.010063\n",
      ">> Epoch 46 finished \tANN training loss 0.011684\n",
      ">> Epoch 47 finished \tANN training loss 0.010094\n",
      ">> Epoch 48 finished \tANN training loss 0.009332\n",
      ">> Epoch 49 finished \tANN training loss 0.007660\n",
      ">> Epoch 50 finished \tANN training loss 0.007969\n",
      ">> Epoch 51 finished \tANN training loss 0.007431\n",
      ">> Epoch 52 finished \tANN training loss 0.007255\n",
      ">> Epoch 53 finished \tANN training loss 0.007042\n",
      ">> Epoch 54 finished \tANN training loss 0.007086\n",
      ">> Epoch 55 finished \tANN training loss 0.007494\n",
      ">> Epoch 56 finished \tANN training loss 0.006802\n",
      ">> Epoch 57 finished \tANN training loss 0.006202\n",
      ">> Epoch 58 finished \tANN training loss 0.005640\n",
      ">> Epoch 59 finished \tANN training loss 0.005120\n",
      ">> Epoch 60 finished \tANN training loss 0.005213\n",
      ">> Epoch 61 finished \tANN training loss 0.004282\n",
      ">> Epoch 62 finished \tANN training loss 0.003938\n",
      ">> Epoch 63 finished \tANN training loss 0.004205\n",
      ">> Epoch 64 finished \tANN training loss 0.004427\n",
      ">> Epoch 65 finished \tANN training loss 0.003942\n",
      ">> Epoch 66 finished \tANN training loss 0.003986\n",
      ">> Epoch 67 finished \tANN training loss 0.003600\n",
      ">> Epoch 68 finished \tANN training loss 0.003570\n",
      ">> Epoch 69 finished \tANN training loss 0.003526\n",
      ">> Epoch 70 finished \tANN training loss 0.002974\n",
      ">> Epoch 71 finished \tANN training loss 0.003263\n",
      ">> Epoch 72 finished \tANN training loss 0.003239\n",
      ">> Epoch 73 finished \tANN training loss 0.003031\n",
      ">> Epoch 74 finished \tANN training loss 0.002755\n",
      ">> Epoch 75 finished \tANN training loss 0.002357\n",
      ">> Epoch 76 finished \tANN training loss 0.002584\n",
      ">> Epoch 77 finished \tANN training loss 0.002637\n",
      ">> Epoch 78 finished \tANN training loss 0.002883\n",
      ">> Epoch 79 finished \tANN training loss 0.002007\n",
      ">> Epoch 80 finished \tANN training loss 0.002045\n",
      ">> Epoch 81 finished \tANN training loss 0.002628\n",
      ">> Epoch 82 finished \tANN training loss 0.002253\n",
      ">> Epoch 83 finished \tANN training loss 0.001678\n",
      ">> Epoch 84 finished \tANN training loss 0.001688\n",
      ">> Epoch 85 finished \tANN training loss 0.001698\n",
      ">> Epoch 86 finished \tANN training loss 0.002060\n",
      ">> Epoch 87 finished \tANN training loss 0.001639\n",
      ">> Epoch 88 finished \tANN training loss 0.001852\n",
      ">> Epoch 89 finished \tANN training loss 0.001653\n",
      ">> Epoch 90 finished \tANN training loss 0.001628\n",
      ">> Epoch 91 finished \tANN training loss 0.002187\n",
      ">> Epoch 92 finished \tANN training loss 0.002082\n",
      ">> Epoch 93 finished \tANN training loss 0.001655\n",
      ">> Epoch 94 finished \tANN training loss 0.001499\n",
      ">> Epoch 95 finished \tANN training loss 0.001431\n",
      ">> Epoch 96 finished \tANN training loss 0.001411\n",
      ">> Epoch 97 finished \tANN training loss 0.001858\n",
      ">> Epoch 98 finished \tANN training loss 0.001385\n",
      ">> Epoch 99 finished \tANN training loss 0.001599\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.930000\n"
     ]
    }
   ],
   "source": [
    "epoch_rbm_acc[2] = deep_belief_net(n_epochs_rbm=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.93\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(epoch_rbm_acc[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 4 (Default)\n",
    "\n",
    "n_epochs_rbm=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.92\n"
     ]
    }
   ],
   "source": [
    "epoch_rbm_acc[3] = default_acc\n",
    "print('ACCURACY: ' + str(default_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 5\n",
    "\n",
    "n_epochs_rbm=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 54.319412\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 63.459541\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 39.351551\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 54.099335\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 64.219872\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 31.824020\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 71.651001\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 37.982578\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 31.841446\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 51.448280\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 42.309395\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 29.266645\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 24.917553\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 23.888802\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 41.501144\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 39.057430\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 46.352142\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 30.200499\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 33.579601\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 41.781570\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 78.465813\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 147.174637\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 148.045120\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 95.456169\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 95.083298\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 85.185837\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 128.573334\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 165.656723\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 127.142197\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 157.904526\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 166.077332\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 133.084335\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 163.618927\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 129.413208\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 122.651169\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 166.202515\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 119.266937\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 144.564194\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 183.519806\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 177.120789\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.639783\n",
      ">> Epoch 1 finished \tANN training loss 0.392679\n",
      ">> Epoch 2 finished \tANN training loss 0.336364\n",
      ">> Epoch 3 finished \tANN training loss 0.282583\n",
      ">> Epoch 4 finished \tANN training loss 0.235083\n",
      ">> Epoch 5 finished \tANN training loss 0.248435\n",
      ">> Epoch 6 finished \tANN training loss 0.197437\n",
      ">> Epoch 7 finished \tANN training loss 0.172333\n",
      ">> Epoch 8 finished \tANN training loss 0.143293\n",
      ">> Epoch 9 finished \tANN training loss 0.130299\n",
      ">> Epoch 10 finished \tANN training loss 0.117909\n",
      ">> Epoch 11 finished \tANN training loss 0.099051\n",
      ">> Epoch 12 finished \tANN training loss 0.099966\n",
      ">> Epoch 13 finished \tANN training loss 0.087086\n",
      ">> Epoch 14 finished \tANN training loss 0.077297\n",
      ">> Epoch 15 finished \tANN training loss 0.069222\n",
      ">> Epoch 16 finished \tANN training loss 0.060532\n",
      ">> Epoch 17 finished \tANN training loss 0.058772\n",
      ">> Epoch 18 finished \tANN training loss 0.057153\n",
      ">> Epoch 19 finished \tANN training loss 0.047141\n",
      ">> Epoch 20 finished \tANN training loss 0.041294\n",
      ">> Epoch 21 finished \tANN training loss 0.037280\n",
      ">> Epoch 22 finished \tANN training loss 0.032907\n",
      ">> Epoch 23 finished \tANN training loss 0.034287\n",
      ">> Epoch 24 finished \tANN training loss 0.030070\n",
      ">> Epoch 25 finished \tANN training loss 0.026440\n",
      ">> Epoch 26 finished \tANN training loss 0.025297\n",
      ">> Epoch 27 finished \tANN training loss 0.023434\n",
      ">> Epoch 28 finished \tANN training loss 0.022735\n",
      ">> Epoch 29 finished \tANN training loss 0.022677\n",
      ">> Epoch 30 finished \tANN training loss 0.022540\n",
      ">> Epoch 31 finished \tANN training loss 0.017956\n",
      ">> Epoch 32 finished \tANN training loss 0.015973\n",
      ">> Epoch 33 finished \tANN training loss 0.014733\n",
      ">> Epoch 34 finished \tANN training loss 0.013212\n",
      ">> Epoch 35 finished \tANN training loss 0.013613\n",
      ">> Epoch 36 finished \tANN training loss 0.013192\n",
      ">> Epoch 37 finished \tANN training loss 0.011684\n",
      ">> Epoch 38 finished \tANN training loss 0.010789\n",
      ">> Epoch 39 finished \tANN training loss 0.011014\n",
      ">> Epoch 40 finished \tANN training loss 0.010853\n",
      ">> Epoch 41 finished \tANN training loss 0.009158\n",
      ">> Epoch 42 finished \tANN training loss 0.009084\n",
      ">> Epoch 43 finished \tANN training loss 0.008462\n",
      ">> Epoch 44 finished \tANN training loss 0.008418\n",
      ">> Epoch 45 finished \tANN training loss 0.007343\n",
      ">> Epoch 46 finished \tANN training loss 0.008198\n",
      ">> Epoch 47 finished \tANN training loss 0.007078\n",
      ">> Epoch 48 finished \tANN training loss 0.007133\n",
      ">> Epoch 49 finished \tANN training loss 0.005850\n",
      ">> Epoch 50 finished \tANN training loss 0.005475\n",
      ">> Epoch 51 finished \tANN training loss 0.005193\n",
      ">> Epoch 52 finished \tANN training loss 0.004970\n",
      ">> Epoch 53 finished \tANN training loss 0.005013\n",
      ">> Epoch 54 finished \tANN training loss 0.004270\n",
      ">> Epoch 55 finished \tANN training loss 0.004066\n",
      ">> Epoch 56 finished \tANN training loss 0.003833\n",
      ">> Epoch 57 finished \tANN training loss 0.003882\n",
      ">> Epoch 58 finished \tANN training loss 0.003740\n",
      ">> Epoch 59 finished \tANN training loss 0.003658\n",
      ">> Epoch 60 finished \tANN training loss 0.003532\n",
      ">> Epoch 61 finished \tANN training loss 0.003219\n",
      ">> Epoch 62 finished \tANN training loss 0.003095\n",
      ">> Epoch 63 finished \tANN training loss 0.002721\n",
      ">> Epoch 64 finished \tANN training loss 0.002891\n",
      ">> Epoch 65 finished \tANN training loss 0.002764\n",
      ">> Epoch 66 finished \tANN training loss 0.002750\n",
      ">> Epoch 67 finished \tANN training loss 0.002492\n",
      ">> Epoch 68 finished \tANN training loss 0.002599\n",
      ">> Epoch 69 finished \tANN training loss 0.002281\n",
      ">> Epoch 70 finished \tANN training loss 0.002060\n",
      ">> Epoch 71 finished \tANN training loss 0.002117\n",
      ">> Epoch 72 finished \tANN training loss 0.003146\n",
      ">> Epoch 73 finished \tANN training loss 0.002006\n",
      ">> Epoch 74 finished \tANN training loss 0.001905\n",
      ">> Epoch 75 finished \tANN training loss 0.001953\n",
      ">> Epoch 76 finished \tANN training loss 0.001742\n",
      ">> Epoch 77 finished \tANN training loss 0.001677\n",
      ">> Epoch 78 finished \tANN training loss 0.001592\n",
      ">> Epoch 79 finished \tANN training loss 0.001942\n",
      ">> Epoch 80 finished \tANN training loss 0.001762\n",
      ">> Epoch 81 finished \tANN training loss 0.001838\n",
      ">> Epoch 82 finished \tANN training loss 0.001541\n",
      ">> Epoch 83 finished \tANN training loss 0.001714\n",
      ">> Epoch 84 finished \tANN training loss 0.001500\n",
      ">> Epoch 85 finished \tANN training loss 0.001609\n",
      ">> Epoch 86 finished \tANN training loss 0.001485\n",
      ">> Epoch 87 finished \tANN training loss 0.001705\n",
      ">> Epoch 88 finished \tANN training loss 0.001504\n",
      ">> Epoch 89 finished \tANN training loss 0.001417\n",
      ">> Epoch 90 finished \tANN training loss 0.001541\n",
      ">> Epoch 91 finished \tANN training loss 0.001368\n",
      ">> Epoch 92 finished \tANN training loss 0.001294\n",
      ">> Epoch 93 finished \tANN training loss 0.001257\n",
      ">> Epoch 94 finished \tANN training loss 0.001133\n",
      ">> Epoch 95 finished \tANN training loss 0.001057\n",
      ">> Epoch 96 finished \tANN training loss 0.001009\n",
      ">> Epoch 97 finished \tANN training loss 0.000925\n",
      ">> Epoch 98 finished \tANN training loss 0.000993\n",
      ">> Epoch 99 finished \tANN training loss 0.000974\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.925000\n"
     ]
    }
   ],
   "source": [
    "epoch_rbm_acc[4] = deep_belief_net(n_epochs_rbm=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.925\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(epoch_rbm_acc[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 6\n",
    "\n",
    "n_epochs_rbm=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 119.058693\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 65.557838\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 67.544075\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 39.892689\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 43.711231\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 57.967655\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 50.951454\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 55.691795\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 40.283005\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 45.715225\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 37.194252\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 31.848616\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 43.089619\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 59.848587\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 42.244930\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 53.655136\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 47.361298\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 45.188522\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 44.613564\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 48.804211\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 44.465992\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 50.147743\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 51.446888\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 44.338604\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 46.279552\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 57.635422\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 51.885620\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 59.122746\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 52.097000\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 59.785477\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 127.247795\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 160.988297\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 78.756531\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 214.832825\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 210.697708\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 145.715408\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 138.853806\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 169.987045\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 178.030624\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 189.227676\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 153.257660\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 199.059616\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 164.430237\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 147.864761\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 242.900269\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 242.457672\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 185.456253\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 223.098328\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 224.546860\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 167.609573\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 192.305405\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 150.278717\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 173.953766\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 118.062347\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 203.252182\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 158.866028\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 177.342300\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 214.501648\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 203.130386\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 191.975174\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.580009\n",
      ">> Epoch 1 finished \tANN training loss 0.418021\n",
      ">> Epoch 2 finished \tANN training loss 0.371747\n",
      ">> Epoch 3 finished \tANN training loss 0.306114\n",
      ">> Epoch 4 finished \tANN training loss 0.261978\n",
      ">> Epoch 5 finished \tANN training loss 0.222210\n",
      ">> Epoch 6 finished \tANN training loss 0.204540\n",
      ">> Epoch 7 finished \tANN training loss 0.178271\n",
      ">> Epoch 8 finished \tANN training loss 0.147977\n",
      ">> Epoch 9 finished \tANN training loss 0.160757\n",
      ">> Epoch 10 finished \tANN training loss 0.120485\n",
      ">> Epoch 11 finished \tANN training loss 0.105766\n",
      ">> Epoch 12 finished \tANN training loss 0.099921\n",
      ">> Epoch 13 finished \tANN training loss 0.092332\n",
      ">> Epoch 14 finished \tANN training loss 0.077099\n",
      ">> Epoch 15 finished \tANN training loss 0.070151\n",
      ">> Epoch 16 finished \tANN training loss 0.065056\n",
      ">> Epoch 17 finished \tANN training loss 0.067782\n",
      ">> Epoch 18 finished \tANN training loss 0.053442\n",
      ">> Epoch 19 finished \tANN training loss 0.046238\n",
      ">> Epoch 20 finished \tANN training loss 0.044684\n",
      ">> Epoch 21 finished \tANN training loss 0.038788\n",
      ">> Epoch 22 finished \tANN training loss 0.040962\n",
      ">> Epoch 23 finished \tANN training loss 0.034788\n",
      ">> Epoch 24 finished \tANN training loss 0.032209\n",
      ">> Epoch 25 finished \tANN training loss 0.034088\n",
      ">> Epoch 26 finished \tANN training loss 0.028738\n",
      ">> Epoch 27 finished \tANN training loss 0.023392\n",
      ">> Epoch 28 finished \tANN training loss 0.024621\n",
      ">> Epoch 29 finished \tANN training loss 0.019795\n",
      ">> Epoch 30 finished \tANN training loss 0.020886\n",
      ">> Epoch 31 finished \tANN training loss 0.018728\n",
      ">> Epoch 32 finished \tANN training loss 0.017961\n",
      ">> Epoch 33 finished \tANN training loss 0.016736\n",
      ">> Epoch 34 finished \tANN training loss 0.016476\n",
      ">> Epoch 35 finished \tANN training loss 0.014979\n",
      ">> Epoch 36 finished \tANN training loss 0.013809\n",
      ">> Epoch 37 finished \tANN training loss 0.014531\n",
      ">> Epoch 38 finished \tANN training loss 0.011500\n",
      ">> Epoch 39 finished \tANN training loss 0.010957\n",
      ">> Epoch 40 finished \tANN training loss 0.010617\n",
      ">> Epoch 41 finished \tANN training loss 0.010137\n",
      ">> Epoch 42 finished \tANN training loss 0.008667\n",
      ">> Epoch 43 finished \tANN training loss 0.010664\n",
      ">> Epoch 44 finished \tANN training loss 0.008521\n",
      ">> Epoch 45 finished \tANN training loss 0.009039\n",
      ">> Epoch 46 finished \tANN training loss 0.009006\n",
      ">> Epoch 47 finished \tANN training loss 0.007954\n",
      ">> Epoch 48 finished \tANN training loss 0.007146\n",
      ">> Epoch 49 finished \tANN training loss 0.006985\n",
      ">> Epoch 50 finished \tANN training loss 0.006996\n",
      ">> Epoch 51 finished \tANN training loss 0.005860\n",
      ">> Epoch 52 finished \tANN training loss 0.006715\n",
      ">> Epoch 53 finished \tANN training loss 0.007014\n",
      ">> Epoch 54 finished \tANN training loss 0.005921\n",
      ">> Epoch 55 finished \tANN training loss 0.005558\n",
      ">> Epoch 56 finished \tANN training loss 0.004830\n",
      ">> Epoch 57 finished \tANN training loss 0.004388\n",
      ">> Epoch 58 finished \tANN training loss 0.004356\n",
      ">> Epoch 59 finished \tANN training loss 0.003932\n",
      ">> Epoch 60 finished \tANN training loss 0.003928\n",
      ">> Epoch 61 finished \tANN training loss 0.003668\n",
      ">> Epoch 62 finished \tANN training loss 0.003612\n",
      ">> Epoch 63 finished \tANN training loss 0.004259\n",
      ">> Epoch 64 finished \tANN training loss 0.004055\n",
      ">> Epoch 65 finished \tANN training loss 0.003285\n",
      ">> Epoch 66 finished \tANN training loss 0.003120\n",
      ">> Epoch 67 finished \tANN training loss 0.003168\n",
      ">> Epoch 68 finished \tANN training loss 0.002879\n",
      ">> Epoch 69 finished \tANN training loss 0.003044\n",
      ">> Epoch 70 finished \tANN training loss 0.002532\n",
      ">> Epoch 71 finished \tANN training loss 0.002785\n",
      ">> Epoch 72 finished \tANN training loss 0.002547\n",
      ">> Epoch 73 finished \tANN training loss 0.002392\n",
      ">> Epoch 74 finished \tANN training loss 0.002516\n",
      ">> Epoch 75 finished \tANN training loss 0.002373\n",
      ">> Epoch 76 finished \tANN training loss 0.002245\n",
      ">> Epoch 77 finished \tANN training loss 0.002073\n",
      ">> Epoch 78 finished \tANN training loss 0.002004\n",
      ">> Epoch 79 finished \tANN training loss 0.001920\n",
      ">> Epoch 80 finished \tANN training loss 0.002021\n",
      ">> Epoch 81 finished \tANN training loss 0.002042\n",
      ">> Epoch 82 finished \tANN training loss 0.002018\n",
      ">> Epoch 83 finished \tANN training loss 0.001783\n",
      ">> Epoch 84 finished \tANN training loss 0.002063\n",
      ">> Epoch 85 finished \tANN training loss 0.001850\n",
      ">> Epoch 86 finished \tANN training loss 0.001736\n",
      ">> Epoch 87 finished \tANN training loss 0.001565\n",
      ">> Epoch 88 finished \tANN training loss 0.001532\n",
      ">> Epoch 89 finished \tANN training loss 0.001432\n",
      ">> Epoch 90 finished \tANN training loss 0.001241\n",
      ">> Epoch 91 finished \tANN training loss 0.001839\n",
      ">> Epoch 92 finished \tANN training loss 0.001353\n",
      ">> Epoch 93 finished \tANN training loss 0.001246\n",
      ">> Epoch 94 finished \tANN training loss 0.001366\n",
      ">> Epoch 95 finished \tANN training loss 0.001405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 96 finished \tANN training loss 0.001113\n",
      ">> Epoch 97 finished \tANN training loss 0.001057\n",
      ">> Epoch 98 finished \tANN training loss 0.001125\n",
      ">> Epoch 99 finished \tANN training loss 0.001351\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.925000\n"
     ]
    }
   ],
   "source": [
    "epoch_rbm_acc[5] = deep_belief_net(n_epochs_rbm=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.925\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(epoch_rbm_acc[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 7\n",
    "\n",
    "n_epochs_rbm=40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 98.491325\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 62.166748\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 43.109913\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 34.269939\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 51.258476\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 44.414074\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 44.369331\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 39.630882\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 38.195812\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 46.515759\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 58.347984\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 49.590366\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 32.750656\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 56.219135\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 38.937584\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 44.719715\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 40.375450\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 48.133820\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 46.230270\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 37.790718\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 37.964008\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 28.959040\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 45.854630\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 41.324127\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 46.382412\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 43.990635\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 38.740189\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 46.253212\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 46.192284\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 44.224640\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 49.471237\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 53.220654\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 62.776901\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 41.693722\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 51.083996\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 59.423817\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 47.981895\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 48.889694\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 56.875370\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 44.700844\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 110.313927\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 134.939697\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 132.079590\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 208.770004\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 167.162537\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 127.829567\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 161.938049\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 89.116798\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 94.803734\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 55.572735\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 103.354340\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 105.283310\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 112.827698\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 106.298225\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 112.952438\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 64.986298\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 154.728577\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 112.032379\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 68.847481\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 110.694313\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 139.197403\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 110.178009\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 92.777145\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 100.466171\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 109.964066\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 94.963379\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 82.277405\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 120.845139\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 86.145096\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 107.836601\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 83.286819\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 87.500702\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 102.135857\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 110.949600\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 106.777245\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 90.700058\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 126.906837\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 105.766190\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 120.708176\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 87.147110\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.585433\n",
      ">> Epoch 1 finished \tANN training loss 0.430126\n",
      ">> Epoch 2 finished \tANN training loss 0.365303\n",
      ">> Epoch 3 finished \tANN training loss 0.278007\n",
      ">> Epoch 4 finished \tANN training loss 0.227414\n",
      ">> Epoch 5 finished \tANN training loss 0.199426\n",
      ">> Epoch 6 finished \tANN training loss 0.184337\n",
      ">> Epoch 7 finished \tANN training loss 0.145943\n",
      ">> Epoch 8 finished \tANN training loss 0.135998\n",
      ">> Epoch 9 finished \tANN training loss 0.127751\n",
      ">> Epoch 10 finished \tANN training loss 0.105964\n",
      ">> Epoch 11 finished \tANN training loss 0.108622\n",
      ">> Epoch 12 finished \tANN training loss 0.093477\n",
      ">> Epoch 13 finished \tANN training loss 0.074975\n",
      ">> Epoch 14 finished \tANN training loss 0.075563\n",
      ">> Epoch 15 finished \tANN training loss 0.078443\n",
      ">> Epoch 16 finished \tANN training loss 0.059268\n",
      ">> Epoch 17 finished \tANN training loss 0.050735\n",
      ">> Epoch 18 finished \tANN training loss 0.049961\n",
      ">> Epoch 19 finished \tANN training loss 0.041482\n",
      ">> Epoch 20 finished \tANN training loss 0.040375\n",
      ">> Epoch 21 finished \tANN training loss 0.038014\n",
      ">> Epoch 22 finished \tANN training loss 0.039363\n",
      ">> Epoch 23 finished \tANN training loss 0.029863\n",
      ">> Epoch 24 finished \tANN training loss 0.031237\n",
      ">> Epoch 25 finished \tANN training loss 0.028480\n",
      ">> Epoch 26 finished \tANN training loss 0.023438\n",
      ">> Epoch 27 finished \tANN training loss 0.021009\n",
      ">> Epoch 28 finished \tANN training loss 0.019586\n",
      ">> Epoch 29 finished \tANN training loss 0.018735\n",
      ">> Epoch 30 finished \tANN training loss 0.025469\n",
      ">> Epoch 31 finished \tANN training loss 0.017652\n",
      ">> Epoch 32 finished \tANN training loss 0.017556\n",
      ">> Epoch 33 finished \tANN training loss 0.016264\n",
      ">> Epoch 34 finished \tANN training loss 0.013507\n",
      ">> Epoch 35 finished \tANN training loss 0.012228\n",
      ">> Epoch 36 finished \tANN training loss 0.011597\n",
      ">> Epoch 37 finished \tANN training loss 0.011180\n",
      ">> Epoch 38 finished \tANN training loss 0.010400\n",
      ">> Epoch 39 finished \tANN training loss 0.009539\n",
      ">> Epoch 40 finished \tANN training loss 0.010413\n",
      ">> Epoch 41 finished \tANN training loss 0.007948\n",
      ">> Epoch 42 finished \tANN training loss 0.007762\n",
      ">> Epoch 43 finished \tANN training loss 0.007450\n",
      ">> Epoch 44 finished \tANN training loss 0.007315\n",
      ">> Epoch 45 finished \tANN training loss 0.006207\n",
      ">> Epoch 46 finished \tANN training loss 0.006075\n",
      ">> Epoch 47 finished \tANN training loss 0.005612\n",
      ">> Epoch 48 finished \tANN training loss 0.006060\n",
      ">> Epoch 49 finished \tANN training loss 0.005869\n",
      ">> Epoch 50 finished \tANN training loss 0.005192\n",
      ">> Epoch 51 finished \tANN training loss 0.004819\n",
      ">> Epoch 52 finished \tANN training loss 0.004715\n",
      ">> Epoch 53 finished \tANN training loss 0.004737\n",
      ">> Epoch 54 finished \tANN training loss 0.004159\n",
      ">> Epoch 55 finished \tANN training loss 0.003859\n",
      ">> Epoch 56 finished \tANN training loss 0.004112\n",
      ">> Epoch 57 finished \tANN training loss 0.004127\n",
      ">> Epoch 58 finished \tANN training loss 0.004178\n",
      ">> Epoch 59 finished \tANN training loss 0.003380\n",
      ">> Epoch 60 finished \tANN training loss 0.003477\n",
      ">> Epoch 61 finished \tANN training loss 0.003638\n",
      ">> Epoch 62 finished \tANN training loss 0.003087\n",
      ">> Epoch 63 finished \tANN training loss 0.002828\n",
      ">> Epoch 64 finished \tANN training loss 0.002718\n",
      ">> Epoch 65 finished \tANN training loss 0.002884\n",
      ">> Epoch 66 finished \tANN training loss 0.003152\n",
      ">> Epoch 67 finished \tANN training loss 0.002765\n",
      ">> Epoch 68 finished \tANN training loss 0.002771\n",
      ">> Epoch 69 finished \tANN training loss 0.002501\n",
      ">> Epoch 70 finished \tANN training loss 0.002561\n",
      ">> Epoch 71 finished \tANN training loss 0.002290\n",
      ">> Epoch 72 finished \tANN training loss 0.002116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 73 finished \tANN training loss 0.001919\n",
      ">> Epoch 74 finished \tANN training loss 0.001898\n",
      ">> Epoch 75 finished \tANN training loss 0.001949\n",
      ">> Epoch 76 finished \tANN training loss 0.002098\n",
      ">> Epoch 77 finished \tANN training loss 0.002842\n",
      ">> Epoch 78 finished \tANN training loss 0.001775\n",
      ">> Epoch 79 finished \tANN training loss 0.001782\n",
      ">> Epoch 80 finished \tANN training loss 0.001818\n",
      ">> Epoch 81 finished \tANN training loss 0.001550\n",
      ">> Epoch 82 finished \tANN training loss 0.001628\n",
      ">> Epoch 83 finished \tANN training loss 0.001463\n",
      ">> Epoch 84 finished \tANN training loss 0.001509\n",
      ">> Epoch 85 finished \tANN training loss 0.001545\n",
      ">> Epoch 86 finished \tANN training loss 0.001594\n",
      ">> Epoch 87 finished \tANN training loss 0.001632\n",
      ">> Epoch 88 finished \tANN training loss 0.001967\n",
      ">> Epoch 89 finished \tANN training loss 0.001663\n",
      ">> Epoch 90 finished \tANN training loss 0.001477\n",
      ">> Epoch 91 finished \tANN training loss 0.001539\n",
      ">> Epoch 92 finished \tANN training loss 0.001367\n",
      ">> Epoch 93 finished \tANN training loss 0.001280\n",
      ">> Epoch 94 finished \tANN training loss 0.001401\n",
      ">> Epoch 95 finished \tANN training loss 0.001499\n",
      ">> Epoch 96 finished \tANN training loss 0.001467\n",
      ">> Epoch 97 finished \tANN training loss 0.001265\n",
      ">> Epoch 98 finished \tANN training loss 0.001613\n",
      ">> Epoch 99 finished \tANN training loss 0.001143\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.930000\n"
     ]
    }
   ],
   "source": [
    "epoch_rbm_acc[6] = deep_belief_net(n_epochs_rbm=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.93\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(epoch_rbm_acc[6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 8\n",
    "\n",
    "n_epochs_rbm=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 69.991081\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 53.935459\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 41.196545\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 54.571129\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 43.848213\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 59.311092\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 40.106819\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 47.300243\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 36.876217\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 27.118109\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 60.691174\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 53.345062\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 35.359818\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 35.779781\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 35.554428\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 40.402821\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 44.550732\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 51.913441\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 42.347816\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 45.685772\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 41.956974\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 55.436699\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 47.472412\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 50.265915\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 57.223549\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 43.798874\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 53.447498\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 48.546768\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 49.385399\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 48.369366\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 57.247372\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 45.389969\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 56.202621\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 58.838085\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 47.516792\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 47.032001\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 44.362881\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 42.631405\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 37.179169\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 55.456768\n",
      ">> Epoch 41 finished \tRBM Reconstruction error 54.769619\n",
      ">> Epoch 42 finished \tRBM Reconstruction error 55.015293\n",
      ">> Epoch 43 finished \tRBM Reconstruction error 49.508907\n",
      ">> Epoch 44 finished \tRBM Reconstruction error 51.706089\n",
      ">> Epoch 45 finished \tRBM Reconstruction error 44.344501\n",
      ">> Epoch 46 finished \tRBM Reconstruction error 49.774357\n",
      ">> Epoch 47 finished \tRBM Reconstruction error 49.533279\n",
      ">> Epoch 48 finished \tRBM Reconstruction error 52.947086\n",
      ">> Epoch 49 finished \tRBM Reconstruction error 39.453072\n",
      ">> Epoch 50 finished \tRBM Reconstruction error 57.382793\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 213.133987\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 338.352448\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 299.145416\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 251.689087\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 307.814728\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 266.905151\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 311.959290\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 353.286804\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 322.057983\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 310.856354\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 327.176239\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 308.853210\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 352.459076\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 319.874878\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 341.069214\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 267.378235\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 407.438507\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 321.490204\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 374.025848\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 329.614380\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 334.114990\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 378.316162\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 349.021454\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 344.572174\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 335.653870\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 292.200043\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 340.525238\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 317.476654\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 365.472687\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 335.029144\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 337.888428\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 416.404205\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 364.206940\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 351.500824\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 438.052277\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 362.763306\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 353.291962\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 397.952576\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 368.366638\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 434.857941\n",
      ">> Epoch 41 finished \tRBM Reconstruction error 320.781525\n",
      ">> Epoch 42 finished \tRBM Reconstruction error 376.645233\n",
      ">> Epoch 43 finished \tRBM Reconstruction error 335.900665\n",
      ">> Epoch 44 finished \tRBM Reconstruction error 398.253357\n",
      ">> Epoch 45 finished \tRBM Reconstruction error 404.312744\n",
      ">> Epoch 46 finished \tRBM Reconstruction error 334.645874\n",
      ">> Epoch 47 finished \tRBM Reconstruction error 375.031952\n",
      ">> Epoch 48 finished \tRBM Reconstruction error 350.045624\n",
      ">> Epoch 49 finished \tRBM Reconstruction error 344.914459\n",
      ">> Epoch 50 finished \tRBM Reconstruction error 344.322266\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.584264\n",
      ">> Epoch 1 finished \tANN training loss 0.601146\n",
      ">> Epoch 2 finished \tANN training loss 0.339275\n",
      ">> Epoch 3 finished \tANN training loss 0.276337\n",
      ">> Epoch 4 finished \tANN training loss 0.229558\n",
      ">> Epoch 5 finished \tANN training loss 0.211784\n",
      ">> Epoch 6 finished \tANN training loss 0.171505\n",
      ">> Epoch 7 finished \tANN training loss 0.159481\n",
      ">> Epoch 8 finished \tANN training loss 0.136390\n",
      ">> Epoch 9 finished \tANN training loss 0.122148\n",
      ">> Epoch 10 finished \tANN training loss 0.119484\n",
      ">> Epoch 11 finished \tANN training loss 0.126666\n",
      ">> Epoch 12 finished \tANN training loss 0.089896\n",
      ">> Epoch 13 finished \tANN training loss 0.083611\n",
      ">> Epoch 14 finished \tANN training loss 0.074343\n",
      ">> Epoch 15 finished \tANN training loss 0.067437\n",
      ">> Epoch 16 finished \tANN training loss 0.061726\n",
      ">> Epoch 17 finished \tANN training loss 0.056064\n",
      ">> Epoch 18 finished \tANN training loss 0.049404\n",
      ">> Epoch 19 finished \tANN training loss 0.039220\n",
      ">> Epoch 20 finished \tANN training loss 0.042682\n",
      ">> Epoch 21 finished \tANN training loss 0.040789\n",
      ">> Epoch 22 finished \tANN training loss 0.034783\n",
      ">> Epoch 23 finished \tANN training loss 0.028799\n",
      ">> Epoch 24 finished \tANN training loss 0.025559\n",
      ">> Epoch 25 finished \tANN training loss 0.026683\n",
      ">> Epoch 26 finished \tANN training loss 0.026904\n",
      ">> Epoch 27 finished \tANN training loss 0.022445\n",
      ">> Epoch 28 finished \tANN training loss 0.018863\n",
      ">> Epoch 29 finished \tANN training loss 0.017471\n",
      ">> Epoch 30 finished \tANN training loss 0.014674\n",
      ">> Epoch 31 finished \tANN training loss 0.014660\n",
      ">> Epoch 32 finished \tANN training loss 0.012769\n",
      ">> Epoch 33 finished \tANN training loss 0.013057\n",
      ">> Epoch 34 finished \tANN training loss 0.012746\n",
      ">> Epoch 35 finished \tANN training loss 0.012224\n",
      ">> Epoch 36 finished \tANN training loss 0.009964\n",
      ">> Epoch 37 finished \tANN training loss 0.010785\n",
      ">> Epoch 38 finished \tANN training loss 0.008956\n",
      ">> Epoch 39 finished \tANN training loss 0.009095\n",
      ">> Epoch 40 finished \tANN training loss 0.007297\n",
      ">> Epoch 41 finished \tANN training loss 0.008115\n",
      ">> Epoch 42 finished \tANN training loss 0.006544\n",
      ">> Epoch 43 finished \tANN training loss 0.006402\n",
      ">> Epoch 44 finished \tANN training loss 0.007203\n",
      ">> Epoch 45 finished \tANN training loss 0.005386\n",
      ">> Epoch 46 finished \tANN training loss 0.005045\n",
      ">> Epoch 47 finished \tANN training loss 0.004793\n",
      ">> Epoch 48 finished \tANN training loss 0.004895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 49 finished \tANN training loss 0.003995\n",
      ">> Epoch 50 finished \tANN training loss 0.004342\n",
      ">> Epoch 51 finished \tANN training loss 0.003956\n",
      ">> Epoch 52 finished \tANN training loss 0.004419\n",
      ">> Epoch 53 finished \tANN training loss 0.003506\n",
      ">> Epoch 54 finished \tANN training loss 0.003394\n",
      ">> Epoch 55 finished \tANN training loss 0.003094\n",
      ">> Epoch 56 finished \tANN training loss 0.003003\n",
      ">> Epoch 57 finished \tANN training loss 0.002911\n",
      ">> Epoch 58 finished \tANN training loss 0.003127\n",
      ">> Epoch 59 finished \tANN training loss 0.002690\n",
      ">> Epoch 60 finished \tANN training loss 0.002647\n",
      ">> Epoch 61 finished \tANN training loss 0.002495\n",
      ">> Epoch 62 finished \tANN training loss 0.002301\n",
      ">> Epoch 63 finished \tANN training loss 0.002322\n",
      ">> Epoch 64 finished \tANN training loss 0.002254\n",
      ">> Epoch 65 finished \tANN training loss 0.002566\n",
      ">> Epoch 66 finished \tANN training loss 0.002215\n",
      ">> Epoch 67 finished \tANN training loss 0.001974\n",
      ">> Epoch 68 finished \tANN training loss 0.002304\n",
      ">> Epoch 69 finished \tANN training loss 0.001946\n",
      ">> Epoch 70 finished \tANN training loss 0.001960\n",
      ">> Epoch 71 finished \tANN training loss 0.002041\n",
      ">> Epoch 72 finished \tANN training loss 0.002321\n",
      ">> Epoch 73 finished \tANN training loss 0.001933\n",
      ">> Epoch 74 finished \tANN training loss 0.001724\n",
      ">> Epoch 75 finished \tANN training loss 0.001629\n",
      ">> Epoch 76 finished \tANN training loss 0.001743\n",
      ">> Epoch 77 finished \tANN training loss 0.003076\n",
      ">> Epoch 78 finished \tANN training loss 0.001888\n",
      ">> Epoch 79 finished \tANN training loss 0.001550\n",
      ">> Epoch 80 finished \tANN training loss 0.001675\n",
      ">> Epoch 81 finished \tANN training loss 0.001476\n",
      ">> Epoch 82 finished \tANN training loss 0.001949\n",
      ">> Epoch 83 finished \tANN training loss 0.001521\n",
      ">> Epoch 84 finished \tANN training loss 0.001630\n",
      ">> Epoch 85 finished \tANN training loss 0.001235\n",
      ">> Epoch 86 finished \tANN training loss 0.001156\n",
      ">> Epoch 87 finished \tANN training loss 0.001300\n",
      ">> Epoch 88 finished \tANN training loss 0.001013\n",
      ">> Epoch 89 finished \tANN training loss 0.001001\n",
      ">> Epoch 90 finished \tANN training loss 0.000935\n",
      ">> Epoch 91 finished \tANN training loss 0.001067\n",
      ">> Epoch 92 finished \tANN training loss 0.000849\n",
      ">> Epoch 93 finished \tANN training loss 0.000783\n",
      ">> Epoch 94 finished \tANN training loss 0.000877\n",
      ">> Epoch 95 finished \tANN training loss 0.000764\n",
      ">> Epoch 96 finished \tANN training loss 0.000745\n",
      ">> Epoch 97 finished \tANN training loss 0.000785\n",
      ">> Epoch 98 finished \tANN training loss 0.000750\n",
      ">> Epoch 99 finished \tANN training loss 0.000882\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.935000\n"
     ]
    }
   ],
   "source": [
    "epoch_rbm_acc[7] = deep_belief_net(n_epochs_rbm=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.935\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(epoch_rbm_acc[7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 9\n",
    "\n",
    "n_epochs_rbm=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 84.992477\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 52.556793\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 73.466675\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 80.173210\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 74.324738\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 48.524315\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 52.935352\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 42.551929\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 57.618774\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 52.438831\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 67.684494\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 50.824688\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 54.989883\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 41.339443\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 51.853363\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 62.527103\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 44.851845\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 57.342430\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 68.836800\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 64.142677\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 50.309280\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 66.416504\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 65.265877\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 68.634033\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 72.447258\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 62.991241\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 53.245419\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 51.869347\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 60.460190\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 63.755905\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 57.239609\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 72.437836\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 58.030655\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 76.129059\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 59.967358\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 58.605927\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 46.845284\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 63.994873\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 81.558426\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 69.042480\n",
      ">> Epoch 41 finished \tRBM Reconstruction error 70.586403\n",
      ">> Epoch 42 finished \tRBM Reconstruction error 69.694931\n",
      ">> Epoch 43 finished \tRBM Reconstruction error 58.761074\n",
      ">> Epoch 44 finished \tRBM Reconstruction error 59.303436\n",
      ">> Epoch 45 finished \tRBM Reconstruction error 70.437767\n",
      ">> Epoch 46 finished \tRBM Reconstruction error 72.135811\n",
      ">> Epoch 47 finished \tRBM Reconstruction error 72.889763\n",
      ">> Epoch 48 finished \tRBM Reconstruction error 66.220627\n",
      ">> Epoch 49 finished \tRBM Reconstruction error 60.591026\n",
      ">> Epoch 50 finished \tRBM Reconstruction error 59.660934\n",
      ">> Epoch 51 finished \tRBM Reconstruction error 52.676762\n",
      ">> Epoch 52 finished \tRBM Reconstruction error 58.841507\n",
      ">> Epoch 53 finished \tRBM Reconstruction error 59.250389\n",
      ">> Epoch 54 finished \tRBM Reconstruction error 61.562881\n",
      ">> Epoch 55 finished \tRBM Reconstruction error 64.020500\n",
      ">> Epoch 56 finished \tRBM Reconstruction error 75.542999\n",
      ">> Epoch 57 finished \tRBM Reconstruction error 86.135468\n",
      ">> Epoch 58 finished \tRBM Reconstruction error 72.930153\n",
      ">> Epoch 59 finished \tRBM Reconstruction error 68.531693\n",
      ">> Epoch 60 finished \tRBM Reconstruction error 66.590271\n",
      ">> Epoch 61 finished \tRBM Reconstruction error 65.800110\n",
      ">> Epoch 62 finished \tRBM Reconstruction error 70.119133\n",
      ">> Epoch 63 finished \tRBM Reconstruction error 51.320995\n",
      ">> Epoch 64 finished \tRBM Reconstruction error 74.674095\n",
      ">> Epoch 65 finished \tRBM Reconstruction error 60.624901\n",
      ">> Epoch 66 finished \tRBM Reconstruction error 65.577812\n",
      ">> Epoch 67 finished \tRBM Reconstruction error 59.038124\n",
      ">> Epoch 68 finished \tRBM Reconstruction error 71.874603\n",
      ">> Epoch 69 finished \tRBM Reconstruction error 61.118320\n",
      ">> Epoch 70 finished \tRBM Reconstruction error 61.531944\n",
      ">> Epoch 71 finished \tRBM Reconstruction error 74.493698\n",
      ">> Epoch 72 finished \tRBM Reconstruction error 61.616100\n",
      ">> Epoch 73 finished \tRBM Reconstruction error 68.625793\n",
      ">> Epoch 74 finished \tRBM Reconstruction error 68.173172\n",
      ">> Epoch 75 finished \tRBM Reconstruction error 73.596764\n",
      ">> Epoch 76 finished \tRBM Reconstruction error 71.690948\n",
      ">> Epoch 77 finished \tRBM Reconstruction error 65.570526\n",
      ">> Epoch 78 finished \tRBM Reconstruction error 67.789726\n",
      ">> Epoch 79 finished \tRBM Reconstruction error 70.412102\n",
      ">> Epoch 80 finished \tRBM Reconstruction error 65.344635\n",
      ">> Epoch 81 finished \tRBM Reconstruction error 74.531395\n",
      ">> Epoch 82 finished \tRBM Reconstruction error 64.325745\n",
      ">> Epoch 83 finished \tRBM Reconstruction error 74.523148\n",
      ">> Epoch 84 finished \tRBM Reconstruction error 76.646523\n",
      ">> Epoch 85 finished \tRBM Reconstruction error 78.887550\n",
      ">> Epoch 86 finished \tRBM Reconstruction error 66.124756\n",
      ">> Epoch 87 finished \tRBM Reconstruction error 78.859802\n",
      ">> Epoch 88 finished \tRBM Reconstruction error 71.580421\n",
      ">> Epoch 89 finished \tRBM Reconstruction error 65.830467\n",
      ">> Epoch 90 finished \tRBM Reconstruction error 61.026554\n",
      ">> Epoch 91 finished \tRBM Reconstruction error 67.471886\n",
      ">> Epoch 92 finished \tRBM Reconstruction error 72.046974\n",
      ">> Epoch 93 finished \tRBM Reconstruction error 86.530197\n",
      ">> Epoch 94 finished \tRBM Reconstruction error 74.723808\n",
      ">> Epoch 95 finished \tRBM Reconstruction error 80.348633\n",
      ">> Epoch 96 finished \tRBM Reconstruction error 83.790108\n",
      ">> Epoch 97 finished \tRBM Reconstruction error 60.073048\n",
      ">> Epoch 98 finished \tRBM Reconstruction error 74.876457\n",
      ">> Epoch 99 finished \tRBM Reconstruction error 68.328323\n",
      ">> Epoch 100 finished \tRBM Reconstruction error 72.572624\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 166.821716\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 178.397888\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 231.433197\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 263.356140\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 329.514954\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 266.165741\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 244.663712\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 284.585052\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 280.442444\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 259.345001\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 329.892212\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 385.542969\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 341.492035\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 357.654053\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 340.163788\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 318.769623\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 346.287109\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 409.188629\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 409.677124\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 393.112030\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 338.218933\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 400.326416\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 350.357056\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 333.267303\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 299.476959\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 379.188293\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 375.037384\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 325.156494\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 398.434723\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 395.710327\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 434.418793\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 368.240540\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 410.497253\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 500.624298\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 397.939148\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 469.171326\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 428.012787\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 437.173706\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 416.097565\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 390.610107\n",
      ">> Epoch 41 finished \tRBM Reconstruction error 506.024597\n",
      ">> Epoch 42 finished \tRBM Reconstruction error 448.386108\n",
      ">> Epoch 43 finished \tRBM Reconstruction error 408.992004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 44 finished \tRBM Reconstruction error 390.676178\n",
      ">> Epoch 45 finished \tRBM Reconstruction error 462.731171\n",
      ">> Epoch 46 finished \tRBM Reconstruction error 442.611176\n",
      ">> Epoch 47 finished \tRBM Reconstruction error 411.340790\n",
      ">> Epoch 48 finished \tRBM Reconstruction error 442.644623\n",
      ">> Epoch 49 finished \tRBM Reconstruction error 374.582275\n",
      ">> Epoch 50 finished \tRBM Reconstruction error 349.972748\n",
      ">> Epoch 51 finished \tRBM Reconstruction error 449.284760\n",
      ">> Epoch 52 finished \tRBM Reconstruction error 421.688660\n",
      ">> Epoch 53 finished \tRBM Reconstruction error 431.388123\n",
      ">> Epoch 54 finished \tRBM Reconstruction error 455.646088\n",
      ">> Epoch 55 finished \tRBM Reconstruction error 441.548523\n",
      ">> Epoch 56 finished \tRBM Reconstruction error 390.123505\n",
      ">> Epoch 57 finished \tRBM Reconstruction error 404.121216\n",
      ">> Epoch 58 finished \tRBM Reconstruction error 499.455383\n",
      ">> Epoch 59 finished \tRBM Reconstruction error 491.206238\n",
      ">> Epoch 60 finished \tRBM Reconstruction error 469.942810\n",
      ">> Epoch 61 finished \tRBM Reconstruction error 378.776733\n",
      ">> Epoch 62 finished \tRBM Reconstruction error 401.625458\n",
      ">> Epoch 63 finished \tRBM Reconstruction error 432.655701\n",
      ">> Epoch 64 finished \tRBM Reconstruction error 378.751770\n",
      ">> Epoch 65 finished \tRBM Reconstruction error 427.068665\n",
      ">> Epoch 66 finished \tRBM Reconstruction error 431.678741\n",
      ">> Epoch 67 finished \tRBM Reconstruction error 425.514221\n",
      ">> Epoch 68 finished \tRBM Reconstruction error 420.490387\n",
      ">> Epoch 69 finished \tRBM Reconstruction error 445.810669\n",
      ">> Epoch 70 finished \tRBM Reconstruction error 421.802734\n",
      ">> Epoch 71 finished \tRBM Reconstruction error 486.526611\n",
      ">> Epoch 72 finished \tRBM Reconstruction error 383.343872\n",
      ">> Epoch 73 finished \tRBM Reconstruction error 398.779724\n",
      ">> Epoch 74 finished \tRBM Reconstruction error 454.570618\n",
      ">> Epoch 75 finished \tRBM Reconstruction error 458.796570\n",
      ">> Epoch 76 finished \tRBM Reconstruction error 435.384857\n",
      ">> Epoch 77 finished \tRBM Reconstruction error 434.145935\n",
      ">> Epoch 78 finished \tRBM Reconstruction error 405.343567\n",
      ">> Epoch 79 finished \tRBM Reconstruction error 408.926056\n",
      ">> Epoch 80 finished \tRBM Reconstruction error 404.321808\n",
      ">> Epoch 81 finished \tRBM Reconstruction error 416.005615\n",
      ">> Epoch 82 finished \tRBM Reconstruction error 454.045044\n",
      ">> Epoch 83 finished \tRBM Reconstruction error 452.026184\n",
      ">> Epoch 84 finished \tRBM Reconstruction error 394.969299\n",
      ">> Epoch 85 finished \tRBM Reconstruction error 393.848816\n",
      ">> Epoch 86 finished \tRBM Reconstruction error 446.170807\n",
      ">> Epoch 87 finished \tRBM Reconstruction error 447.480743\n",
      ">> Epoch 88 finished \tRBM Reconstruction error 416.879852\n",
      ">> Epoch 89 finished \tRBM Reconstruction error 485.849854\n",
      ">> Epoch 90 finished \tRBM Reconstruction error 417.005280\n",
      ">> Epoch 91 finished \tRBM Reconstruction error 412.727112\n",
      ">> Epoch 92 finished \tRBM Reconstruction error 432.149536\n",
      ">> Epoch 93 finished \tRBM Reconstruction error 443.331329\n",
      ">> Epoch 94 finished \tRBM Reconstruction error 395.693939\n",
      ">> Epoch 95 finished \tRBM Reconstruction error 414.196472\n",
      ">> Epoch 96 finished \tRBM Reconstruction error 441.622894\n",
      ">> Epoch 97 finished \tRBM Reconstruction error 408.144989\n",
      ">> Epoch 98 finished \tRBM Reconstruction error 404.593597\n",
      ">> Epoch 99 finished \tRBM Reconstruction error 403.426514\n",
      ">> Epoch 100 finished \tRBM Reconstruction error 396.056885\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.576869\n",
      ">> Epoch 1 finished \tANN training loss 0.407056\n",
      ">> Epoch 2 finished \tANN training loss 0.345779\n",
      ">> Epoch 3 finished \tANN training loss 0.261881\n",
      ">> Epoch 4 finished \tANN training loss 0.240444\n",
      ">> Epoch 5 finished \tANN training loss 0.198612\n",
      ">> Epoch 6 finished \tANN training loss 0.194819\n",
      ">> Epoch 7 finished \tANN training loss 0.165919\n",
      ">> Epoch 8 finished \tANN training loss 0.132419\n",
      ">> Epoch 9 finished \tANN training loss 0.128545\n",
      ">> Epoch 10 finished \tANN training loss 0.107657\n",
      ">> Epoch 11 finished \tANN training loss 0.100526\n",
      ">> Epoch 12 finished \tANN training loss 0.082155\n",
      ">> Epoch 13 finished \tANN training loss 0.074399\n",
      ">> Epoch 14 finished \tANN training loss 0.071147\n",
      ">> Epoch 15 finished \tANN training loss 0.067799\n",
      ">> Epoch 16 finished \tANN training loss 0.055883\n",
      ">> Epoch 17 finished \tANN training loss 0.049922\n",
      ">> Epoch 18 finished \tANN training loss 0.048260\n",
      ">> Epoch 19 finished \tANN training loss 0.040317\n",
      ">> Epoch 20 finished \tANN training loss 0.036422\n",
      ">> Epoch 21 finished \tANN training loss 0.032949\n",
      ">> Epoch 22 finished \tANN training loss 0.034304\n",
      ">> Epoch 23 finished \tANN training loss 0.031671\n",
      ">> Epoch 24 finished \tANN training loss 0.026090\n",
      ">> Epoch 25 finished \tANN training loss 0.023633\n",
      ">> Epoch 26 finished \tANN training loss 0.027602\n",
      ">> Epoch 27 finished \tANN training loss 0.021058\n",
      ">> Epoch 28 finished \tANN training loss 0.017867\n",
      ">> Epoch 29 finished \tANN training loss 0.018436\n",
      ">> Epoch 30 finished \tANN training loss 0.015290\n",
      ">> Epoch 31 finished \tANN training loss 0.015817\n",
      ">> Epoch 32 finished \tANN training loss 0.014544\n",
      ">> Epoch 33 finished \tANN training loss 0.014058\n",
      ">> Epoch 34 finished \tANN training loss 0.013738\n",
      ">> Epoch 35 finished \tANN training loss 0.014632\n",
      ">> Epoch 36 finished \tANN training loss 0.011158\n",
      ">> Epoch 37 finished \tANN training loss 0.010219\n",
      ">> Epoch 38 finished \tANN training loss 0.009698\n",
      ">> Epoch 39 finished \tANN training loss 0.009048\n",
      ">> Epoch 40 finished \tANN training loss 0.008777\n",
      ">> Epoch 41 finished \tANN training loss 0.007614\n",
      ">> Epoch 42 finished \tANN training loss 0.007825\n",
      ">> Epoch 43 finished \tANN training loss 0.007512\n",
      ">> Epoch 44 finished \tANN training loss 0.008187\n",
      ">> Epoch 45 finished \tANN training loss 0.006796\n",
      ">> Epoch 46 finished \tANN training loss 0.007157\n",
      ">> Epoch 47 finished \tANN training loss 0.006648\n",
      ">> Epoch 48 finished \tANN training loss 0.006239\n",
      ">> Epoch 49 finished \tANN training loss 0.005633\n",
      ">> Epoch 50 finished \tANN training loss 0.005273\n",
      ">> Epoch 51 finished \tANN training loss 0.005476\n",
      ">> Epoch 52 finished \tANN training loss 0.004579\n",
      ">> Epoch 53 finished \tANN training loss 0.004341\n",
      ">> Epoch 54 finished \tANN training loss 0.004947\n",
      ">> Epoch 55 finished \tANN training loss 0.004789\n",
      ">> Epoch 56 finished \tANN training loss 0.003734\n",
      ">> Epoch 57 finished \tANN training loss 0.003994\n",
      ">> Epoch 58 finished \tANN training loss 0.003475\n",
      ">> Epoch 59 finished \tANN training loss 0.003076\n",
      ">> Epoch 60 finished \tANN training loss 0.002800\n",
      ">> Epoch 61 finished \tANN training loss 0.003819\n",
      ">> Epoch 62 finished \tANN training loss 0.003440\n",
      ">> Epoch 63 finished \tANN training loss 0.003221\n",
      ">> Epoch 64 finished \tANN training loss 0.002572\n",
      ">> Epoch 65 finished \tANN training loss 0.003055\n",
      ">> Epoch 66 finished \tANN training loss 0.002751\n",
      ">> Epoch 67 finished \tANN training loss 0.002545\n",
      ">> Epoch 68 finished \tANN training loss 0.002377\n",
      ">> Epoch 69 finished \tANN training loss 0.002148\n",
      ">> Epoch 70 finished \tANN training loss 0.001939\n",
      ">> Epoch 71 finished \tANN training loss 0.001876\n",
      ">> Epoch 72 finished \tANN training loss 0.002231\n",
      ">> Epoch 73 finished \tANN training loss 0.001977\n",
      ">> Epoch 74 finished \tANN training loss 0.001693\n",
      ">> Epoch 75 finished \tANN training loss 0.001671\n",
      ">> Epoch 76 finished \tANN training loss 0.001542\n",
      ">> Epoch 77 finished \tANN training loss 0.001481\n",
      ">> Epoch 78 finished \tANN training loss 0.001561\n",
      ">> Epoch 79 finished \tANN training loss 0.001599\n",
      ">> Epoch 80 finished \tANN training loss 0.001304\n",
      ">> Epoch 81 finished \tANN training loss 0.001401\n",
      ">> Epoch 82 finished \tANN training loss 0.001284\n",
      ">> Epoch 83 finished \tANN training loss 0.001515\n",
      ">> Epoch 84 finished \tANN training loss 0.001475\n",
      ">> Epoch 85 finished \tANN training loss 0.001241\n",
      ">> Epoch 86 finished \tANN training loss 0.001370\n",
      ">> Epoch 87 finished \tANN training loss 0.001128\n",
      ">> Epoch 88 finished \tANN training loss 0.001349\n",
      ">> Epoch 89 finished \tANN training loss 0.001245\n",
      ">> Epoch 90 finished \tANN training loss 0.001305\n",
      ">> Epoch 91 finished \tANN training loss 0.001100\n",
      ">> Epoch 92 finished \tANN training loss 0.001052\n",
      ">> Epoch 93 finished \tANN training loss 0.001122\n",
      ">> Epoch 94 finished \tANN training loss 0.001050\n",
      ">> Epoch 95 finished \tANN training loss 0.001096\n",
      ">> Epoch 96 finished \tANN training loss 0.001131\n",
      ">> Epoch 97 finished \tANN training loss 0.001043\n",
      ">> Epoch 98 finished \tANN training loss 0.001037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 99 finished \tANN training loss 0.001104\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.935000\n"
     ]
    }
   ],
   "source": [
    "epoch_rbm_acc[8] = deep_belief_net(n_epochs_rbm=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.935\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(epoch_rbm_acc[8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 10\n",
    "\n",
    "n_epochs_rbm=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 86.169647\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 81.358040\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 60.489590\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 40.834915\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 65.904037\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 63.146137\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 44.753300\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 48.984268\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 38.161121\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 49.548874\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 39.734428\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 49.296494\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 45.057617\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 55.243423\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 70.538704\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 51.605606\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 50.278858\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 51.347343\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 48.179760\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 54.281837\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 55.873173\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 44.195770\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 51.919102\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 44.696171\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 63.384068\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 51.782612\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 44.389736\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 69.149544\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 58.376766\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 61.423660\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 50.851192\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 54.898991\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 54.685997\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 59.066662\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 58.568642\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 41.347946\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 66.320129\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 42.053623\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 66.066628\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 51.240334\n",
      ">> Epoch 41 finished \tRBM Reconstruction error 56.586914\n",
      ">> Epoch 42 finished \tRBM Reconstruction error 61.814941\n",
      ">> Epoch 43 finished \tRBM Reconstruction error 73.004051\n",
      ">> Epoch 44 finished \tRBM Reconstruction error 61.175583\n",
      ">> Epoch 45 finished \tRBM Reconstruction error 57.854366\n",
      ">> Epoch 46 finished \tRBM Reconstruction error 60.186592\n",
      ">> Epoch 47 finished \tRBM Reconstruction error 55.606598\n",
      ">> Epoch 48 finished \tRBM Reconstruction error 62.470951\n",
      ">> Epoch 49 finished \tRBM Reconstruction error 61.750484\n",
      ">> Epoch 50 finished \tRBM Reconstruction error 61.097965\n",
      ">> Epoch 51 finished \tRBM Reconstruction error 56.686623\n",
      ">> Epoch 52 finished \tRBM Reconstruction error 53.605816\n",
      ">> Epoch 53 finished \tRBM Reconstruction error 69.164619\n",
      ">> Epoch 54 finished \tRBM Reconstruction error 59.010941\n",
      ">> Epoch 55 finished \tRBM Reconstruction error 60.593632\n",
      ">> Epoch 56 finished \tRBM Reconstruction error 65.537514\n",
      ">> Epoch 57 finished \tRBM Reconstruction error 63.109623\n",
      ">> Epoch 58 finished \tRBM Reconstruction error 56.241947\n",
      ">> Epoch 59 finished \tRBM Reconstruction error 74.845139\n",
      ">> Epoch 60 finished \tRBM Reconstruction error 64.817276\n",
      ">> Epoch 61 finished \tRBM Reconstruction error 55.165424\n",
      ">> Epoch 62 finished \tRBM Reconstruction error 66.517609\n",
      ">> Epoch 63 finished \tRBM Reconstruction error 69.718819\n",
      ">> Epoch 64 finished \tRBM Reconstruction error 61.343643\n",
      ">> Epoch 65 finished \tRBM Reconstruction error 73.802361\n",
      ">> Epoch 66 finished \tRBM Reconstruction error 68.328934\n",
      ">> Epoch 67 finished \tRBM Reconstruction error 57.592617\n",
      ">> Epoch 68 finished \tRBM Reconstruction error 73.209778\n",
      ">> Epoch 69 finished \tRBM Reconstruction error 65.677193\n",
      ">> Epoch 70 finished \tRBM Reconstruction error 75.776428\n",
      ">> Epoch 71 finished \tRBM Reconstruction error 83.466217\n",
      ">> Epoch 72 finished \tRBM Reconstruction error 78.278069\n",
      ">> Epoch 73 finished \tRBM Reconstruction error 67.068916\n",
      ">> Epoch 74 finished \tRBM Reconstruction error 94.925179\n",
      ">> Epoch 75 finished \tRBM Reconstruction error 67.252594\n",
      ">> Epoch 76 finished \tRBM Reconstruction error 73.531258\n",
      ">> Epoch 77 finished \tRBM Reconstruction error 73.991745\n",
      ">> Epoch 78 finished \tRBM Reconstruction error 65.813904\n",
      ">> Epoch 79 finished \tRBM Reconstruction error 67.631790\n",
      ">> Epoch 80 finished \tRBM Reconstruction error 68.221146\n",
      ">> Epoch 81 finished \tRBM Reconstruction error 70.927673\n",
      ">> Epoch 82 finished \tRBM Reconstruction error 92.732536\n",
      ">> Epoch 83 finished \tRBM Reconstruction error 84.057869\n",
      ">> Epoch 84 finished \tRBM Reconstruction error 70.770325\n",
      ">> Epoch 85 finished \tRBM Reconstruction error 76.784637\n",
      ">> Epoch 86 finished \tRBM Reconstruction error 69.351105\n",
      ">> Epoch 87 finished \tRBM Reconstruction error 73.990410\n",
      ">> Epoch 88 finished \tRBM Reconstruction error 63.194855\n",
      ">> Epoch 89 finished \tRBM Reconstruction error 80.614197\n",
      ">> Epoch 90 finished \tRBM Reconstruction error 70.397087\n",
      ">> Epoch 91 finished \tRBM Reconstruction error 83.896873\n",
      ">> Epoch 92 finished \tRBM Reconstruction error 76.449364\n",
      ">> Epoch 93 finished \tRBM Reconstruction error 67.390244\n",
      ">> Epoch 94 finished \tRBM Reconstruction error 69.847427\n",
      ">> Epoch 95 finished \tRBM Reconstruction error 80.721565\n",
      ">> Epoch 96 finished \tRBM Reconstruction error 101.535843\n",
      ">> Epoch 97 finished \tRBM Reconstruction error 69.988991\n",
      ">> Epoch 98 finished \tRBM Reconstruction error 76.583069\n",
      ">> Epoch 99 finished \tRBM Reconstruction error 64.983322\n",
      ">> Epoch 100 finished \tRBM Reconstruction error 80.810028\n",
      ">> Epoch 101 finished \tRBM Reconstruction error 73.804680\n",
      ">> Epoch 102 finished \tRBM Reconstruction error 76.415520\n",
      ">> Epoch 103 finished \tRBM Reconstruction error 74.318832\n",
      ">> Epoch 104 finished \tRBM Reconstruction error 77.692200\n",
      ">> Epoch 105 finished \tRBM Reconstruction error 83.253944\n",
      ">> Epoch 106 finished \tRBM Reconstruction error 78.030594\n",
      ">> Epoch 107 finished \tRBM Reconstruction error 77.043137\n",
      ">> Epoch 108 finished \tRBM Reconstruction error 78.982445\n",
      ">> Epoch 109 finished \tRBM Reconstruction error 86.739471\n",
      ">> Epoch 110 finished \tRBM Reconstruction error 85.330849\n",
      ">> Epoch 111 finished \tRBM Reconstruction error 74.359673\n",
      ">> Epoch 112 finished \tRBM Reconstruction error 87.982132\n",
      ">> Epoch 113 finished \tRBM Reconstruction error 77.173943\n",
      ">> Epoch 114 finished \tRBM Reconstruction error 77.547699\n",
      ">> Epoch 115 finished \tRBM Reconstruction error 76.070030\n",
      ">> Epoch 116 finished \tRBM Reconstruction error 78.245758\n",
      ">> Epoch 117 finished \tRBM Reconstruction error 77.661072\n",
      ">> Epoch 118 finished \tRBM Reconstruction error 77.824265\n",
      ">> Epoch 119 finished \tRBM Reconstruction error 83.252792\n",
      ">> Epoch 120 finished \tRBM Reconstruction error 73.695251\n",
      ">> Epoch 121 finished \tRBM Reconstruction error 74.751289\n",
      ">> Epoch 122 finished \tRBM Reconstruction error 66.545647\n",
      ">> Epoch 123 finished \tRBM Reconstruction error 71.500481\n",
      ">> Epoch 124 finished \tRBM Reconstruction error 77.872147\n",
      ">> Epoch 125 finished \tRBM Reconstruction error 76.337883\n",
      ">> Epoch 126 finished \tRBM Reconstruction error 79.714294\n",
      ">> Epoch 127 finished \tRBM Reconstruction error 71.653366\n",
      ">> Epoch 128 finished \tRBM Reconstruction error 79.781425\n",
      ">> Epoch 129 finished \tRBM Reconstruction error 68.465622\n",
      ">> Epoch 130 finished \tRBM Reconstruction error 73.658562\n",
      ">> Epoch 131 finished \tRBM Reconstruction error 87.482605\n",
      ">> Epoch 132 finished \tRBM Reconstruction error 88.910751\n",
      ">> Epoch 133 finished \tRBM Reconstruction error 71.230736\n",
      ">> Epoch 134 finished \tRBM Reconstruction error 83.502472\n",
      ">> Epoch 135 finished \tRBM Reconstruction error 79.419579\n",
      ">> Epoch 136 finished \tRBM Reconstruction error 71.616287\n",
      ">> Epoch 137 finished \tRBM Reconstruction error 82.100830\n",
      ">> Epoch 138 finished \tRBM Reconstruction error 73.456741\n",
      ">> Epoch 139 finished \tRBM Reconstruction error 73.453003\n",
      ">> Epoch 140 finished \tRBM Reconstruction error 82.125877\n",
      ">> Epoch 141 finished \tRBM Reconstruction error 82.988487\n",
      ">> Epoch 142 finished \tRBM Reconstruction error 76.397919\n",
      ">> Epoch 143 finished \tRBM Reconstruction error 82.337692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 144 finished \tRBM Reconstruction error 81.587013\n",
      ">> Epoch 145 finished \tRBM Reconstruction error 75.689514\n",
      ">> Epoch 146 finished \tRBM Reconstruction error 79.387154\n",
      ">> Epoch 147 finished \tRBM Reconstruction error 76.072952\n",
      ">> Epoch 148 finished \tRBM Reconstruction error 95.129082\n",
      ">> Epoch 149 finished \tRBM Reconstruction error 74.569229\n",
      ">> Epoch 150 finished \tRBM Reconstruction error 73.790115\n",
      ">> Epoch 151 finished \tRBM Reconstruction error 83.404228\n",
      ">> Epoch 152 finished \tRBM Reconstruction error 90.879547\n",
      ">> Epoch 153 finished \tRBM Reconstruction error 100.215530\n",
      ">> Epoch 154 finished \tRBM Reconstruction error 86.833084\n",
      ">> Epoch 155 finished \tRBM Reconstruction error 81.367180\n",
      ">> Epoch 156 finished \tRBM Reconstruction error 70.140007\n",
      ">> Epoch 157 finished \tRBM Reconstruction error 75.073547\n",
      ">> Epoch 158 finished \tRBM Reconstruction error 84.982422\n",
      ">> Epoch 159 finished \tRBM Reconstruction error 83.696251\n",
      ">> Epoch 160 finished \tRBM Reconstruction error 97.877678\n",
      ">> Epoch 161 finished \tRBM Reconstruction error 92.834785\n",
      ">> Epoch 162 finished \tRBM Reconstruction error 75.993179\n",
      ">> Epoch 163 finished \tRBM Reconstruction error 72.128563\n",
      ">> Epoch 164 finished \tRBM Reconstruction error 84.952599\n",
      ">> Epoch 165 finished \tRBM Reconstruction error 90.093224\n",
      ">> Epoch 166 finished \tRBM Reconstruction error 74.558708\n",
      ">> Epoch 167 finished \tRBM Reconstruction error 77.679665\n",
      ">> Epoch 168 finished \tRBM Reconstruction error 74.439850\n",
      ">> Epoch 169 finished \tRBM Reconstruction error 77.217209\n",
      ">> Epoch 170 finished \tRBM Reconstruction error 83.473640\n",
      ">> Epoch 171 finished \tRBM Reconstruction error 85.622849\n",
      ">> Epoch 172 finished \tRBM Reconstruction error 75.487206\n",
      ">> Epoch 173 finished \tRBM Reconstruction error 75.090172\n",
      ">> Epoch 174 finished \tRBM Reconstruction error 81.217461\n",
      ">> Epoch 175 finished \tRBM Reconstruction error 84.559418\n",
      ">> Epoch 176 finished \tRBM Reconstruction error 79.244530\n",
      ">> Epoch 177 finished \tRBM Reconstruction error 79.027519\n",
      ">> Epoch 178 finished \tRBM Reconstruction error 83.754539\n",
      ">> Epoch 179 finished \tRBM Reconstruction error 85.253136\n",
      ">> Epoch 180 finished \tRBM Reconstruction error 89.411407\n",
      ">> Epoch 181 finished \tRBM Reconstruction error 83.771263\n",
      ">> Epoch 182 finished \tRBM Reconstruction error 85.432816\n",
      ">> Epoch 183 finished \tRBM Reconstruction error 94.084122\n",
      ">> Epoch 184 finished \tRBM Reconstruction error 81.510033\n",
      ">> Epoch 185 finished \tRBM Reconstruction error 85.016922\n",
      ">> Epoch 186 finished \tRBM Reconstruction error 84.395645\n",
      ">> Epoch 187 finished \tRBM Reconstruction error 90.970016\n",
      ">> Epoch 188 finished \tRBM Reconstruction error 84.848083\n",
      ">> Epoch 189 finished \tRBM Reconstruction error 92.935577\n",
      ">> Epoch 190 finished \tRBM Reconstruction error 88.207207\n",
      ">> Epoch 191 finished \tRBM Reconstruction error 83.159416\n",
      ">> Epoch 192 finished \tRBM Reconstruction error 89.521912\n",
      ">> Epoch 193 finished \tRBM Reconstruction error 98.056778\n",
      ">> Epoch 194 finished \tRBM Reconstruction error 83.408142\n",
      ">> Epoch 195 finished \tRBM Reconstruction error 79.687767\n",
      ">> Epoch 196 finished \tRBM Reconstruction error 85.398499\n",
      ">> Epoch 197 finished \tRBM Reconstruction error 81.319656\n",
      ">> Epoch 198 finished \tRBM Reconstruction error 80.226807\n",
      ">> Epoch 199 finished \tRBM Reconstruction error 91.611427\n",
      ">> Epoch 200 finished \tRBM Reconstruction error 84.971115\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 287.830597\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 323.149261\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 252.951263\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 258.009155\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 333.345123\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 487.712341\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 347.790588\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 264.041199\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 329.729218\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 321.138489\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 305.326599\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 341.571014\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 425.777191\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 492.711029\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 487.143524\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 394.336334\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 380.845123\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 477.089294\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 525.601929\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 374.534302\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 436.522369\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 530.643372\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 433.983215\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 540.533875\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 449.924438\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 440.710388\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 466.263519\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 442.993958\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 459.286316\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 451.170319\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 533.898010\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 579.201477\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 509.056335\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 494.003784\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 468.574615\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 453.717377\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 472.755005\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 548.911621\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 507.398895\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 425.570221\n",
      ">> Epoch 41 finished \tRBM Reconstruction error 545.266479\n",
      ">> Epoch 42 finished \tRBM Reconstruction error 508.583038\n",
      ">> Epoch 43 finished \tRBM Reconstruction error 466.329987\n",
      ">> Epoch 44 finished \tRBM Reconstruction error 504.993073\n",
      ">> Epoch 45 finished \tRBM Reconstruction error 518.451599\n",
      ">> Epoch 46 finished \tRBM Reconstruction error 543.099976\n",
      ">> Epoch 47 finished \tRBM Reconstruction error 514.068848\n",
      ">> Epoch 48 finished \tRBM Reconstruction error 524.385010\n",
      ">> Epoch 49 finished \tRBM Reconstruction error 544.380554\n",
      ">> Epoch 50 finished \tRBM Reconstruction error 489.181732\n",
      ">> Epoch 51 finished \tRBM Reconstruction error 431.480011\n",
      ">> Epoch 52 finished \tRBM Reconstruction error 533.855774\n",
      ">> Epoch 53 finished \tRBM Reconstruction error 489.243286\n",
      ">> Epoch 54 finished \tRBM Reconstruction error 496.322113\n",
      ">> Epoch 55 finished \tRBM Reconstruction error 483.236847\n",
      ">> Epoch 56 finished \tRBM Reconstruction error 521.697937\n",
      ">> Epoch 57 finished \tRBM Reconstruction error 468.555695\n",
      ">> Epoch 58 finished \tRBM Reconstruction error 510.729858\n",
      ">> Epoch 59 finished \tRBM Reconstruction error 473.394409\n",
      ">> Epoch 60 finished \tRBM Reconstruction error 472.054871\n",
      ">> Epoch 61 finished \tRBM Reconstruction error 461.579285\n",
      ">> Epoch 62 finished \tRBM Reconstruction error 449.784027\n",
      ">> Epoch 63 finished \tRBM Reconstruction error 498.795380\n",
      ">> Epoch 64 finished \tRBM Reconstruction error 487.803528\n",
      ">> Epoch 65 finished \tRBM Reconstruction error 475.901886\n",
      ">> Epoch 66 finished \tRBM Reconstruction error 559.555603\n",
      ">> Epoch 67 finished \tRBM Reconstruction error 529.383972\n",
      ">> Epoch 68 finished \tRBM Reconstruction error 507.898529\n",
      ">> Epoch 69 finished \tRBM Reconstruction error 504.421753\n",
      ">> Epoch 70 finished \tRBM Reconstruction error 463.379517\n",
      ">> Epoch 71 finished \tRBM Reconstruction error 537.247742\n",
      ">> Epoch 72 finished \tRBM Reconstruction error 470.558197\n",
      ">> Epoch 73 finished \tRBM Reconstruction error 501.816711\n",
      ">> Epoch 74 finished \tRBM Reconstruction error 545.857483\n",
      ">> Epoch 75 finished \tRBM Reconstruction error 573.621460\n",
      ">> Epoch 76 finished \tRBM Reconstruction error 537.900024\n",
      ">> Epoch 77 finished \tRBM Reconstruction error 511.548218\n",
      ">> Epoch 78 finished \tRBM Reconstruction error 541.434631\n",
      ">> Epoch 79 finished \tRBM Reconstruction error 525.470337\n",
      ">> Epoch 80 finished \tRBM Reconstruction error 547.909302\n",
      ">> Epoch 81 finished \tRBM Reconstruction error 476.379852\n",
      ">> Epoch 82 finished \tRBM Reconstruction error 496.112976\n",
      ">> Epoch 83 finished \tRBM Reconstruction error 493.870483\n",
      ">> Epoch 84 finished \tRBM Reconstruction error 508.424133\n",
      ">> Epoch 85 finished \tRBM Reconstruction error 491.556946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 86 finished \tRBM Reconstruction error 547.499268\n",
      ">> Epoch 87 finished \tRBM Reconstruction error 541.558228\n",
      ">> Epoch 88 finished \tRBM Reconstruction error 509.784424\n",
      ">> Epoch 89 finished \tRBM Reconstruction error 531.049011\n",
      ">> Epoch 90 finished \tRBM Reconstruction error 473.747772\n",
      ">> Epoch 91 finished \tRBM Reconstruction error 538.157654\n",
      ">> Epoch 92 finished \tRBM Reconstruction error 507.703735\n",
      ">> Epoch 93 finished \tRBM Reconstruction error 526.731812\n",
      ">> Epoch 94 finished \tRBM Reconstruction error 467.144379\n",
      ">> Epoch 95 finished \tRBM Reconstruction error 477.169128\n",
      ">> Epoch 96 finished \tRBM Reconstruction error 461.572815\n",
      ">> Epoch 97 finished \tRBM Reconstruction error 547.458740\n",
      ">> Epoch 98 finished \tRBM Reconstruction error 516.406616\n",
      ">> Epoch 99 finished \tRBM Reconstruction error 531.138611\n",
      ">> Epoch 100 finished \tRBM Reconstruction error 565.549011\n",
      ">> Epoch 101 finished \tRBM Reconstruction error 506.119995\n",
      ">> Epoch 102 finished \tRBM Reconstruction error 529.180786\n",
      ">> Epoch 103 finished \tRBM Reconstruction error 507.806732\n",
      ">> Epoch 104 finished \tRBM Reconstruction error 535.891724\n",
      ">> Epoch 105 finished \tRBM Reconstruction error 546.504639\n",
      ">> Epoch 106 finished \tRBM Reconstruction error 519.683899\n",
      ">> Epoch 107 finished \tRBM Reconstruction error 497.724762\n",
      ">> Epoch 108 finished \tRBM Reconstruction error 542.475952\n",
      ">> Epoch 109 finished \tRBM Reconstruction error 524.110779\n",
      ">> Epoch 110 finished \tRBM Reconstruction error 564.368958\n",
      ">> Epoch 111 finished \tRBM Reconstruction error 563.614319\n",
      ">> Epoch 112 finished \tRBM Reconstruction error 530.627991\n",
      ">> Epoch 113 finished \tRBM Reconstruction error 527.164246\n",
      ">> Epoch 114 finished \tRBM Reconstruction error 506.067383\n",
      ">> Epoch 115 finished \tRBM Reconstruction error 554.619568\n",
      ">> Epoch 116 finished \tRBM Reconstruction error 519.718445\n",
      ">> Epoch 117 finished \tRBM Reconstruction error 527.304138\n",
      ">> Epoch 118 finished \tRBM Reconstruction error 496.443604\n",
      ">> Epoch 119 finished \tRBM Reconstruction error 450.829529\n",
      ">> Epoch 120 finished \tRBM Reconstruction error 510.649536\n",
      ">> Epoch 121 finished \tRBM Reconstruction error 519.881897\n",
      ">> Epoch 122 finished \tRBM Reconstruction error 557.935730\n",
      ">> Epoch 123 finished \tRBM Reconstruction error 515.735229\n",
      ">> Epoch 124 finished \tRBM Reconstruction error 523.202332\n",
      ">> Epoch 125 finished \tRBM Reconstruction error 476.216431\n",
      ">> Epoch 126 finished \tRBM Reconstruction error 547.662842\n",
      ">> Epoch 127 finished \tRBM Reconstruction error 552.725403\n",
      ">> Epoch 128 finished \tRBM Reconstruction error 551.952759\n",
      ">> Epoch 129 finished \tRBM Reconstruction error 528.844238\n",
      ">> Epoch 130 finished \tRBM Reconstruction error 586.989441\n",
      ">> Epoch 131 finished \tRBM Reconstruction error 510.629852\n",
      ">> Epoch 132 finished \tRBM Reconstruction error 528.456238\n",
      ">> Epoch 133 finished \tRBM Reconstruction error 490.756409\n",
      ">> Epoch 134 finished \tRBM Reconstruction error 567.987183\n",
      ">> Epoch 135 finished \tRBM Reconstruction error 498.159485\n",
      ">> Epoch 136 finished \tRBM Reconstruction error 513.407654\n",
      ">> Epoch 137 finished \tRBM Reconstruction error 518.971130\n",
      ">> Epoch 138 finished \tRBM Reconstruction error 524.058350\n",
      ">> Epoch 139 finished \tRBM Reconstruction error 545.792908\n",
      ">> Epoch 140 finished \tRBM Reconstruction error 538.389465\n",
      ">> Epoch 141 finished \tRBM Reconstruction error 552.671021\n",
      ">> Epoch 142 finished \tRBM Reconstruction error 544.624817\n",
      ">> Epoch 143 finished \tRBM Reconstruction error 509.464600\n",
      ">> Epoch 144 finished \tRBM Reconstruction error 507.593658\n",
      ">> Epoch 145 finished \tRBM Reconstruction error 497.818726\n",
      ">> Epoch 146 finished \tRBM Reconstruction error 501.663208\n",
      ">> Epoch 147 finished \tRBM Reconstruction error 586.163696\n",
      ">> Epoch 148 finished \tRBM Reconstruction error 543.924316\n",
      ">> Epoch 149 finished \tRBM Reconstruction error 553.488342\n",
      ">> Epoch 150 finished \tRBM Reconstruction error 504.382294\n",
      ">> Epoch 151 finished \tRBM Reconstruction error 546.111511\n",
      ">> Epoch 152 finished \tRBM Reconstruction error 567.015747\n",
      ">> Epoch 153 finished \tRBM Reconstruction error 542.365906\n",
      ">> Epoch 154 finished \tRBM Reconstruction error 508.317352\n",
      ">> Epoch 155 finished \tRBM Reconstruction error 511.143188\n",
      ">> Epoch 156 finished \tRBM Reconstruction error 523.533569\n",
      ">> Epoch 157 finished \tRBM Reconstruction error 491.992706\n",
      ">> Epoch 158 finished \tRBM Reconstruction error 537.225586\n",
      ">> Epoch 159 finished \tRBM Reconstruction error 504.751099\n",
      ">> Epoch 160 finished \tRBM Reconstruction error 557.211670\n",
      ">> Epoch 161 finished \tRBM Reconstruction error 506.961212\n",
      ">> Epoch 162 finished \tRBM Reconstruction error 526.593994\n",
      ">> Epoch 163 finished \tRBM Reconstruction error 502.207642\n",
      ">> Epoch 164 finished \tRBM Reconstruction error 557.939697\n",
      ">> Epoch 165 finished \tRBM Reconstruction error 528.606934\n",
      ">> Epoch 166 finished \tRBM Reconstruction error 563.265442\n",
      ">> Epoch 167 finished \tRBM Reconstruction error 514.486084\n",
      ">> Epoch 168 finished \tRBM Reconstruction error 513.973389\n",
      ">> Epoch 169 finished \tRBM Reconstruction error 535.867432\n",
      ">> Epoch 170 finished \tRBM Reconstruction error 561.356384\n",
      ">> Epoch 171 finished \tRBM Reconstruction error 524.609131\n",
      ">> Epoch 172 finished \tRBM Reconstruction error 518.933838\n",
      ">> Epoch 173 finished \tRBM Reconstruction error 497.827515\n",
      ">> Epoch 174 finished \tRBM Reconstruction error 520.868530\n",
      ">> Epoch 175 finished \tRBM Reconstruction error 496.534607\n",
      ">> Epoch 176 finished \tRBM Reconstruction error 582.810486\n",
      ">> Epoch 177 finished \tRBM Reconstruction error 518.333435\n",
      ">> Epoch 178 finished \tRBM Reconstruction error 589.350952\n",
      ">> Epoch 179 finished \tRBM Reconstruction error 552.725891\n",
      ">> Epoch 180 finished \tRBM Reconstruction error 545.217407\n",
      ">> Epoch 181 finished \tRBM Reconstruction error 562.634277\n",
      ">> Epoch 182 finished \tRBM Reconstruction error 527.428833\n",
      ">> Epoch 183 finished \tRBM Reconstruction error 540.541870\n",
      ">> Epoch 184 finished \tRBM Reconstruction error 537.282837\n",
      ">> Epoch 185 finished \tRBM Reconstruction error 535.384827\n",
      ">> Epoch 186 finished \tRBM Reconstruction error 535.844543\n",
      ">> Epoch 187 finished \tRBM Reconstruction error 518.362183\n",
      ">> Epoch 188 finished \tRBM Reconstruction error 553.779236\n",
      ">> Epoch 189 finished \tRBM Reconstruction error 537.115417\n",
      ">> Epoch 190 finished \tRBM Reconstruction error 498.032074\n",
      ">> Epoch 191 finished \tRBM Reconstruction error 519.525574\n",
      ">> Epoch 192 finished \tRBM Reconstruction error 558.555908\n",
      ">> Epoch 193 finished \tRBM Reconstruction error 542.932129\n",
      ">> Epoch 194 finished \tRBM Reconstruction error 506.329071\n",
      ">> Epoch 195 finished \tRBM Reconstruction error 498.104340\n",
      ">> Epoch 196 finished \tRBM Reconstruction error 522.869629\n",
      ">> Epoch 197 finished \tRBM Reconstruction error 534.738159\n",
      ">> Epoch 198 finished \tRBM Reconstruction error 554.097839\n",
      ">> Epoch 199 finished \tRBM Reconstruction error 594.726013\n",
      ">> Epoch 200 finished \tRBM Reconstruction error 542.792053\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.560108\n",
      ">> Epoch 1 finished \tANN training loss 0.478133\n",
      ">> Epoch 2 finished \tANN training loss 0.326959\n",
      ">> Epoch 3 finished \tANN training loss 0.273707\n",
      ">> Epoch 4 finished \tANN training loss 0.233515\n",
      ">> Epoch 5 finished \tANN training loss 0.184579\n",
      ">> Epoch 6 finished \tANN training loss 0.173294\n",
      ">> Epoch 7 finished \tANN training loss 0.152690\n",
      ">> Epoch 8 finished \tANN training loss 0.131315\n",
      ">> Epoch 9 finished \tANN training loss 0.101903\n",
      ">> Epoch 10 finished \tANN training loss 0.100464\n",
      ">> Epoch 11 finished \tANN training loss 0.077701\n",
      ">> Epoch 12 finished \tANN training loss 0.074068\n",
      ">> Epoch 13 finished \tANN training loss 0.070626\n",
      ">> Epoch 14 finished \tANN training loss 0.062504\n",
      ">> Epoch 15 finished \tANN training loss 0.052151\n",
      ">> Epoch 16 finished \tANN training loss 0.050839\n",
      ">> Epoch 17 finished \tANN training loss 0.041720\n",
      ">> Epoch 18 finished \tANN training loss 0.045319\n",
      ">> Epoch 19 finished \tANN training loss 0.036140\n",
      ">> Epoch 20 finished \tANN training loss 0.030284\n",
      ">> Epoch 21 finished \tANN training loss 0.031474\n",
      ">> Epoch 22 finished \tANN training loss 0.024104\n",
      ">> Epoch 23 finished \tANN training loss 0.025288\n",
      ">> Epoch 24 finished \tANN training loss 0.020760\n",
      ">> Epoch 25 finished \tANN training loss 0.019948\n",
      ">> Epoch 26 finished \tANN training loss 0.019727\n",
      ">> Epoch 27 finished \tANN training loss 0.017002\n",
      ">> Epoch 28 finished \tANN training loss 0.017408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 29 finished \tANN training loss 0.014865\n",
      ">> Epoch 30 finished \tANN training loss 0.017195\n",
      ">> Epoch 31 finished \tANN training loss 0.012178\n",
      ">> Epoch 32 finished \tANN training loss 0.012078\n",
      ">> Epoch 33 finished \tANN training loss 0.011508\n",
      ">> Epoch 34 finished \tANN training loss 0.010311\n",
      ">> Epoch 35 finished \tANN training loss 0.010251\n",
      ">> Epoch 36 finished \tANN training loss 0.009114\n",
      ">> Epoch 37 finished \tANN training loss 0.008137\n",
      ">> Epoch 38 finished \tANN training loss 0.007561\n",
      ">> Epoch 39 finished \tANN training loss 0.009356\n",
      ">> Epoch 40 finished \tANN training loss 0.008041\n",
      ">> Epoch 41 finished \tANN training loss 0.006675\n",
      ">> Epoch 42 finished \tANN training loss 0.006967\n",
      ">> Epoch 43 finished \tANN training loss 0.006316\n",
      ">> Epoch 44 finished \tANN training loss 0.006178\n",
      ">> Epoch 45 finished \tANN training loss 0.005090\n",
      ">> Epoch 46 finished \tANN training loss 0.004839\n",
      ">> Epoch 47 finished \tANN training loss 0.005528\n",
      ">> Epoch 48 finished \tANN training loss 0.005558\n",
      ">> Epoch 49 finished \tANN training loss 0.004318\n",
      ">> Epoch 50 finished \tANN training loss 0.004103\n",
      ">> Epoch 51 finished \tANN training loss 0.003702\n",
      ">> Epoch 52 finished \tANN training loss 0.003993\n",
      ">> Epoch 53 finished \tANN training loss 0.003434\n",
      ">> Epoch 54 finished \tANN training loss 0.003578\n",
      ">> Epoch 55 finished \tANN training loss 0.002954\n",
      ">> Epoch 56 finished \tANN training loss 0.002660\n",
      ">> Epoch 57 finished \tANN training loss 0.003508\n",
      ">> Epoch 58 finished \tANN training loss 0.002743\n",
      ">> Epoch 59 finished \tANN training loss 0.002926\n",
      ">> Epoch 60 finished \tANN training loss 0.002511\n",
      ">> Epoch 61 finished \tANN training loss 0.002084\n",
      ">> Epoch 62 finished \tANN training loss 0.002145\n",
      ">> Epoch 63 finished \tANN training loss 0.002103\n",
      ">> Epoch 64 finished \tANN training loss 0.001781\n",
      ">> Epoch 65 finished \tANN training loss 0.001953\n",
      ">> Epoch 66 finished \tANN training loss 0.002772\n",
      ">> Epoch 67 finished \tANN training loss 0.001792\n",
      ">> Epoch 68 finished \tANN training loss 0.001873\n",
      ">> Epoch 69 finished \tANN training loss 0.001706\n",
      ">> Epoch 70 finished \tANN training loss 0.001847\n",
      ">> Epoch 71 finished \tANN training loss 0.002355\n",
      ">> Epoch 72 finished \tANN training loss 0.002011\n",
      ">> Epoch 73 finished \tANN training loss 0.001785\n",
      ">> Epoch 74 finished \tANN training loss 0.001674\n",
      ">> Epoch 75 finished \tANN training loss 0.001480\n",
      ">> Epoch 76 finished \tANN training loss 0.001611\n",
      ">> Epoch 77 finished \tANN training loss 0.001493\n",
      ">> Epoch 78 finished \tANN training loss 0.001313\n",
      ">> Epoch 79 finished \tANN training loss 0.001934\n",
      ">> Epoch 80 finished \tANN training loss 0.001204\n",
      ">> Epoch 81 finished \tANN training loss 0.001126\n",
      ">> Epoch 82 finished \tANN training loss 0.001206\n",
      ">> Epoch 83 finished \tANN training loss 0.001159\n",
      ">> Epoch 84 finished \tANN training loss 0.001021\n",
      ">> Epoch 85 finished \tANN training loss 0.000978\n",
      ">> Epoch 86 finished \tANN training loss 0.001200\n",
      ">> Epoch 87 finished \tANN training loss 0.001134\n",
      ">> Epoch 88 finished \tANN training loss 0.001004\n",
      ">> Epoch 89 finished \tANN training loss 0.001177\n",
      ">> Epoch 90 finished \tANN training loss 0.001069\n",
      ">> Epoch 91 finished \tANN training loss 0.000993\n",
      ">> Epoch 92 finished \tANN training loss 0.000942\n",
      ">> Epoch 93 finished \tANN training loss 0.000937\n",
      ">> Epoch 94 finished \tANN training loss 0.000810\n",
      ">> Epoch 95 finished \tANN training loss 0.000777\n",
      ">> Epoch 96 finished \tANN training loss 0.000862\n",
      ">> Epoch 97 finished \tANN training loss 0.000792\n",
      ">> Epoch 98 finished \tANN training loss 0.000853\n",
      ">> Epoch 99 finished \tANN training loss 0.000891\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.925000\n"
     ]
    }
   ],
   "source": [
    "epoch_rbm_acc[9] = deep_belief_net(n_epochs_rbm=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.925\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(epoch_rbm_acc[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88, 0.915, 0.93, 0.92, 0.925, 0.925, 0.93, 0.935, 0.935, 0.925]\n",
      "Most accurate n_epochs_rbm setting is Setting 8\n"
     ]
    }
   ],
   "source": [
    "# Collated results\n",
    "print(epoch_rbm_acc)\n",
    "print('Most accurate n_epochs_rbm setting is Setting ' + str(epoch_rbm_acc.index(max(epoch_rbm_acc)) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create bar graph\n",
    "plt.clf()\n",
    "objects = ('1', '2', '5', '10', '20', '30', '40', '50', '100', '200')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [0.88, 0.88, 0.91000000000000003, 0.91000000000000003, 0.90500000000000003, 0.92000000000000004, 0.93000000000000005, 0.90000000000000002, 0.90000000000000002, 0.89000000000000001]\n",
    "\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.8)\n",
    "plt.xticks(y_pos, objects, rotation=30)\n",
    "plt.xlabel('Settings')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('n_epochs_rbm Settings')\n",
    " \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Fine-tuning / Backprop Lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store accuracies\n",
    "backprop_acc = [0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 1\n",
    "\n",
    "n_iter_backprop=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 46.417393\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 47.657658\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 54.955650\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 50.019699\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 35.628067\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 27.058317\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 22.336702\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 29.815464\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 37.099140\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 33.422985\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 109.720528\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 73.426537\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 165.549896\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 118.949020\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 56.879375\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 83.812889\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 78.645920\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 108.058670\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 113.879547\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 198.149704\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.689387\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.785000\n"
     ]
    }
   ],
   "source": [
    "backprop_acc[0] = deep_belief_net(n_iter_backprop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.785\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(backprop_acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 2\n",
    "\n",
    "n_iter_backprop=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 53.670822\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 51.106125\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 46.766945\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 73.860001\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 37.919289\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 57.819492\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 42.827061\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 66.841301\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 44.626221\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 49.498726\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 266.658173\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 286.935852\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 368.736053\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 222.824295\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 194.785858\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 210.890808\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 237.214142\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 190.448593\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 182.679337\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 181.904037\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.709323\n",
      ">> Epoch 1 finished \tANN training loss 0.475414\n",
      ">> Epoch 2 finished \tANN training loss 0.398361\n",
      ">> Epoch 3 finished \tANN training loss 0.320243\n",
      ">> Epoch 4 finished \tANN training loss 0.291597\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.845000\n"
     ]
    }
   ],
   "source": [
    "backprop_acc[1] = deep_belief_net(n_iter_backprop=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.845\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(backprop_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 3\n",
    "\n",
    "n_iter_backprop=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 43.616425\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 55.513237\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 46.779030\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 35.020477\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 30.569477\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 44.275070\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 35.792793\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 54.732651\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 34.298092\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 44.391815\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 350.911316\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 219.627441\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 169.178024\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 270.033569\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 145.235703\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 95.522713\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 137.270813\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 148.698242\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 260.118256\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 147.416199\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.629362\n",
      ">> Epoch 1 finished \tANN training loss 0.433091\n",
      ">> Epoch 2 finished \tANN training loss 0.355336\n",
      ">> Epoch 3 finished \tANN training loss 0.311123\n",
      ">> Epoch 4 finished \tANN training loss 0.250547\n",
      ">> Epoch 5 finished \tANN training loss 0.231075\n",
      ">> Epoch 6 finished \tANN training loss 0.244113\n",
      ">> Epoch 7 finished \tANN training loss 0.179006\n",
      ">> Epoch 8 finished \tANN training loss 0.169945\n",
      ">> Epoch 9 finished \tANN training loss 0.154585\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.875000\n"
     ]
    }
   ],
   "source": [
    "backprop_acc[2] = deep_belief_net(n_iter_backprop=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.875\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(backprop_acc[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 4\n",
    "\n",
    "n_iter_backprop=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 64.079750\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 61.173603\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 69.858727\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 55.314083\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 42.462158\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 54.526485\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 49.569740\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 42.065937\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 32.949043\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 41.708828\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 179.793121\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 165.348633\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 203.255463\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 102.367752\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 188.277344\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 184.923706\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 214.414215\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 191.238678\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 122.805740\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 200.752228\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.709038\n",
      ">> Epoch 1 finished \tANN training loss 0.539233\n",
      ">> Epoch 2 finished \tANN training loss 0.377923\n",
      ">> Epoch 3 finished \tANN training loss 0.335729\n",
      ">> Epoch 4 finished \tANN training loss 0.297464\n",
      ">> Epoch 5 finished \tANN training loss 0.266764\n",
      ">> Epoch 6 finished \tANN training loss 0.236264\n",
      ">> Epoch 7 finished \tANN training loss 0.207258\n",
      ">> Epoch 8 finished \tANN training loss 0.182920\n",
      ">> Epoch 9 finished \tANN training loss 0.162755\n",
      ">> Epoch 10 finished \tANN training loss 0.137288\n",
      ">> Epoch 11 finished \tANN training loss 0.131667\n",
      ">> Epoch 12 finished \tANN training loss 0.115789\n",
      ">> Epoch 13 finished \tANN training loss 0.106285\n",
      ">> Epoch 14 finished \tANN training loss 0.095241\n",
      ">> Epoch 15 finished \tANN training loss 0.084004\n",
      ">> Epoch 16 finished \tANN training loss 0.074981\n",
      ">> Epoch 17 finished \tANN training loss 0.068201\n",
      ">> Epoch 18 finished \tANN training loss 0.060686\n",
      ">> Epoch 19 finished \tANN training loss 0.062969\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.900000\n"
     ]
    }
   ],
   "source": [
    "backprop_acc[3] = deep_belief_net(n_iter_backprop=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.9\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(backprop_acc[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 5\n",
    "\n",
    "n_iter_backprop=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 42.539101\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 48.427383\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 46.007954\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 27.606701\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 34.014862\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 49.891167\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 31.004084\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 25.910332\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 27.662582\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 49.573643\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 214.598724\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 176.054138\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 201.304138\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 234.982071\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 257.270844\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 155.380753\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 311.022003\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 268.813019\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 196.346130\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 188.891953\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.635767\n",
      ">> Epoch 1 finished \tANN training loss 0.441494\n",
      ">> Epoch 2 finished \tANN training loss 0.442132\n",
      ">> Epoch 3 finished \tANN training loss 0.311342\n",
      ">> Epoch 4 finished \tANN training loss 0.305937\n",
      ">> Epoch 5 finished \tANN training loss 0.286882\n",
      ">> Epoch 6 finished \tANN training loss 0.213888\n",
      ">> Epoch 7 finished \tANN training loss 0.206446\n",
      ">> Epoch 8 finished \tANN training loss 0.173293\n",
      ">> Epoch 9 finished \tANN training loss 0.161047\n",
      ">> Epoch 10 finished \tANN training loss 0.136715\n",
      ">> Epoch 11 finished \tANN training loss 0.123731\n",
      ">> Epoch 12 finished \tANN training loss 0.113966\n",
      ">> Epoch 13 finished \tANN training loss 0.108265\n",
      ">> Epoch 14 finished \tANN training loss 0.089739\n",
      ">> Epoch 15 finished \tANN training loss 0.083927\n",
      ">> Epoch 16 finished \tANN training loss 0.074046\n",
      ">> Epoch 17 finished \tANN training loss 0.070823\n",
      ">> Epoch 18 finished \tANN training loss 0.061464\n",
      ">> Epoch 19 finished \tANN training loss 0.052659\n",
      ">> Epoch 20 finished \tANN training loss 0.057696\n",
      ">> Epoch 21 finished \tANN training loss 0.047390\n",
      ">> Epoch 22 finished \tANN training loss 0.043418\n",
      ">> Epoch 23 finished \tANN training loss 0.039756\n",
      ">> Epoch 24 finished \tANN training loss 0.035646\n",
      ">> Epoch 25 finished \tANN training loss 0.033006\n",
      ">> Epoch 26 finished \tANN training loss 0.030432\n",
      ">> Epoch 27 finished \tANN training loss 0.030045\n",
      ">> Epoch 28 finished \tANN training loss 0.026954\n",
      ">> Epoch 29 finished \tANN training loss 0.024224\n",
      ">> Epoch 30 finished \tANN training loss 0.022229\n",
      ">> Epoch 31 finished \tANN training loss 0.021842\n",
      ">> Epoch 32 finished \tANN training loss 0.023601\n",
      ">> Epoch 33 finished \tANN training loss 0.018629\n",
      ">> Epoch 34 finished \tANN training loss 0.017043\n",
      ">> Epoch 35 finished \tANN training loss 0.017117\n",
      ">> Epoch 36 finished \tANN training loss 0.016426\n",
      ">> Epoch 37 finished \tANN training loss 0.014213\n",
      ">> Epoch 38 finished \tANN training loss 0.012463\n",
      ">> Epoch 39 finished \tANN training loss 0.011859\n",
      ">> Epoch 40 finished \tANN training loss 0.010856\n",
      ">> Epoch 41 finished \tANN training loss 0.010203\n",
      ">> Epoch 42 finished \tANN training loss 0.009449\n",
      ">> Epoch 43 finished \tANN training loss 0.014757\n",
      ">> Epoch 44 finished \tANN training loss 0.010546\n",
      ">> Epoch 45 finished \tANN training loss 0.008845\n",
      ">> Epoch 46 finished \tANN training loss 0.011153\n",
      ">> Epoch 47 finished \tANN training loss 0.007825\n",
      ">> Epoch 48 finished \tANN training loss 0.007644\n",
      ">> Epoch 49 finished \tANN training loss 0.006724\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.925000\n"
     ]
    }
   ],
   "source": [
    "backprop_acc[4] = deep_belief_net(n_iter_backprop=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.925\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(backprop_acc[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 6 (Default)\n",
    "\n",
    "n_iter_backprop=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "backprop_acc[5] = default_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.92\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(backprop_acc[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 7\n",
    "\n",
    "n_iter_backprop=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 78.270699\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 53.280502\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 71.744499\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 41.465714\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 43.127274\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 52.982025\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 42.504639\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 36.534718\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 47.537033\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 55.294544\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 342.604034\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 301.330322\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 243.681061\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 235.799332\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 191.945007\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 160.528473\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 320.610199\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 369.602631\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 226.601288\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 203.788986\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.647701\n",
      ">> Epoch 1 finished \tANN training loss 0.474274\n",
      ">> Epoch 2 finished \tANN training loss 0.382358\n",
      ">> Epoch 3 finished \tANN training loss 0.342266\n",
      ">> Epoch 4 finished \tANN training loss 0.303025\n",
      ">> Epoch 5 finished \tANN training loss 0.260843\n",
      ">> Epoch 6 finished \tANN training loss 0.240567\n",
      ">> Epoch 7 finished \tANN training loss 0.206317\n",
      ">> Epoch 8 finished \tANN training loss 0.180028\n",
      ">> Epoch 9 finished \tANN training loss 0.157604\n",
      ">> Epoch 10 finished \tANN training loss 0.148237\n",
      ">> Epoch 11 finished \tANN training loss 0.123130\n",
      ">> Epoch 12 finished \tANN training loss 0.114223\n",
      ">> Epoch 13 finished \tANN training loss 0.104016\n",
      ">> Epoch 14 finished \tANN training loss 0.101606\n",
      ">> Epoch 15 finished \tANN training loss 0.088622\n",
      ">> Epoch 16 finished \tANN training loss 0.080971\n",
      ">> Epoch 17 finished \tANN training loss 0.076944\n",
      ">> Epoch 18 finished \tANN training loss 0.065854\n",
      ">> Epoch 19 finished \tANN training loss 0.066379\n",
      ">> Epoch 20 finished \tANN training loss 0.055655\n",
      ">> Epoch 21 finished \tANN training loss 0.049862\n",
      ">> Epoch 22 finished \tANN training loss 0.049661\n",
      ">> Epoch 23 finished \tANN training loss 0.047930\n",
      ">> Epoch 24 finished \tANN training loss 0.042155\n",
      ">> Epoch 25 finished \tANN training loss 0.035827\n",
      ">> Epoch 26 finished \tANN training loss 0.036045\n",
      ">> Epoch 27 finished \tANN training loss 0.034883\n",
      ">> Epoch 28 finished \tANN training loss 0.030397\n",
      ">> Epoch 29 finished \tANN training loss 0.030755\n",
      ">> Epoch 30 finished \tANN training loss 0.028789\n",
      ">> Epoch 31 finished \tANN training loss 0.026859\n",
      ">> Epoch 32 finished \tANN training loss 0.022422\n",
      ">> Epoch 33 finished \tANN training loss 0.020953\n",
      ">> Epoch 34 finished \tANN training loss 0.020083\n",
      ">> Epoch 35 finished \tANN training loss 0.019830\n",
      ">> Epoch 36 finished \tANN training loss 0.017286\n",
      ">> Epoch 37 finished \tANN training loss 0.017568\n",
      ">> Epoch 38 finished \tANN training loss 0.019686\n",
      ">> Epoch 39 finished \tANN training loss 0.016919\n",
      ">> Epoch 40 finished \tANN training loss 0.013635\n",
      ">> Epoch 41 finished \tANN training loss 0.013763\n",
      ">> Epoch 42 finished \tANN training loss 0.013501\n",
      ">> Epoch 43 finished \tANN training loss 0.012666\n",
      ">> Epoch 44 finished \tANN training loss 0.011114\n",
      ">> Epoch 45 finished \tANN training loss 0.012617\n",
      ">> Epoch 46 finished \tANN training loss 0.010561\n",
      ">> Epoch 47 finished \tANN training loss 0.009131\n",
      ">> Epoch 48 finished \tANN training loss 0.012146\n",
      ">> Epoch 49 finished \tANN training loss 0.009666\n",
      ">> Epoch 50 finished \tANN training loss 0.008378\n",
      ">> Epoch 51 finished \tANN training loss 0.007912\n",
      ">> Epoch 52 finished \tANN training loss 0.006899\n",
      ">> Epoch 53 finished \tANN training loss 0.006320\n",
      ">> Epoch 54 finished \tANN training loss 0.006260\n",
      ">> Epoch 55 finished \tANN training loss 0.006451\n",
      ">> Epoch 56 finished \tANN training loss 0.005520\n",
      ">> Epoch 57 finished \tANN training loss 0.005164\n",
      ">> Epoch 58 finished \tANN training loss 0.005555\n",
      ">> Epoch 59 finished \tANN training loss 0.004828\n",
      ">> Epoch 60 finished \tANN training loss 0.004788\n",
      ">> Epoch 61 finished \tANN training loss 0.004589\n",
      ">> Epoch 62 finished \tANN training loss 0.004785\n",
      ">> Epoch 63 finished \tANN training loss 0.004704\n",
      ">> Epoch 64 finished \tANN training loss 0.004846\n",
      ">> Epoch 65 finished \tANN training loss 0.004720\n",
      ">> Epoch 66 finished \tANN training loss 0.004498\n",
      ">> Epoch 67 finished \tANN training loss 0.003754\n",
      ">> Epoch 68 finished \tANN training loss 0.003411\n",
      ">> Epoch 69 finished \tANN training loss 0.003275\n",
      ">> Epoch 70 finished \tANN training loss 0.003542\n",
      ">> Epoch 71 finished \tANN training loss 0.002864\n",
      ">> Epoch 72 finished \tANN training loss 0.003878\n",
      ">> Epoch 73 finished \tANN training loss 0.002614\n",
      ">> Epoch 74 finished \tANN training loss 0.002883\n",
      ">> Epoch 75 finished \tANN training loss 0.002585\n",
      ">> Epoch 76 finished \tANN training loss 0.002855\n",
      ">> Epoch 77 finished \tANN training loss 0.002507\n",
      ">> Epoch 78 finished \tANN training loss 0.002487\n",
      ">> Epoch 79 finished \tANN training loss 0.002455\n",
      ">> Epoch 80 finished \tANN training loss 0.002618\n",
      ">> Epoch 81 finished \tANN training loss 0.002405\n",
      ">> Epoch 82 finished \tANN training loss 0.001965\n",
      ">> Epoch 83 finished \tANN training loss 0.002077\n",
      ">> Epoch 84 finished \tANN training loss 0.001984\n",
      ">> Epoch 85 finished \tANN training loss 0.001927\n",
      ">> Epoch 86 finished \tANN training loss 0.001774\n",
      ">> Epoch 87 finished \tANN training loss 0.001888\n",
      ">> Epoch 88 finished \tANN training loss 0.001813\n",
      ">> Epoch 89 finished \tANN training loss 0.001794\n",
      ">> Epoch 90 finished \tANN training loss 0.001933\n",
      ">> Epoch 91 finished \tANN training loss 0.001811\n",
      ">> Epoch 92 finished \tANN training loss 0.001493\n",
      ">> Epoch 93 finished \tANN training loss 0.001527\n",
      ">> Epoch 94 finished \tANN training loss 0.001481\n",
      ">> Epoch 95 finished \tANN training loss 0.001377\n",
      ">> Epoch 96 finished \tANN training loss 0.001674\n",
      ">> Epoch 97 finished \tANN training loss 0.001384\n",
      ">> Epoch 98 finished \tANN training loss 0.001308\n",
      ">> Epoch 99 finished \tANN training loss 0.001703\n",
      ">> Epoch 100 finished \tANN training loss 0.001355\n",
      ">> Epoch 101 finished \tANN training loss 0.001244\n",
      ">> Epoch 102 finished \tANN training loss 0.001421\n",
      ">> Epoch 103 finished \tANN training loss 0.001432\n",
      ">> Epoch 104 finished \tANN training loss 0.001174\n",
      ">> Epoch 105 finished \tANN training loss 0.001632\n",
      ">> Epoch 106 finished \tANN training loss 0.001347\n",
      ">> Epoch 107 finished \tANN training loss 0.001332\n",
      ">> Epoch 108 finished \tANN training loss 0.001206\n",
      ">> Epoch 109 finished \tANN training loss 0.001091\n",
      ">> Epoch 110 finished \tANN training loss 0.001240\n",
      ">> Epoch 111 finished \tANN training loss 0.001130\n",
      ">> Epoch 112 finished \tANN training loss 0.001099\n",
      ">> Epoch 113 finished \tANN training loss 0.001117\n",
      ">> Epoch 114 finished \tANN training loss 0.001093\n",
      ">> Epoch 115 finished \tANN training loss 0.001331\n",
      ">> Epoch 116 finished \tANN training loss 0.000856\n",
      ">> Epoch 117 finished \tANN training loss 0.000903\n",
      ">> Epoch 118 finished \tANN training loss 0.001301\n",
      ">> Epoch 119 finished \tANN training loss 0.001150\n",
      ">> Epoch 120 finished \tANN training loss 0.000908\n",
      ">> Epoch 121 finished \tANN training loss 0.000961\n",
      ">> Epoch 122 finished \tANN training loss 0.000850\n",
      ">> Epoch 123 finished \tANN training loss 0.001034\n",
      ">> Epoch 124 finished \tANN training loss 0.000814\n",
      ">> Epoch 125 finished \tANN training loss 0.001139\n",
      ">> Epoch 126 finished \tANN training loss 0.000824\n",
      ">> Epoch 127 finished \tANN training loss 0.000810\n",
      ">> Epoch 128 finished \tANN training loss 0.000818\n",
      ">> Epoch 129 finished \tANN training loss 0.000765\n",
      ">> Epoch 130 finished \tANN training loss 0.000646\n",
      ">> Epoch 131 finished \tANN training loss 0.000748\n",
      ">> Epoch 132 finished \tANN training loss 0.000637\n",
      ">> Epoch 133 finished \tANN training loss 0.000638\n",
      ">> Epoch 134 finished \tANN training loss 0.000699\n",
      ">> Epoch 135 finished \tANN training loss 0.000729\n",
      ">> Epoch 136 finished \tANN training loss 0.000554\n",
      ">> Epoch 137 finished \tANN training loss 0.000635\n",
      ">> Epoch 138 finished \tANN training loss 0.000588\n",
      ">> Epoch 139 finished \tANN training loss 0.000609\n",
      ">> Epoch 140 finished \tANN training loss 0.000598\n",
      ">> Epoch 141 finished \tANN training loss 0.000492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 142 finished \tANN training loss 0.000573\n",
      ">> Epoch 143 finished \tANN training loss 0.000519\n",
      ">> Epoch 144 finished \tANN training loss 0.000606\n",
      ">> Epoch 145 finished \tANN training loss 0.000662\n",
      ">> Epoch 146 finished \tANN training loss 0.000643\n",
      ">> Epoch 147 finished \tANN training loss 0.000554\n",
      ">> Epoch 148 finished \tANN training loss 0.000508\n",
      ">> Epoch 149 finished \tANN training loss 0.000574\n",
      ">> Epoch 150 finished \tANN training loss 0.000665\n",
      ">> Epoch 151 finished \tANN training loss 0.000589\n",
      ">> Epoch 152 finished \tANN training loss 0.000508\n",
      ">> Epoch 153 finished \tANN training loss 0.000484\n",
      ">> Epoch 154 finished \tANN training loss 0.000443\n",
      ">> Epoch 155 finished \tANN training loss 0.000494\n",
      ">> Epoch 156 finished \tANN training loss 0.000444\n",
      ">> Epoch 157 finished \tANN training loss 0.000457\n",
      ">> Epoch 158 finished \tANN training loss 0.000502\n",
      ">> Epoch 159 finished \tANN training loss 0.000456\n",
      ">> Epoch 160 finished \tANN training loss 0.000422\n",
      ">> Epoch 161 finished \tANN training loss 0.000393\n",
      ">> Epoch 162 finished \tANN training loss 0.000417\n",
      ">> Epoch 163 finished \tANN training loss 0.000373\n",
      ">> Epoch 164 finished \tANN training loss 0.000381\n",
      ">> Epoch 165 finished \tANN training loss 0.000441\n",
      ">> Epoch 166 finished \tANN training loss 0.000435\n",
      ">> Epoch 167 finished \tANN training loss 0.000422\n",
      ">> Epoch 168 finished \tANN training loss 0.000352\n",
      ">> Epoch 169 finished \tANN training loss 0.000416\n",
      ">> Epoch 170 finished \tANN training loss 0.000395\n",
      ">> Epoch 171 finished \tANN training loss 0.000364\n",
      ">> Epoch 172 finished \tANN training loss 0.000348\n",
      ">> Epoch 173 finished \tANN training loss 0.000379\n",
      ">> Epoch 174 finished \tANN training loss 0.000343\n",
      ">> Epoch 175 finished \tANN training loss 0.000325\n",
      ">> Epoch 176 finished \tANN training loss 0.000304\n",
      ">> Epoch 177 finished \tANN training loss 0.000292\n",
      ">> Epoch 178 finished \tANN training loss 0.000342\n",
      ">> Epoch 179 finished \tANN training loss 0.000349\n",
      ">> Epoch 180 finished \tANN training loss 0.000306\n",
      ">> Epoch 181 finished \tANN training loss 0.000284\n",
      ">> Epoch 182 finished \tANN training loss 0.000346\n",
      ">> Epoch 183 finished \tANN training loss 0.000287\n",
      ">> Epoch 184 finished \tANN training loss 0.000263\n",
      ">> Epoch 185 finished \tANN training loss 0.000269\n",
      ">> Epoch 186 finished \tANN training loss 0.000255\n",
      ">> Epoch 187 finished \tANN training loss 0.000292\n",
      ">> Epoch 188 finished \tANN training loss 0.000291\n",
      ">> Epoch 189 finished \tANN training loss 0.000326\n",
      ">> Epoch 190 finished \tANN training loss 0.000299\n",
      ">> Epoch 191 finished \tANN training loss 0.000274\n",
      ">> Epoch 192 finished \tANN training loss 0.000255\n",
      ">> Epoch 193 finished \tANN training loss 0.000291\n",
      ">> Epoch 194 finished \tANN training loss 0.000251\n",
      ">> Epoch 195 finished \tANN training loss 0.000223\n",
      ">> Epoch 196 finished \tANN training loss 0.000218\n",
      ">> Epoch 197 finished \tANN training loss 0.000225\n",
      ">> Epoch 198 finished \tANN training loss 0.000302\n",
      ">> Epoch 199 finished \tANN training loss 0.000306\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.930000\n"
     ]
    }
   ],
   "source": [
    "backprop_acc[6] = deep_belief_net(n_iter_backprop=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.93\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(backprop_acc[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.785, 0.845, 0.875, 0.9, 0.925, 0.92, 0.93]\n",
      "Most accurate backprop iteration setting is Setting 7\n"
     ]
    }
   ],
   "source": [
    "# Collated results\n",
    "print(backprop_acc)\n",
    "print('Most accurate backprop iteration setting is Setting ' + str(backprop_acc.index(max(backprop_acc)) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create bar graph\n",
    "plt.clf()\n",
    "objects = ('1', '5', '10', '20', '50', '100', '200')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [0.78500000000000003, 0.84499999999999997, 0.87, 0.89000000000000001, 0.92000000000000004, 0.91000000000000003, 0.93000000000000005]\n",
    "\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.3)\n",
    "plt.xticks(y_pos, objects, rotation=30)\n",
    "plt.xlabel('Settings')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('n_iter_backprop Settings')\n",
    " \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark 1:\n",
    "Take the best setting for each n-layer setup and use the best performing parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 62.965160\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 56.377430\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 41.571564\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 49.783279\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 69.517647\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 45.104824\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 43.680145\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 37.531254\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 54.635376\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 53.856903\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 26.298256\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 60.240707\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 42.176445\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 45.950020\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 52.033173\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 42.436924\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 47.170143\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 51.320171\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 48.811520\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 29.796917\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 43.883419\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 37.661678\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 39.911346\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 55.439430\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 57.217518\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 35.121620\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 50.386818\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 58.663342\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 50.020996\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 50.962433\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 50.387238\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 33.647007\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 44.004288\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 63.671486\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 45.246593\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 54.966232\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 41.159706\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 46.985176\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 65.298767\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 55.111885\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.737916\n",
      ">> Epoch 1 finished \tANN training loss 0.519546\n",
      ">> Epoch 2 finished \tANN training loss 0.437401\n",
      ">> Epoch 3 finished \tANN training loss 0.357565\n",
      ">> Epoch 4 finished \tANN training loss 0.341053\n",
      ">> Epoch 5 finished \tANN training loss 0.272484\n",
      ">> Epoch 6 finished \tANN training loss 0.248520\n",
      ">> Epoch 7 finished \tANN training loss 0.234667\n",
      ">> Epoch 8 finished \tANN training loss 0.208356\n",
      ">> Epoch 9 finished \tANN training loss 0.190628\n",
      ">> Epoch 10 finished \tANN training loss 0.174290\n",
      ">> Epoch 11 finished \tANN training loss 0.162094\n",
      ">> Epoch 12 finished \tANN training loss 0.144602\n",
      ">> Epoch 13 finished \tANN training loss 0.130844\n",
      ">> Epoch 14 finished \tANN training loss 0.119703\n",
      ">> Epoch 15 finished \tANN training loss 0.116365\n",
      ">> Epoch 16 finished \tANN training loss 0.110318\n",
      ">> Epoch 17 finished \tANN training loss 0.102484\n",
      ">> Epoch 18 finished \tANN training loss 0.092412\n",
      ">> Epoch 19 finished \tANN training loss 0.083125\n",
      ">> Epoch 20 finished \tANN training loss 0.078500\n",
      ">> Epoch 21 finished \tANN training loss 0.071512\n",
      ">> Epoch 22 finished \tANN training loss 0.068592\n",
      ">> Epoch 23 finished \tANN training loss 0.062787\n",
      ">> Epoch 24 finished \tANN training loss 0.059403\n",
      ">> Epoch 25 finished \tANN training loss 0.054001\n",
      ">> Epoch 26 finished \tANN training loss 0.058337\n",
      ">> Epoch 27 finished \tANN training loss 0.050384\n",
      ">> Epoch 28 finished \tANN training loss 0.046054\n",
      ">> Epoch 29 finished \tANN training loss 0.041735\n",
      ">> Epoch 30 finished \tANN training loss 0.042602\n",
      ">> Epoch 31 finished \tANN training loss 0.038610\n",
      ">> Epoch 32 finished \tANN training loss 0.036508\n",
      ">> Epoch 33 finished \tANN training loss 0.034588\n",
      ">> Epoch 34 finished \tANN training loss 0.033740\n",
      ">> Epoch 35 finished \tANN training loss 0.031696\n",
      ">> Epoch 36 finished \tANN training loss 0.029101\n",
      ">> Epoch 37 finished \tANN training loss 0.026640\n",
      ">> Epoch 38 finished \tANN training loss 0.024914\n",
      ">> Epoch 39 finished \tANN training loss 0.025707\n",
      ">> Epoch 40 finished \tANN training loss 0.022630\n",
      ">> Epoch 41 finished \tANN training loss 0.023126\n",
      ">> Epoch 42 finished \tANN training loss 0.021629\n",
      ">> Epoch 43 finished \tANN training loss 0.020262\n",
      ">> Epoch 44 finished \tANN training loss 0.019355\n",
      ">> Epoch 45 finished \tANN training loss 0.018792\n",
      ">> Epoch 46 finished \tANN training loss 0.017383\n",
      ">> Epoch 47 finished \tANN training loss 0.017221\n",
      ">> Epoch 48 finished \tANN training loss 0.016227\n",
      ">> Epoch 49 finished \tANN training loss 0.015892\n",
      ">> Epoch 50 finished \tANN training loss 0.014825\n",
      ">> Epoch 51 finished \tANN training loss 0.014546\n",
      ">> Epoch 52 finished \tANN training loss 0.014850\n",
      ">> Epoch 53 finished \tANN training loss 0.013662\n",
      ">> Epoch 54 finished \tANN training loss 0.013524\n",
      ">> Epoch 55 finished \tANN training loss 0.012895\n",
      ">> Epoch 56 finished \tANN training loss 0.011968\n",
      ">> Epoch 57 finished \tANN training loss 0.011810\n",
      ">> Epoch 58 finished \tANN training loss 0.011306\n",
      ">> Epoch 59 finished \tANN training loss 0.011003\n",
      ">> Epoch 60 finished \tANN training loss 0.010592\n",
      ">> Epoch 61 finished \tANN training loss 0.010766\n",
      ">> Epoch 62 finished \tANN training loss 0.009290\n",
      ">> Epoch 63 finished \tANN training loss 0.009143\n",
      ">> Epoch 64 finished \tANN training loss 0.009304\n",
      ">> Epoch 65 finished \tANN training loss 0.008643\n",
      ">> Epoch 66 finished \tANN training loss 0.008357\n",
      ">> Epoch 67 finished \tANN training loss 0.008837\n",
      ">> Epoch 68 finished \tANN training loss 0.007608\n",
      ">> Epoch 69 finished \tANN training loss 0.007456\n",
      ">> Epoch 70 finished \tANN training loss 0.007268\n",
      ">> Epoch 71 finished \tANN training loss 0.007091\n",
      ">> Epoch 72 finished \tANN training loss 0.007159\n",
      ">> Epoch 73 finished \tANN training loss 0.006757\n",
      ">> Epoch 74 finished \tANN training loss 0.006431\n",
      ">> Epoch 75 finished \tANN training loss 0.006475\n",
      ">> Epoch 76 finished \tANN training loss 0.006236\n",
      ">> Epoch 77 finished \tANN training loss 0.006127\n",
      ">> Epoch 78 finished \tANN training loss 0.005718\n",
      ">> Epoch 79 finished \tANN training loss 0.005478\n",
      ">> Epoch 80 finished \tANN training loss 0.005542\n",
      ">> Epoch 81 finished \tANN training loss 0.005624\n",
      ">> Epoch 82 finished \tANN training loss 0.005346\n",
      ">> Epoch 83 finished \tANN training loss 0.005258\n",
      ">> Epoch 84 finished \tANN training loss 0.005086\n",
      ">> Epoch 85 finished \tANN training loss 0.004860\n",
      ">> Epoch 86 finished \tANN training loss 0.004812\n",
      ">> Epoch 87 finished \tANN training loss 0.004779\n",
      ">> Epoch 88 finished \tANN training loss 0.004760\n",
      ">> Epoch 89 finished \tANN training loss 0.004473\n",
      ">> Epoch 90 finished \tANN training loss 0.004319\n",
      ">> Epoch 91 finished \tANN training loss 0.004439\n",
      ">> Epoch 92 finished \tANN training loss 0.004202\n",
      ">> Epoch 93 finished \tANN training loss 0.004094\n",
      ">> Epoch 94 finished \tANN training loss 0.004137\n",
      ">> Epoch 95 finished \tANN training loss 0.003769\n",
      ">> Epoch 96 finished \tANN training loss 0.003795\n",
      ">> Epoch 97 finished \tANN training loss 0.003814\n",
      ">> Epoch 98 finished \tANN training loss 0.003826\n",
      ">> Epoch 99 finished \tANN training loss 0.003565\n",
      ">> Epoch 100 finished \tANN training loss 0.003546\n",
      ">> Epoch 101 finished \tANN training loss 0.003441\n",
      ">> Epoch 102 finished \tANN training loss 0.003406\n",
      ">> Epoch 103 finished \tANN training loss 0.003359\n",
      ">> Epoch 104 finished \tANN training loss 0.003242\n",
      ">> Epoch 105 finished \tANN training loss 0.003265\n",
      ">> Epoch 106 finished \tANN training loss 0.003538\n",
      ">> Epoch 107 finished \tANN training loss 0.003255\n",
      ">> Epoch 108 finished \tANN training loss 0.003223\n",
      ">> Epoch 109 finished \tANN training loss 0.003036\n",
      ">> Epoch 110 finished \tANN training loss 0.002898\n",
      ">> Epoch 111 finished \tANN training loss 0.003045\n",
      ">> Epoch 112 finished \tANN training loss 0.002911\n",
      ">> Epoch 113 finished \tANN training loss 0.002831\n",
      ">> Epoch 114 finished \tANN training loss 0.002840\n",
      ">> Epoch 115 finished \tANN training loss 0.002838\n",
      ">> Epoch 116 finished \tANN training loss 0.002851\n",
      ">> Epoch 117 finished \tANN training loss 0.002694\n",
      ">> Epoch 118 finished \tANN training loss 0.002705\n",
      ">> Epoch 119 finished \tANN training loss 0.002732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 120 finished \tANN training loss 0.002688\n",
      ">> Epoch 121 finished \tANN training loss 0.002687\n",
      ">> Epoch 122 finished \tANN training loss 0.002592\n",
      ">> Epoch 123 finished \tANN training loss 0.002535\n",
      ">> Epoch 124 finished \tANN training loss 0.002575\n",
      ">> Epoch 125 finished \tANN training loss 0.002574\n",
      ">> Epoch 126 finished \tANN training loss 0.002543\n",
      ">> Epoch 127 finished \tANN training loss 0.002506\n",
      ">> Epoch 128 finished \tANN training loss 0.002366\n",
      ">> Epoch 129 finished \tANN training loss 0.002322\n",
      ">> Epoch 130 finished \tANN training loss 0.002239\n",
      ">> Epoch 131 finished \tANN training loss 0.002181\n",
      ">> Epoch 132 finished \tANN training loss 0.002062\n",
      ">> Epoch 133 finished \tANN training loss 0.002070\n",
      ">> Epoch 134 finished \tANN training loss 0.001994\n",
      ">> Epoch 135 finished \tANN training loss 0.001988\n",
      ">> Epoch 136 finished \tANN training loss 0.001986\n",
      ">> Epoch 137 finished \tANN training loss 0.001932\n",
      ">> Epoch 138 finished \tANN training loss 0.001977\n",
      ">> Epoch 139 finished \tANN training loss 0.001995\n",
      ">> Epoch 140 finished \tANN training loss 0.001877\n",
      ">> Epoch 141 finished \tANN training loss 0.001825\n",
      ">> Epoch 142 finished \tANN training loss 0.001758\n",
      ">> Epoch 143 finished \tANN training loss 0.001719\n",
      ">> Epoch 144 finished \tANN training loss 0.001699\n",
      ">> Epoch 145 finished \tANN training loss 0.001816\n",
      ">> Epoch 146 finished \tANN training loss 0.001809\n",
      ">> Epoch 147 finished \tANN training loss 0.001751\n",
      ">> Epoch 148 finished \tANN training loss 0.001694\n",
      ">> Epoch 149 finished \tANN training loss 0.001765\n",
      ">> Epoch 150 finished \tANN training loss 0.001773\n",
      ">> Epoch 151 finished \tANN training loss 0.001638\n",
      ">> Epoch 152 finished \tANN training loss 0.001578\n",
      ">> Epoch 153 finished \tANN training loss 0.001464\n",
      ">> Epoch 154 finished \tANN training loss 0.001500\n",
      ">> Epoch 155 finished \tANN training loss 0.001479\n",
      ">> Epoch 156 finished \tANN training loss 0.001545\n",
      ">> Epoch 157 finished \tANN training loss 0.001538\n",
      ">> Epoch 158 finished \tANN training loss 0.001472\n",
      ">> Epoch 159 finished \tANN training loss 0.001415\n",
      ">> Epoch 160 finished \tANN training loss 0.001439\n",
      ">> Epoch 161 finished \tANN training loss 0.001398\n",
      ">> Epoch 162 finished \tANN training loss 0.001352\n",
      ">> Epoch 163 finished \tANN training loss 0.001374\n",
      ">> Epoch 164 finished \tANN training loss 0.001348\n",
      ">> Epoch 165 finished \tANN training loss 0.001315\n",
      ">> Epoch 166 finished \tANN training loss 0.001245\n",
      ">> Epoch 167 finished \tANN training loss 0.001349\n",
      ">> Epoch 168 finished \tANN training loss 0.001273\n",
      ">> Epoch 169 finished \tANN training loss 0.001269\n",
      ">> Epoch 170 finished \tANN training loss 0.001264\n",
      ">> Epoch 171 finished \tANN training loss 0.001236\n",
      ">> Epoch 172 finished \tANN training loss 0.001213\n",
      ">> Epoch 173 finished \tANN training loss 0.001180\n",
      ">> Epoch 174 finished \tANN training loss 0.001178\n",
      ">> Epoch 175 finished \tANN training loss 0.001168\n",
      ">> Epoch 176 finished \tANN training loss 0.001131\n",
      ">> Epoch 177 finished \tANN training loss 0.001278\n",
      ">> Epoch 178 finished \tANN training loss 0.001182\n",
      ">> Epoch 179 finished \tANN training loss 0.001241\n",
      ">> Epoch 180 finished \tANN training loss 0.001137\n",
      ">> Epoch 181 finished \tANN training loss 0.001138\n",
      ">> Epoch 182 finished \tANN training loss 0.001084\n",
      ">> Epoch 183 finished \tANN training loss 0.001062\n",
      ">> Epoch 184 finished \tANN training loss 0.001165\n",
      ">> Epoch 185 finished \tANN training loss 0.001086\n",
      ">> Epoch 186 finished \tANN training loss 0.001056\n",
      ">> Epoch 187 finished \tANN training loss 0.001265\n",
      ">> Epoch 188 finished \tANN training loss 0.001136\n",
      ">> Epoch 189 finished \tANN training loss 0.001021\n",
      ">> Epoch 190 finished \tANN training loss 0.001006\n",
      ">> Epoch 191 finished \tANN training loss 0.000993\n",
      ">> Epoch 192 finished \tANN training loss 0.001082\n",
      ">> Epoch 193 finished \tANN training loss 0.001087\n",
      ">> Epoch 194 finished \tANN training loss 0.001013\n",
      ">> Epoch 195 finished \tANN training loss 0.000969\n",
      ">> Epoch 196 finished \tANN training loss 0.000955\n",
      ">> Epoch 197 finished \tANN training loss 0.000914\n",
      ">> Epoch 198 finished \tANN training loss 0.000934\n",
      ">> Epoch 199 finished \tANN training loss 0.000903\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.925000\n",
      "ACCURACY: 0.925\n"
     ]
    }
   ],
   "source": [
    "# 1-layer\n",
    "acc1 = deep_belief_net(hidden_layers_structure=[200], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.2)\n",
    "print('ACCURACY: ' + str(acc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 59.534481\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 85.250023\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 54.699650\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 80.817772\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 80.662491\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 43.503086\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 55.378395\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 43.733780\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 57.075821\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 72.376610\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 63.547684\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 50.875511\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 55.329201\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 63.671581\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 44.878490\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 53.757824\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 46.167168\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 72.337395\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 69.765205\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 46.501549\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 64.065041\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 65.597305\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 62.805294\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 60.807491\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 45.779171\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 66.202316\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 57.785458\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 62.540997\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 61.863602\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 53.859718\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 68.695358\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 64.742332\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 53.195869\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 80.075905\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 73.219269\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 66.706787\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 68.994415\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 62.665997\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 56.164677\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 79.310760\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.763332\n",
      ">> Epoch 1 finished \tANN training loss 0.525281\n",
      ">> Epoch 2 finished \tANN training loss 0.435748\n",
      ">> Epoch 3 finished \tANN training loss 0.375584\n",
      ">> Epoch 4 finished \tANN training loss 0.345087\n",
      ">> Epoch 5 finished \tANN training loss 0.300132\n",
      ">> Epoch 6 finished \tANN training loss 0.264026\n",
      ">> Epoch 7 finished \tANN training loss 0.241888\n",
      ">> Epoch 8 finished \tANN training loss 0.229965\n",
      ">> Epoch 9 finished \tANN training loss 0.209940\n",
      ">> Epoch 10 finished \tANN training loss 0.188133\n",
      ">> Epoch 11 finished \tANN training loss 0.172643\n",
      ">> Epoch 12 finished \tANN training loss 0.173558\n",
      ">> Epoch 13 finished \tANN training loss 0.148977\n",
      ">> Epoch 14 finished \tANN training loss 0.139720\n",
      ">> Epoch 15 finished \tANN training loss 0.131389\n",
      ">> Epoch 16 finished \tANN training loss 0.125240\n",
      ">> Epoch 17 finished \tANN training loss 0.112581\n",
      ">> Epoch 18 finished \tANN training loss 0.101163\n",
      ">> Epoch 19 finished \tANN training loss 0.094257\n",
      ">> Epoch 20 finished \tANN training loss 0.089537\n",
      ">> Epoch 21 finished \tANN training loss 0.079658\n",
      ">> Epoch 22 finished \tANN training loss 0.076691\n",
      ">> Epoch 23 finished \tANN training loss 0.071429\n",
      ">> Epoch 24 finished \tANN training loss 0.065621\n",
      ">> Epoch 25 finished \tANN training loss 0.060549\n",
      ">> Epoch 26 finished \tANN training loss 0.058332\n",
      ">> Epoch 27 finished \tANN training loss 0.059031\n",
      ">> Epoch 28 finished \tANN training loss 0.051245\n",
      ">> Epoch 29 finished \tANN training loss 0.050937\n",
      ">> Epoch 30 finished \tANN training loss 0.048095\n",
      ">> Epoch 31 finished \tANN training loss 0.041593\n",
      ">> Epoch 32 finished \tANN training loss 0.041362\n",
      ">> Epoch 33 finished \tANN training loss 0.037133\n",
      ">> Epoch 34 finished \tANN training loss 0.038192\n",
      ">> Epoch 35 finished \tANN training loss 0.035898\n",
      ">> Epoch 36 finished \tANN training loss 0.033459\n",
      ">> Epoch 37 finished \tANN training loss 0.030747\n",
      ">> Epoch 38 finished \tANN training loss 0.028680\n",
      ">> Epoch 39 finished \tANN training loss 0.028686\n",
      ">> Epoch 40 finished \tANN training loss 0.025504\n",
      ">> Epoch 41 finished \tANN training loss 0.025162\n",
      ">> Epoch 42 finished \tANN training loss 0.023140\n",
      ">> Epoch 43 finished \tANN training loss 0.023849\n",
      ">> Epoch 44 finished \tANN training loss 0.022095\n",
      ">> Epoch 45 finished \tANN training loss 0.021024\n",
      ">> Epoch 46 finished \tANN training loss 0.022923\n",
      ">> Epoch 47 finished \tANN training loss 0.018587\n",
      ">> Epoch 48 finished \tANN training loss 0.018969\n",
      ">> Epoch 49 finished \tANN training loss 0.017099\n",
      ">> Epoch 50 finished \tANN training loss 0.016660\n",
      ">> Epoch 51 finished \tANN training loss 0.016029\n",
      ">> Epoch 52 finished \tANN training loss 0.015447\n",
      ">> Epoch 53 finished \tANN training loss 0.014795\n",
      ">> Epoch 54 finished \tANN training loss 0.014225\n",
      ">> Epoch 55 finished \tANN training loss 0.013291\n",
      ">> Epoch 56 finished \tANN training loss 0.013169\n",
      ">> Epoch 57 finished \tANN training loss 0.013591\n",
      ">> Epoch 58 finished \tANN training loss 0.012515\n",
      ">> Epoch 59 finished \tANN training loss 0.012180\n",
      ">> Epoch 60 finished \tANN training loss 0.012045\n",
      ">> Epoch 61 finished \tANN training loss 0.011952\n",
      ">> Epoch 62 finished \tANN training loss 0.011128\n",
      ">> Epoch 63 finished \tANN training loss 0.011507\n",
      ">> Epoch 64 finished \tANN training loss 0.010470\n",
      ">> Epoch 65 finished \tANN training loss 0.010507\n",
      ">> Epoch 66 finished \tANN training loss 0.009612\n",
      ">> Epoch 67 finished \tANN training loss 0.009305\n",
      ">> Epoch 68 finished \tANN training loss 0.009109\n",
      ">> Epoch 69 finished \tANN training loss 0.009139\n",
      ">> Epoch 70 finished \tANN training loss 0.008798\n",
      ">> Epoch 71 finished \tANN training loss 0.008314\n",
      ">> Epoch 72 finished \tANN training loss 0.008170\n",
      ">> Epoch 73 finished \tANN training loss 0.007774\n",
      ">> Epoch 74 finished \tANN training loss 0.007651\n",
      ">> Epoch 75 finished \tANN training loss 0.007475\n",
      ">> Epoch 76 finished \tANN training loss 0.007846\n",
      ">> Epoch 77 finished \tANN training loss 0.007287\n",
      ">> Epoch 78 finished \tANN training loss 0.006672\n",
      ">> Epoch 79 finished \tANN training loss 0.006672\n",
      ">> Epoch 80 finished \tANN training loss 0.006548\n",
      ">> Epoch 81 finished \tANN training loss 0.006479\n",
      ">> Epoch 82 finished \tANN training loss 0.006301\n",
      ">> Epoch 83 finished \tANN training loss 0.006088\n",
      ">> Epoch 84 finished \tANN training loss 0.006256\n",
      ">> Epoch 85 finished \tANN training loss 0.005582\n",
      ">> Epoch 86 finished \tANN training loss 0.005451\n",
      ">> Epoch 87 finished \tANN training loss 0.005278\n",
      ">> Epoch 88 finished \tANN training loss 0.005468\n",
      ">> Epoch 89 finished \tANN training loss 0.005305\n",
      ">> Epoch 90 finished \tANN training loss 0.004851\n",
      ">> Epoch 91 finished \tANN training loss 0.004831\n",
      ">> Epoch 92 finished \tANN training loss 0.004898\n",
      ">> Epoch 93 finished \tANN training loss 0.004903\n",
      ">> Epoch 94 finished \tANN training loss 0.004671\n",
      ">> Epoch 95 finished \tANN training loss 0.004405\n",
      ">> Epoch 96 finished \tANN training loss 0.004505\n",
      ">> Epoch 97 finished \tANN training loss 0.004372\n",
      ">> Epoch 98 finished \tANN training loss 0.004399\n",
      ">> Epoch 99 finished \tANN training loss 0.004075\n",
      ">> Epoch 100 finished \tANN training loss 0.004117\n",
      ">> Epoch 101 finished \tANN training loss 0.003964\n",
      ">> Epoch 102 finished \tANN training loss 0.003997\n",
      ">> Epoch 103 finished \tANN training loss 0.003944\n",
      ">> Epoch 104 finished \tANN training loss 0.003803\n",
      ">> Epoch 105 finished \tANN training loss 0.003541\n",
      ">> Epoch 106 finished \tANN training loss 0.003883\n",
      ">> Epoch 107 finished \tANN training loss 0.003565\n",
      ">> Epoch 108 finished \tANN training loss 0.003408\n",
      ">> Epoch 109 finished \tANN training loss 0.003632\n",
      ">> Epoch 110 finished \tANN training loss 0.003324\n",
      ">> Epoch 111 finished \tANN training loss 0.003244\n",
      ">> Epoch 112 finished \tANN training loss 0.003390\n",
      ">> Epoch 113 finished \tANN training loss 0.003247\n",
      ">> Epoch 114 finished \tANN training loss 0.003244\n",
      ">> Epoch 115 finished \tANN training loss 0.003483\n",
      ">> Epoch 116 finished \tANN training loss 0.003206\n",
      ">> Epoch 117 finished \tANN training loss 0.003095\n",
      ">> Epoch 118 finished \tANN training loss 0.003063\n",
      ">> Epoch 119 finished \tANN training loss 0.002977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 120 finished \tANN training loss 0.002961\n",
      ">> Epoch 121 finished \tANN training loss 0.003031\n",
      ">> Epoch 122 finished \tANN training loss 0.002868\n",
      ">> Epoch 123 finished \tANN training loss 0.002750\n",
      ">> Epoch 124 finished \tANN training loss 0.002707\n",
      ">> Epoch 125 finished \tANN training loss 0.002577\n",
      ">> Epoch 126 finished \tANN training loss 0.002704\n",
      ">> Epoch 127 finished \tANN training loss 0.002563\n",
      ">> Epoch 128 finished \tANN training loss 0.002466\n",
      ">> Epoch 129 finished \tANN training loss 0.002592\n",
      ">> Epoch 130 finished \tANN training loss 0.002325\n",
      ">> Epoch 131 finished \tANN training loss 0.002457\n",
      ">> Epoch 132 finished \tANN training loss 0.002363\n",
      ">> Epoch 133 finished \tANN training loss 0.002357\n",
      ">> Epoch 134 finished \tANN training loss 0.002243\n",
      ">> Epoch 135 finished \tANN training loss 0.002257\n",
      ">> Epoch 136 finished \tANN training loss 0.002157\n",
      ">> Epoch 137 finished \tANN training loss 0.002104\n",
      ">> Epoch 138 finished \tANN training loss 0.002062\n",
      ">> Epoch 139 finished \tANN training loss 0.002001\n",
      ">> Epoch 140 finished \tANN training loss 0.001991\n",
      ">> Epoch 141 finished \tANN training loss 0.002012\n",
      ">> Epoch 142 finished \tANN training loss 0.002100\n",
      ">> Epoch 143 finished \tANN training loss 0.002046\n",
      ">> Epoch 144 finished \tANN training loss 0.002020\n",
      ">> Epoch 145 finished \tANN training loss 0.002004\n",
      ">> Epoch 146 finished \tANN training loss 0.001956\n",
      ">> Epoch 147 finished \tANN training loss 0.001970\n",
      ">> Epoch 148 finished \tANN training loss 0.001885\n",
      ">> Epoch 149 finished \tANN training loss 0.001848\n",
      ">> Epoch 150 finished \tANN training loss 0.001829\n",
      ">> Epoch 151 finished \tANN training loss 0.001806\n",
      ">> Epoch 152 finished \tANN training loss 0.001783\n",
      ">> Epoch 153 finished \tANN training loss 0.001804\n",
      ">> Epoch 154 finished \tANN training loss 0.001730\n",
      ">> Epoch 155 finished \tANN training loss 0.001682\n",
      ">> Epoch 156 finished \tANN training loss 0.001677\n",
      ">> Epoch 157 finished \tANN training loss 0.001678\n",
      ">> Epoch 158 finished \tANN training loss 0.001599\n",
      ">> Epoch 159 finished \tANN training loss 0.001556\n",
      ">> Epoch 160 finished \tANN training loss 0.001517\n",
      ">> Epoch 161 finished \tANN training loss 0.001528\n",
      ">> Epoch 162 finished \tANN training loss 0.001502\n",
      ">> Epoch 163 finished \tANN training loss 0.001528\n",
      ">> Epoch 164 finished \tANN training loss 0.001457\n",
      ">> Epoch 165 finished \tANN training loss 0.001438\n",
      ">> Epoch 166 finished \tANN training loss 0.001478\n",
      ">> Epoch 167 finished \tANN training loss 0.001591\n",
      ">> Epoch 168 finished \tANN training loss 0.001414\n",
      ">> Epoch 169 finished \tANN training loss 0.001399\n",
      ">> Epoch 170 finished \tANN training loss 0.001342\n",
      ">> Epoch 171 finished \tANN training loss 0.001292\n",
      ">> Epoch 172 finished \tANN training loss 0.001315\n",
      ">> Epoch 173 finished \tANN training loss 0.001254\n",
      ">> Epoch 174 finished \tANN training loss 0.001339\n",
      ">> Epoch 175 finished \tANN training loss 0.001294\n",
      ">> Epoch 176 finished \tANN training loss 0.001250\n",
      ">> Epoch 177 finished \tANN training loss 0.001319\n",
      ">> Epoch 178 finished \tANN training loss 0.001277\n",
      ">> Epoch 179 finished \tANN training loss 0.001322\n",
      ">> Epoch 180 finished \tANN training loss 0.001242\n",
      ">> Epoch 181 finished \tANN training loss 0.001423\n",
      ">> Epoch 182 finished \tANN training loss 0.001335\n",
      ">> Epoch 183 finished \tANN training loss 0.001194\n",
      ">> Epoch 184 finished \tANN training loss 0.001182\n",
      ">> Epoch 185 finished \tANN training loss 0.001166\n",
      ">> Epoch 186 finished \tANN training loss 0.001147\n",
      ">> Epoch 187 finished \tANN training loss 0.001116\n",
      ">> Epoch 188 finished \tANN training loss 0.001222\n",
      ">> Epoch 189 finished \tANN training loss 0.001184\n",
      ">> Epoch 190 finished \tANN training loss 0.001219\n",
      ">> Epoch 191 finished \tANN training loss 0.001239\n",
      ">> Epoch 192 finished \tANN training loss 0.001267\n",
      ">> Epoch 193 finished \tANN training loss 0.001086\n",
      ">> Epoch 194 finished \tANN training loss 0.001169\n",
      ">> Epoch 195 finished \tANN training loss 0.001106\n",
      ">> Epoch 196 finished \tANN training loss 0.001059\n",
      ">> Epoch 197 finished \tANN training loss 0.001040\n",
      ">> Epoch 198 finished \tANN training loss 0.001076\n",
      ">> Epoch 199 finished \tANN training loss 0.001118\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.925000\n",
      "ACCURACY: 0.925\n"
     ]
    }
   ],
   "source": [
    "# 1-layer (2nd one tied)\n",
    "acc2 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.2)\n",
    "print('ACCURACY: ' + str(acc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 72.365013\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 59.222641\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 47.150780\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 62.283836\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 70.769768\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 88.854065\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 105.223846\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 81.825027\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 99.801979\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 105.395134\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 106.450813\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 120.503769\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 142.858063\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 136.069275\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 138.071564\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 163.779938\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 137.830490\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 142.365677\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 163.303696\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 149.320694\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 148.039291\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 184.989883\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 176.896439\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 169.834915\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 194.992401\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 165.127625\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 233.850433\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 191.004135\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 185.259842\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 195.047379\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 209.788315\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 183.105759\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 202.009186\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 210.700470\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 196.206360\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 238.192230\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 209.537338\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 223.371460\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 224.429733\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 209.946213\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 543.722107\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 945.417053\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 1408.460938\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 869.075012\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 1830.790161\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 1394.925049\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 1042.788940\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 1294.259399\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 1514.017456\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 1582.412842\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 1672.300049\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 1427.102173\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 1698.848877\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 1872.556152\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 1707.910278\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 1949.795288\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 1834.680664\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 1822.474487\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 1820.377197\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 1463.044678\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 1730.371216\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 1857.521851\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 1860.320312\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 2130.277832\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 2084.341553\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 1890.499268\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 2016.219482\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 1806.375610\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 2028.965942\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 2061.619873\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 1668.918945\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 1845.810059\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 1987.931274\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 1939.995728\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 1850.166992\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 1673.720581\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 1684.961060\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 1742.832520\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 1804.985352\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 1880.997803\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.629258\n",
      ">> Epoch 1 finished \tANN training loss 0.429317\n",
      ">> Epoch 2 finished \tANN training loss 0.373101\n",
      ">> Epoch 3 finished \tANN training loss 0.331970\n",
      ">> Epoch 4 finished \tANN training loss 0.256877\n",
      ">> Epoch 5 finished \tANN training loss 0.228462\n",
      ">> Epoch 6 finished \tANN training loss 0.214413\n",
      ">> Epoch 7 finished \tANN training loss 0.187619\n",
      ">> Epoch 8 finished \tANN training loss 0.166027\n",
      ">> Epoch 9 finished \tANN training loss 0.156356\n",
      ">> Epoch 10 finished \tANN training loss 0.128994\n",
      ">> Epoch 11 finished \tANN training loss 0.121721\n",
      ">> Epoch 12 finished \tANN training loss 0.112294\n",
      ">> Epoch 13 finished \tANN training loss 0.097649\n",
      ">> Epoch 14 finished \tANN training loss 0.108019\n",
      ">> Epoch 15 finished \tANN training loss 0.090055\n",
      ">> Epoch 16 finished \tANN training loss 0.083347\n",
      ">> Epoch 17 finished \tANN training loss 0.073408\n",
      ">> Epoch 18 finished \tANN training loss 0.057931\n",
      ">> Epoch 19 finished \tANN training loss 0.055644\n",
      ">> Epoch 20 finished \tANN training loss 0.050590\n",
      ">> Epoch 21 finished \tANN training loss 0.045046\n",
      ">> Epoch 22 finished \tANN training loss 0.048840\n",
      ">> Epoch 23 finished \tANN training loss 0.046229\n",
      ">> Epoch 24 finished \tANN training loss 0.036579\n",
      ">> Epoch 25 finished \tANN training loss 0.035739\n",
      ">> Epoch 26 finished \tANN training loss 0.030570\n",
      ">> Epoch 27 finished \tANN training loss 0.030835\n",
      ">> Epoch 28 finished \tANN training loss 0.026816\n",
      ">> Epoch 29 finished \tANN training loss 0.022123\n",
      ">> Epoch 30 finished \tANN training loss 0.022224\n",
      ">> Epoch 31 finished \tANN training loss 0.020581\n",
      ">> Epoch 32 finished \tANN training loss 0.017104\n",
      ">> Epoch 33 finished \tANN training loss 0.018226\n",
      ">> Epoch 34 finished \tANN training loss 0.020100\n",
      ">> Epoch 35 finished \tANN training loss 0.015253\n",
      ">> Epoch 36 finished \tANN training loss 0.015878\n",
      ">> Epoch 37 finished \tANN training loss 0.017658\n",
      ">> Epoch 38 finished \tANN training loss 0.014966\n",
      ">> Epoch 39 finished \tANN training loss 0.013493\n",
      ">> Epoch 40 finished \tANN training loss 0.012097\n",
      ">> Epoch 41 finished \tANN training loss 0.011389\n",
      ">> Epoch 42 finished \tANN training loss 0.014139\n",
      ">> Epoch 43 finished \tANN training loss 0.014045\n",
      ">> Epoch 44 finished \tANN training loss 0.009428\n",
      ">> Epoch 45 finished \tANN training loss 0.010329\n",
      ">> Epoch 46 finished \tANN training loss 0.008312\n",
      ">> Epoch 47 finished \tANN training loss 0.008388\n",
      ">> Epoch 48 finished \tANN training loss 0.007799\n",
      ">> Epoch 49 finished \tANN training loss 0.008398\n",
      ">> Epoch 50 finished \tANN training loss 0.007820\n",
      ">> Epoch 51 finished \tANN training loss 0.007457\n",
      ">> Epoch 52 finished \tANN training loss 0.007676\n",
      ">> Epoch 53 finished \tANN training loss 0.007810\n",
      ">> Epoch 54 finished \tANN training loss 0.006855\n",
      ">> Epoch 55 finished \tANN training loss 0.006284\n",
      ">> Epoch 56 finished \tANN training loss 0.005421\n",
      ">> Epoch 57 finished \tANN training loss 0.004774\n",
      ">> Epoch 58 finished \tANN training loss 0.005584\n",
      ">> Epoch 59 finished \tANN training loss 0.004808\n",
      ">> Epoch 60 finished \tANN training loss 0.004793\n",
      ">> Epoch 61 finished \tANN training loss 0.004343\n",
      ">> Epoch 62 finished \tANN training loss 0.003867\n",
      ">> Epoch 63 finished \tANN training loss 0.003697\n",
      ">> Epoch 64 finished \tANN training loss 0.003253\n",
      ">> Epoch 65 finished \tANN training loss 0.003236\n",
      ">> Epoch 66 finished \tANN training loss 0.003513\n",
      ">> Epoch 67 finished \tANN training loss 0.004076\n",
      ">> Epoch 68 finished \tANN training loss 0.003507\n",
      ">> Epoch 69 finished \tANN training loss 0.003666\n",
      ">> Epoch 70 finished \tANN training loss 0.003498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 71 finished \tANN training loss 0.003197\n",
      ">> Epoch 72 finished \tANN training loss 0.002908\n",
      ">> Epoch 73 finished \tANN training loss 0.004324\n",
      ">> Epoch 74 finished \tANN training loss 0.003147\n",
      ">> Epoch 75 finished \tANN training loss 0.002689\n",
      ">> Epoch 76 finished \tANN training loss 0.002451\n",
      ">> Epoch 77 finished \tANN training loss 0.002586\n",
      ">> Epoch 78 finished \tANN training loss 0.002539\n",
      ">> Epoch 79 finished \tANN training loss 0.002470\n",
      ">> Epoch 80 finished \tANN training loss 0.002465\n",
      ">> Epoch 81 finished \tANN training loss 0.002282\n",
      ">> Epoch 82 finished \tANN training loss 0.002167\n",
      ">> Epoch 83 finished \tANN training loss 0.001901\n",
      ">> Epoch 84 finished \tANN training loss 0.001833\n",
      ">> Epoch 85 finished \tANN training loss 0.002186\n",
      ">> Epoch 86 finished \tANN training loss 0.002160\n",
      ">> Epoch 87 finished \tANN training loss 0.001880\n",
      ">> Epoch 88 finished \tANN training loss 0.002421\n",
      ">> Epoch 89 finished \tANN training loss 0.002259\n",
      ">> Epoch 90 finished \tANN training loss 0.001771\n",
      ">> Epoch 91 finished \tANN training loss 0.001740\n",
      ">> Epoch 92 finished \tANN training loss 0.001633\n",
      ">> Epoch 93 finished \tANN training loss 0.001626\n",
      ">> Epoch 94 finished \tANN training loss 0.001579\n",
      ">> Epoch 95 finished \tANN training loss 0.001497\n",
      ">> Epoch 96 finished \tANN training loss 0.001836\n",
      ">> Epoch 97 finished \tANN training loss 0.001736\n",
      ">> Epoch 98 finished \tANN training loss 0.001330\n",
      ">> Epoch 99 finished \tANN training loss 0.001416\n",
      ">> Epoch 100 finished \tANN training loss 0.001555\n",
      ">> Epoch 101 finished \tANN training loss 0.001364\n",
      ">> Epoch 102 finished \tANN training loss 0.001353\n",
      ">> Epoch 103 finished \tANN training loss 0.001653\n",
      ">> Epoch 104 finished \tANN training loss 0.001402\n",
      ">> Epoch 105 finished \tANN training loss 0.001267\n",
      ">> Epoch 106 finished \tANN training loss 0.001332\n",
      ">> Epoch 107 finished \tANN training loss 0.001148\n",
      ">> Epoch 108 finished \tANN training loss 0.001158\n",
      ">> Epoch 109 finished \tANN training loss 0.001060\n",
      ">> Epoch 110 finished \tANN training loss 0.001089\n",
      ">> Epoch 111 finished \tANN training loss 0.001210\n",
      ">> Epoch 112 finished \tANN training loss 0.001148\n",
      ">> Epoch 113 finished \tANN training loss 0.001055\n",
      ">> Epoch 114 finished \tANN training loss 0.001126\n",
      ">> Epoch 115 finished \tANN training loss 0.001171\n",
      ">> Epoch 116 finished \tANN training loss 0.000989\n",
      ">> Epoch 117 finished \tANN training loss 0.000891\n",
      ">> Epoch 118 finished \tANN training loss 0.000807\n",
      ">> Epoch 119 finished \tANN training loss 0.000935\n",
      ">> Epoch 120 finished \tANN training loss 0.001082\n",
      ">> Epoch 121 finished \tANN training loss 0.001025\n",
      ">> Epoch 122 finished \tANN training loss 0.001047\n",
      ">> Epoch 123 finished \tANN training loss 0.000896\n",
      ">> Epoch 124 finished \tANN training loss 0.000831\n",
      ">> Epoch 125 finished \tANN training loss 0.000701\n",
      ">> Epoch 126 finished \tANN training loss 0.000712\n",
      ">> Epoch 127 finished \tANN training loss 0.000609\n",
      ">> Epoch 128 finished \tANN training loss 0.000599\n",
      ">> Epoch 129 finished \tANN training loss 0.000632\n",
      ">> Epoch 130 finished \tANN training loss 0.000606\n",
      ">> Epoch 131 finished \tANN training loss 0.000753\n",
      ">> Epoch 132 finished \tANN training loss 0.000767\n",
      ">> Epoch 133 finished \tANN training loss 0.000644\n",
      ">> Epoch 134 finished \tANN training loss 0.000725\n",
      ">> Epoch 135 finished \tANN training loss 0.000890\n",
      ">> Epoch 136 finished \tANN training loss 0.000667\n",
      ">> Epoch 137 finished \tANN training loss 0.000659\n",
      ">> Epoch 138 finished \tANN training loss 0.000660\n",
      ">> Epoch 139 finished \tANN training loss 0.000583\n",
      ">> Epoch 140 finished \tANN training loss 0.000650\n",
      ">> Epoch 141 finished \tANN training loss 0.000640\n",
      ">> Epoch 142 finished \tANN training loss 0.000774\n",
      ">> Epoch 143 finished \tANN training loss 0.000590\n",
      ">> Epoch 144 finished \tANN training loss 0.000728\n",
      ">> Epoch 145 finished \tANN training loss 0.000583\n",
      ">> Epoch 146 finished \tANN training loss 0.000546\n",
      ">> Epoch 147 finished \tANN training loss 0.000508\n",
      ">> Epoch 148 finished \tANN training loss 0.000740\n",
      ">> Epoch 149 finished \tANN training loss 0.000626\n",
      ">> Epoch 150 finished \tANN training loss 0.000530\n",
      ">> Epoch 151 finished \tANN training loss 0.000709\n",
      ">> Epoch 152 finished \tANN training loss 0.000679\n",
      ">> Epoch 153 finished \tANN training loss 0.000624\n",
      ">> Epoch 154 finished \tANN training loss 0.000562\n",
      ">> Epoch 155 finished \tANN training loss 0.000548\n",
      ">> Epoch 156 finished \tANN training loss 0.000413\n",
      ">> Epoch 157 finished \tANN training loss 0.000440\n",
      ">> Epoch 158 finished \tANN training loss 0.000397\n",
      ">> Epoch 159 finished \tANN training loss 0.000483\n",
      ">> Epoch 160 finished \tANN training loss 0.000650\n",
      ">> Epoch 161 finished \tANN training loss 0.000528\n",
      ">> Epoch 162 finished \tANN training loss 0.000450\n",
      ">> Epoch 163 finished \tANN training loss 0.000446\n",
      ">> Epoch 164 finished \tANN training loss 0.000472\n",
      ">> Epoch 165 finished \tANN training loss 0.000417\n",
      ">> Epoch 166 finished \tANN training loss 0.000424\n",
      ">> Epoch 167 finished \tANN training loss 0.000383\n",
      ">> Epoch 168 finished \tANN training loss 0.000440\n",
      ">> Epoch 169 finished \tANN training loss 0.000372\n",
      ">> Epoch 170 finished \tANN training loss 0.000369\n",
      ">> Epoch 171 finished \tANN training loss 0.000340\n",
      ">> Epoch 172 finished \tANN training loss 0.000459\n",
      ">> Epoch 173 finished \tANN training loss 0.000389\n",
      ">> Epoch 174 finished \tANN training loss 0.000318\n",
      ">> Epoch 175 finished \tANN training loss 0.000348\n",
      ">> Epoch 176 finished \tANN training loss 0.000397\n",
      ">> Epoch 177 finished \tANN training loss 0.000457\n",
      ">> Epoch 178 finished \tANN training loss 0.000349\n",
      ">> Epoch 179 finished \tANN training loss 0.000331\n",
      ">> Epoch 180 finished \tANN training loss 0.000295\n",
      ">> Epoch 181 finished \tANN training loss 0.000296\n",
      ">> Epoch 182 finished \tANN training loss 0.000256\n",
      ">> Epoch 183 finished \tANN training loss 0.000309\n",
      ">> Epoch 184 finished \tANN training loss 0.000256\n",
      ">> Epoch 185 finished \tANN training loss 0.000314\n",
      ">> Epoch 186 finished \tANN training loss 0.000301\n",
      ">> Epoch 187 finished \tANN training loss 0.000290\n",
      ">> Epoch 188 finished \tANN training loss 0.000301\n",
      ">> Epoch 189 finished \tANN training loss 0.000274\n",
      ">> Epoch 190 finished \tANN training loss 0.000272\n",
      ">> Epoch 191 finished \tANN training loss 0.000251\n",
      ">> Epoch 192 finished \tANN training loss 0.000276\n",
      ">> Epoch 193 finished \tANN training loss 0.000258\n",
      ">> Epoch 194 finished \tANN training loss 0.000236\n",
      ">> Epoch 195 finished \tANN training loss 0.000221\n",
      ">> Epoch 196 finished \tANN training loss 0.000230\n",
      ">> Epoch 197 finished \tANN training loss 0.000191\n",
      ">> Epoch 198 finished \tANN training loss 0.000245\n",
      ">> Epoch 199 finished \tANN training loss 0.000186\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.930000\n",
      "ACCURACY: 0.93\n"
     ]
    }
   ],
   "source": [
    "# 2-layer\n",
    "acc3 = deep_belief_net(hidden_layers_structure=[100, 500], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.2)\n",
    "print('ACCURACY: ' + str(acc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 49.976906\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 39.193859\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 60.738590\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 40.068138\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 36.381416\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 27.695738\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 31.692266\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 22.926769\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 26.704662\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 50.590729\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 43.781010\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 44.857765\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 27.561945\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 41.463779\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 26.169941\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 37.501453\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 29.253149\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 33.052502\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 32.312393\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 31.389067\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 21.296227\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 26.187267\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 39.849171\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 29.634150\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 40.812279\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 42.959599\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 37.425438\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 30.140884\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 33.903286\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 34.914486\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 26.608259\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 37.091728\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 39.818306\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 35.382458\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 38.537567\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 38.315800\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 33.588486\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 35.622738\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 48.679142\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 35.314220\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 180.476089\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 225.567886\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 176.648941\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 180.464844\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 264.905151\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 196.203964\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 277.745758\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 271.667419\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 338.135193\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 307.140900\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 297.623383\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 277.545502\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 390.069855\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 337.505005\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 350.964569\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 329.049530\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 360.897736\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 334.683167\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 379.955933\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 422.093048\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 406.426361\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 390.894775\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 415.112335\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 404.142120\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 474.451324\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 381.996643\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 425.734283\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 412.326019\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 437.668915\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 437.440704\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 444.260712\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 491.722565\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 417.637238\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 461.670319\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 483.363373\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 484.862549\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 491.169739\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 495.012146\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 423.387970\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 428.286041\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 2934.960938\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 5030.222656\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 6104.488281\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 8854.126953\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 8653.106445\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 11379.825195\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 12570.495117\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 13759.152344\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 14209.438477\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 19051.427734\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 17329.632812\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 20735.753906\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 21200.845703\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 21008.546875\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 25367.699219\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 24598.609375\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 25335.789062\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 27463.523438\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 27770.546875\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 26145.570312\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 27050.550781\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 26424.250000\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 28078.681641\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 27537.992188\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 27005.591797\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 30757.904297\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 26916.005859\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 29909.357422\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 28604.019531\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 31249.949219\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 31314.730469\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 30333.994141\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 29864.095703\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 32253.339844\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 31815.750000\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 31666.410156\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 32227.132812\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 32283.505859\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 34032.593750\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 32990.699219\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.911362\n",
      ">> Epoch 1 finished \tANN training loss 0.604457\n",
      ">> Epoch 2 finished \tANN training loss 0.474070\n",
      ">> Epoch 3 finished \tANN training loss 0.381709\n",
      ">> Epoch 4 finished \tANN training loss 0.319585\n",
      ">> Epoch 5 finished \tANN training loss 0.259750\n",
      ">> Epoch 6 finished \tANN training loss 0.199558\n",
      ">> Epoch 7 finished \tANN training loss 0.184357\n",
      ">> Epoch 8 finished \tANN training loss 0.173245\n",
      ">> Epoch 9 finished \tANN training loss 0.129617\n",
      ">> Epoch 10 finished \tANN training loss 0.109312\n",
      ">> Epoch 11 finished \tANN training loss 0.108086\n",
      ">> Epoch 12 finished \tANN training loss 0.092397\n",
      ">> Epoch 13 finished \tANN training loss 0.084253\n",
      ">> Epoch 14 finished \tANN training loss 0.071196\n",
      ">> Epoch 15 finished \tANN training loss 0.060142\n",
      ">> Epoch 16 finished \tANN training loss 0.042267\n",
      ">> Epoch 17 finished \tANN training loss 0.040446\n",
      ">> Epoch 18 finished \tANN training loss 0.039330\n",
      ">> Epoch 19 finished \tANN training loss 0.032213\n",
      ">> Epoch 20 finished \tANN training loss 0.034231\n",
      ">> Epoch 21 finished \tANN training loss 0.028234\n",
      ">> Epoch 22 finished \tANN training loss 0.023979\n",
      ">> Epoch 23 finished \tANN training loss 0.019085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 24 finished \tANN training loss 0.015687\n",
      ">> Epoch 25 finished \tANN training loss 0.015189\n",
      ">> Epoch 26 finished \tANN training loss 0.012369\n",
      ">> Epoch 27 finished \tANN training loss 0.014277\n",
      ">> Epoch 28 finished \tANN training loss 0.016163\n",
      ">> Epoch 29 finished \tANN training loss 0.011858\n",
      ">> Epoch 30 finished \tANN training loss 0.008881\n",
      ">> Epoch 31 finished \tANN training loss 0.009524\n",
      ">> Epoch 32 finished \tANN training loss 0.006928\n",
      ">> Epoch 33 finished \tANN training loss 0.006045\n",
      ">> Epoch 34 finished \tANN training loss 0.007251\n",
      ">> Epoch 35 finished \tANN training loss 0.009590\n",
      ">> Epoch 36 finished \tANN training loss 0.010445\n",
      ">> Epoch 37 finished \tANN training loss 0.005224\n",
      ">> Epoch 38 finished \tANN training loss 0.006138\n",
      ">> Epoch 39 finished \tANN training loss 0.007295\n",
      ">> Epoch 40 finished \tANN training loss 0.004161\n",
      ">> Epoch 41 finished \tANN training loss 0.004550\n",
      ">> Epoch 42 finished \tANN training loss 0.003801\n",
      ">> Epoch 43 finished \tANN training loss 0.003151\n",
      ">> Epoch 44 finished \tANN training loss 0.002783\n",
      ">> Epoch 45 finished \tANN training loss 0.001878\n",
      ">> Epoch 46 finished \tANN training loss 0.002939\n",
      ">> Epoch 47 finished \tANN training loss 0.001946\n",
      ">> Epoch 48 finished \tANN training loss 0.001881\n",
      ">> Epoch 49 finished \tANN training loss 0.003158\n",
      ">> Epoch 50 finished \tANN training loss 0.001818\n",
      ">> Epoch 51 finished \tANN training loss 0.002380\n",
      ">> Epoch 52 finished \tANN training loss 0.001708\n",
      ">> Epoch 53 finished \tANN training loss 0.002065\n",
      ">> Epoch 54 finished \tANN training loss 0.001669\n",
      ">> Epoch 55 finished \tANN training loss 0.001336\n",
      ">> Epoch 56 finished \tANN training loss 0.001100\n",
      ">> Epoch 57 finished \tANN training loss 0.001639\n",
      ">> Epoch 58 finished \tANN training loss 0.001091\n",
      ">> Epoch 59 finished \tANN training loss 0.002623\n",
      ">> Epoch 60 finished \tANN training loss 0.001415\n",
      ">> Epoch 61 finished \tANN training loss 0.001250\n",
      ">> Epoch 62 finished \tANN training loss 0.003194\n",
      ">> Epoch 63 finished \tANN training loss 0.001029\n",
      ">> Epoch 64 finished \tANN training loss 0.001206\n",
      ">> Epoch 65 finished \tANN training loss 0.001571\n",
      ">> Epoch 66 finished \tANN training loss 0.000823\n",
      ">> Epoch 67 finished \tANN training loss 0.001452\n",
      ">> Epoch 68 finished \tANN training loss 0.000900\n",
      ">> Epoch 69 finished \tANN training loss 0.001003\n",
      ">> Epoch 70 finished \tANN training loss 0.000786\n",
      ">> Epoch 71 finished \tANN training loss 0.000559\n",
      ">> Epoch 72 finished \tANN training loss 0.000766\n",
      ">> Epoch 73 finished \tANN training loss 0.000535\n",
      ">> Epoch 74 finished \tANN training loss 0.001324\n",
      ">> Epoch 75 finished \tANN training loss 0.000568\n",
      ">> Epoch 76 finished \tANN training loss 0.000446\n",
      ">> Epoch 77 finished \tANN training loss 0.000498\n",
      ">> Epoch 78 finished \tANN training loss 0.000413\n",
      ">> Epoch 79 finished \tANN training loss 0.000584\n",
      ">> Epoch 80 finished \tANN training loss 0.000383\n",
      ">> Epoch 81 finished \tANN training loss 0.000410\n",
      ">> Epoch 82 finished \tANN training loss 0.000377\n",
      ">> Epoch 83 finished \tANN training loss 0.000473\n",
      ">> Epoch 84 finished \tANN training loss 0.000485\n",
      ">> Epoch 85 finished \tANN training loss 0.000427\n",
      ">> Epoch 86 finished \tANN training loss 0.000452\n",
      ">> Epoch 87 finished \tANN training loss 0.000637\n",
      ">> Epoch 88 finished \tANN training loss 0.000297\n",
      ">> Epoch 89 finished \tANN training loss 0.000217\n",
      ">> Epoch 90 finished \tANN training loss 0.000573\n",
      ">> Epoch 91 finished \tANN training loss 0.000367\n",
      ">> Epoch 92 finished \tANN training loss 0.000492\n",
      ">> Epoch 93 finished \tANN training loss 0.000587\n",
      ">> Epoch 94 finished \tANN training loss 0.000256\n",
      ">> Epoch 95 finished \tANN training loss 0.000250\n",
      ">> Epoch 96 finished \tANN training loss 0.000207\n",
      ">> Epoch 97 finished \tANN training loss 0.000202\n",
      ">> Epoch 98 finished \tANN training loss 0.000232\n",
      ">> Epoch 99 finished \tANN training loss 0.000192\n",
      ">> Epoch 100 finished \tANN training loss 0.000242\n",
      ">> Epoch 101 finished \tANN training loss 0.000321\n",
      ">> Epoch 102 finished \tANN training loss 0.000187\n",
      ">> Epoch 103 finished \tANN training loss 0.000168\n",
      ">> Epoch 104 finished \tANN training loss 0.000418\n",
      ">> Epoch 105 finished \tANN training loss 0.000531\n",
      ">> Epoch 106 finished \tANN training loss 0.000366\n",
      ">> Epoch 107 finished \tANN training loss 0.000313\n",
      ">> Epoch 108 finished \tANN training loss 0.000286\n",
      ">> Epoch 109 finished \tANN training loss 0.000196\n",
      ">> Epoch 110 finished \tANN training loss 0.000224\n",
      ">> Epoch 111 finished \tANN training loss 0.000352\n",
      ">> Epoch 112 finished \tANN training loss 0.000328\n",
      ">> Epoch 113 finished \tANN training loss 0.000191\n",
      ">> Epoch 114 finished \tANN training loss 0.000180\n",
      ">> Epoch 115 finished \tANN training loss 0.000125\n",
      ">> Epoch 116 finished \tANN training loss 0.000097\n",
      ">> Epoch 117 finished \tANN training loss 0.000133\n",
      ">> Epoch 118 finished \tANN training loss 0.000189\n",
      ">> Epoch 119 finished \tANN training loss 0.000162\n",
      ">> Epoch 120 finished \tANN training loss 0.000135\n",
      ">> Epoch 121 finished \tANN training loss 0.000114\n",
      ">> Epoch 122 finished \tANN training loss 0.000105\n",
      ">> Epoch 123 finished \tANN training loss 0.000098\n",
      ">> Epoch 124 finished \tANN training loss 0.000085\n",
      ">> Epoch 125 finished \tANN training loss 0.000197\n",
      ">> Epoch 126 finished \tANN training loss 0.000093\n",
      ">> Epoch 127 finished \tANN training loss 0.000074\n",
      ">> Epoch 128 finished \tANN training loss 0.000100\n",
      ">> Epoch 129 finished \tANN training loss 0.001002\n",
      ">> Epoch 130 finished \tANN training loss 0.000132\n",
      ">> Epoch 131 finished \tANN training loss 0.000183\n",
      ">> Epoch 132 finished \tANN training loss 0.000185\n",
      ">> Epoch 133 finished \tANN training loss 0.000232\n",
      ">> Epoch 134 finished \tANN training loss 0.000079\n",
      ">> Epoch 135 finished \tANN training loss 0.000088\n",
      ">> Epoch 136 finished \tANN training loss 0.000097\n",
      ">> Epoch 137 finished \tANN training loss 0.000116\n",
      ">> Epoch 138 finished \tANN training loss 0.000078\n",
      ">> Epoch 139 finished \tANN training loss 0.000074\n",
      ">> Epoch 140 finished \tANN training loss 0.000153\n",
      ">> Epoch 141 finished \tANN training loss 0.000051\n",
      ">> Epoch 142 finished \tANN training loss 0.000045\n",
      ">> Epoch 143 finished \tANN training loss 0.000048\n",
      ">> Epoch 144 finished \tANN training loss 0.000051\n",
      ">> Epoch 145 finished \tANN training loss 0.000080\n",
      ">> Epoch 146 finished \tANN training loss 0.000079\n",
      ">> Epoch 147 finished \tANN training loss 0.000043\n",
      ">> Epoch 148 finished \tANN training loss 0.000034\n",
      ">> Epoch 149 finished \tANN training loss 0.000042\n",
      ">> Epoch 150 finished \tANN training loss 0.000042\n",
      ">> Epoch 151 finished \tANN training loss 0.000136\n",
      ">> Epoch 152 finished \tANN training loss 0.000193\n",
      ">> Epoch 153 finished \tANN training loss 0.000091\n",
      ">> Epoch 154 finished \tANN training loss 0.000147\n",
      ">> Epoch 155 finished \tANN training loss 0.000091\n",
      ">> Epoch 156 finished \tANN training loss 0.000239\n",
      ">> Epoch 157 finished \tANN training loss 0.000163\n",
      ">> Epoch 158 finished \tANN training loss 0.000071\n",
      ">> Epoch 159 finished \tANN training loss 0.000062\n",
      ">> Epoch 160 finished \tANN training loss 0.000055\n",
      ">> Epoch 161 finished \tANN training loss 0.000058\n",
      ">> Epoch 162 finished \tANN training loss 0.000099\n",
      ">> Epoch 163 finished \tANN training loss 0.000056\n",
      ">> Epoch 164 finished \tANN training loss 0.000047\n",
      ">> Epoch 165 finished \tANN training loss 0.000042\n",
      ">> Epoch 166 finished \tANN training loss 0.000055\n",
      ">> Epoch 167 finished \tANN training loss 0.000059\n",
      ">> Epoch 168 finished \tANN training loss 0.000044\n",
      ">> Epoch 169 finished \tANN training loss 0.000068\n",
      ">> Epoch 170 finished \tANN training loss 0.000049\n",
      ">> Epoch 171 finished \tANN training loss 0.000059\n",
      ">> Epoch 172 finished \tANN training loss 0.000048\n",
      ">> Epoch 173 finished \tANN training loss 0.000059\n",
      ">> Epoch 174 finished \tANN training loss 0.000065\n",
      ">> Epoch 175 finished \tANN training loss 0.000062\n",
      ">> Epoch 176 finished \tANN training loss 0.000048\n",
      ">> Epoch 177 finished \tANN training loss 0.000059\n",
      ">> Epoch 178 finished \tANN training loss 0.000102\n",
      ">> Epoch 179 finished \tANN training loss 0.000066\n",
      ">> Epoch 180 finished \tANN training loss 0.000049\n",
      ">> Epoch 181 finished \tANN training loss 0.000048\n",
      ">> Epoch 182 finished \tANN training loss 0.000054\n",
      ">> Epoch 183 finished \tANN training loss 0.000033\n",
      ">> Epoch 184 finished \tANN training loss 0.000050\n",
      ">> Epoch 185 finished \tANN training loss 0.000054\n",
      ">> Epoch 186 finished \tANN training loss 0.000053\n",
      ">> Epoch 187 finished \tANN training loss 0.000047\n",
      ">> Epoch 188 finished \tANN training loss 0.000031\n",
      ">> Epoch 189 finished \tANN training loss 0.000041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 190 finished \tANN training loss 0.000074\n",
      ">> Epoch 191 finished \tANN training loss 0.000069\n",
      ">> Epoch 192 finished \tANN training loss 0.000039\n",
      ">> Epoch 193 finished \tANN training loss 0.000027\n",
      ">> Epoch 194 finished \tANN training loss 0.000044\n",
      ">> Epoch 195 finished \tANN training loss 0.000042\n",
      ">> Epoch 196 finished \tANN training loss 0.000080\n",
      ">> Epoch 197 finished \tANN training loss 0.000035\n",
      ">> Epoch 198 finished \tANN training loss 0.000022\n",
      ">> Epoch 199 finished \tANN training loss 0.000021\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.925000\n",
      "ACCURACY: 0.925\n"
     ]
    }
   ],
   "source": [
    "# 3-layer\n",
    "acc4 = deep_belief_net(hidden_layers_structure=[500, 200, 300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.2)\n",
    "print('ACCURACY: ' + str(acc4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create bar graph\n",
    "plt.clf()\n",
    "objects = ('[200]', '[300]', '[100, 500]', '[500, 200, 300]')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [0.9, 0.92, 0.905, 0.905]\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects, rotation=30)\n",
    "plt.xlabel('Settings')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Benchmark 1 Settings')\n",
    " \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark 2:\n",
    "Take the best performing one from benchmark 1 and test with the sigmoidal activation function and various dropouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deep_belief_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f90a10155deb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Sigmoidal activation, dropout = 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m acc1 = deep_belief_net(hidden_layers_structure=[300], \n\u001b[0m\u001b[0;32m      3\u001b[0m                        \u001b[0mlearning_rate_rbm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                        \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                        \u001b[0mn_epochs_rbm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'deep_belief_net' is not defined"
     ]
    }
   ],
   "source": [
    "# Sigmoidal activation, dropout = 0\n",
    "acc1 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='sigmoid',\n",
    "                       dropout_p=0.)\n",
    "print('ACCURACY: ' + str(acc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoidal activation, dropout = 0.1\n",
    "acc2 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='sigmoid',\n",
    "                       dropout_p=0.1)\n",
    "print('ACCURACY: ' + str(acc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoidal activation, dropout = 0.2\n",
    "acc3 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='sigmoid',\n",
    "                       dropout_p=0.2)\n",
    "print('ACCURACY: ' + str(acc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoidal activation, dropout=0.3\n",
    "acc4 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='sigmoid',\n",
    "                       dropout_p=0.3)\n",
    "print('ACCURACY: ' + str(acc4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoidal activation, dropout=0.4\n",
    "acc5 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='sigmoid',\n",
    "                       dropout_p=0.4)\n",
    "print('ACCURACY: ' + str(acc5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoidal activation, dropout=0.5\n",
    "acc6 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='sigmoid',\n",
    "                       dropout_p=0.5)\n",
    "print('ACCURACY: ' + str(acc6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoidal activation, dropout=0.6\n",
    "acc7 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='sigmoid',\n",
    "                       dropout_p=0.6)\n",
    "print('ACCURACY: ' + str(acc7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoidal activation, dropout=0.7\n",
    "acc8 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='sigmoid',\n",
    "                       dropout_p=0.7)\n",
    "print('ACCURACY: ' + str(acc8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoidal activation, dropout=0.8\n",
    "acc9 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='sigmoid',\n",
    "                       dropout_p=0.8)\n",
    "print('ACCURACY: ' + str(acc9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoidal activation, dropout=0.9\n",
    "acc10 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='sigmoid',\n",
    "                       dropout_p=0.9)\n",
    "print('ACCURACY: ' + str(acc10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoidal activation, dropout=1\n",
    "acc11 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='sigmoid',\n",
    "                       dropout_p=1.)\n",
    "print('ACCURACY: ' + str(acc11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar graph\n",
    "plt.clf()\n",
    "objects = ('0', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '1')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [0.885, 0.92, 0.895, 0.885, 0.885, 0.875, 0.88, 0.85, 0.845, 0.7, 0.1]\n",
    "plt.bar(y_pos, performance, align='center', alpha=1)\n",
    "plt.xticks(y_pos, objects, rotation=30)\n",
    "plt.xlabel('dropout_p')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Benchmark 2, sigmoid Settings')\n",
    " \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark 2 (ReLu)\n",
    "Same benchmark as benchmark 2 but with the ReLu function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu, dropout = 0\n",
    "acc0 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.)\n",
    "print('ACCURACY: ' + str(acc0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu, dropout = 0.1\n",
    "acc1 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.1)\n",
    "print('ACCURACY: ' + str(acc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu, dropout = 0.3\n",
    "acc3 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.3)\n",
    "print('ACCURACY: ' + str(acc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu, dropout = 0.4\n",
    "acc4 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.4)\n",
    "print('ACCURACY: ' + str(acc4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu, dropout = 0.5\n",
    "acc5 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.5)\n",
    "print('ACCURACY: ' + str(acc5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu, dropout = 0.6\n",
    "acc6 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.6)\n",
    "print('ACCURACY: ' + str(acc6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu, dropout = 0.7\n",
    "acc7 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.7)\n",
    "print('ACCURACY: ' + str(acc7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu, dropout = 0.8\n",
    "acc8 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.8)\n",
    "print('ACCURACY: ' + str(acc8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu, dropout = 0.9\n",
    "acc9 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.9)\n",
    "print('ACCURACY: ' + str(acc9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu, dropout = 1\n",
    "acc10 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=1.)\n",
    "print('ACCURACY: ' + str(acc10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar graph\n",
    "plt.clf()\n",
    "objects = ('0', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '1')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [0.88, 0.89, 0.92, 0.90, 0.89, 0.89, 0.87, 0.90, 0.80, 0.155, 0.1]\n",
    "plt.bar(y_pos, performance, align='center', alpha=1)\n",
    "plt.xticks(y_pos, objects, rotation=30)\n",
    "plt.xlabel('dropout_p')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Benchmark 2, relu Settings')\n",
    " \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SVM modules\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC Performance\n",
    "def linSVC(C):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "    clf = LinearSVC(C=C)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    correct = 0\n",
    "    for i in range(0, len(Y_test)):\n",
    "        if Y_pred[i] == Y_test[i]:\n",
    "            correct += 1\n",
    "    return float(correct / len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linSVC_acc1 = linSVC(C=0.1)\n",
    "print('ACCURACY: ' + str(linSVC_acc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linSVC_acc2 = linSVC(C=1.)\n",
    "print('ACCURACY: ' + str(linSVC_acc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linSVC_acc3 = linSVC(C=10)\n",
    "print('ACCURACY: ' + str(linSVC_acc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linSVC_acc4 = linSVC(C=100)\n",
    "print('ACCURACY: ' + str(linSVC_acc4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Performance\n",
    "def decision_tree():\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    correct = 0\n",
    "    for i in range(0, len(Y_test)):\n",
    "        if Y_pred[i] == Y_test[i]:\n",
    "            correct += 1\n",
    "    return float(correct / len(Y_test))\n",
    "\n",
    "decision_tree_acc = decision_tree()\n",
    "print('ACCURACY: ' + str(decision_tree_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting Performance\n",
    "def boosting():\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "    clf = AdaBoostClassifier()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    correct = 0\n",
    "    for i in range(0, len(Y_test)):\n",
    "        if Y_pred[i] == Y_test[i]:\n",
    "            correct += 1\n",
    "    return float(correct / len(Y_test))\n",
    "\n",
    "boosting_acc = boosting()\n",
    "print('ACCURACY: ' + str(boosting_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Performance\n",
    "def random_forest():\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    correct = 0\n",
    "    for i in range(0, len(Y_test)):\n",
    "        if Y_pred[i] == Y_test[i]:\n",
    "            correct += 1\n",
    "    return float(correct / len(Y_test))\n",
    "\n",
    "random_forest_acc = random_forest()\n",
    "print('ACCURACY: ' + str(random_forest_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC, 3rd degree rbf\n",
    "def svc_classifier(C):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "    clf = SVC(C=C, max_iter=-1)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    correct = 0\n",
    "    for i in range(0, len(Y_test)):\n",
    "        if Y_pred[i] == Y_test[i]:\n",
    "            correct += 1\n",
    "    return float(correct / len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc1_acc = svc_classifier(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc2_acc = svc_classifier(1.0)\n",
    "print('ACCURACY: ' + str(svc2_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc3_acc = svc_classifier(10.)\n",
    "print('ACCURACY: ' + str(svc3_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc4_acc = svc_classifier(100.)\n",
    "print('ACCURACY: ' + str(svc4_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar graph\n",
    "plt.clf()\n",
    "objects = ('ReLu (0.2)', 'Sigmoid (0.1)', \n",
    "           'LinearSVC (0.1)', 'LinearSVC (1)', 'LinearSVC (10)', 'LinearSVC (100)', \n",
    "           'DecisionTree', 'AdaBoost', 'RandomForest', \n",
    "           'SVC (0.1)', 'SVC (1)', 'SVC (10)', 'SVC (100)')\n",
    "\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [0.92, 0.92, \n",
    "               0.86, 0.845, 0.84, 0.84,\n",
    "               0.665, 0.395, 0.795,\n",
    "               0.115, 0.805, 0.885, 0.895]\n",
    "plt.bar(y_pos, performance, align='center', alpha=1)\n",
    "plt.xticks(y_pos, objects, rotation=90)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy, Performance')\n",
    "plt.title('Model Comparisons')\n",
    " \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
